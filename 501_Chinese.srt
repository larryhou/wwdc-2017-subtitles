1
00:00:29,763 --> 00:00:31,798
谢谢大家 大家下午好

2
00:00:32,499 --> 00:00:35,169
欢迎参加“音频中的新功能”演讲

3
00:00:36,136 --> 00:00:38,038
我是Akshatha Nagesh
来自音频团队

4
00:00:38,405 --> 00:00:41,108
今天我要跟你们大家分享

5
00:00:41,441 --> 00:00:46,113
在今年的OS发布中
关于音频的一些新功能

6
00:00:48,081 --> 00:00:50,984
我要从音频堆栈的快速概览开始讲

7
00:00:52,152 --> 00:00:55,656
音频框架提供了多种多样的API

8
00:00:55,722 --> 00:00:58,125
我们的主要目标是帮助你

9
00:00:58,292 --> 00:01:02,129
通过你的应用给你的终端用户
提交超常的音频体验

10
00:01:03,330 --> 00:01:06,166
在顶层我们有AV基础框架

11
00:01:06,333 --> 00:01:11,939
API有AVAudioSession、Engine、
Player、Recorder等等

12
00:01:12,706 --> 00:01:16,243
这些API迎合了绝大部分应用的需求

13
00:01:17,177 --> 00:01:19,847
但如果你想进一步自定义体验

14
00:01:20,180 --> 00:01:24,618
你可以使用我们的其他框架
和API 比如AUAudioUnit

15
00:01:24,685 --> 00:01:28,989
音频工具箱框架中的音频编解码器、
Code Mini框架、

16
00:01:29,356 --> 00:01:31,425
AudioHAL框架等等

17
00:01:32,759 --> 00:01:35,162
在我们去年WWDC的演讲中

18
00:01:35,362 --> 00:01:39,633
我们通过堆栈逐个谈了全部API
以及更多内容

19
00:01:40,133 --> 00:01:42,569
我强烈推荐你们参考一下去年的资料

20
00:01:44,071 --> 00:01:46,540
现在让我们看看今天的议事日程

21
00:01:48,008 --> 00:01:51,645
我们要了解我们给某些API中
添加的新功能

22
00:01:51,879 --> 00:01:53,547
从AVFoundation框架

23
00:01:53,614 --> 00:01:56,416
中的API开始讲 其中包括

24
00:01:56,483 --> 00:01:58,819
AVAudioEngine、AVAudioSession

25
00:01:59,152 --> 00:02:02,890
以及在watchOS 4上
AVFoundation中的增强功能

26
00:02:04,191 --> 00:02:07,361
稍后我们将了解音频工具箱的世界

27
00:02:07,661 --> 00:02:11,632
查看AUAudioUnit
和音频格式的增强功能

28
00:02:12,332 --> 00:02:14,801
最后我们会讲设备间音频模式的更新

29
00:02:15,035 --> 00:02:17,371
圆满结束今天的演讲

30
00:02:18,739 --> 00:02:21,275
其间还有几个演示

31
00:02:21,341 --> 00:02:24,478
用于在实际操作中展示今天的新功能

32
00:02:26,280 --> 00:02:28,549
那么让我们从AVAudioEngine开始吧

33
00:02:29,783 --> 00:02:32,085
这是这个API的一个快速回顾

34
00:02:33,187 --> 00:02:38,325
AVAudioEngine是一个强大的基于
Objective-C和Swift的API集合

35
00:02:38,892 --> 00:02:40,827
这个API的主要目标

36
00:02:40,994 --> 00:02:43,697
是简化对实时音频的处理

37
00:02:44,097 --> 00:02:47,234
并让你写代码

38
00:02:47,568 --> 00:02:52,306
以实施各种音频任务的过程
变得非常简单 从简单播放到录音

39
00:02:52,606 --> 00:02:56,076
到更复杂的任务 比如音频处理、混音

40
00:02:56,143 --> 00:02:58,011
甚至是3D音频特化

41
00:02:59,079 --> 00:03:03,350
在我们去年的WWDC演讲中

42
00:03:03,617 --> 00:03:05,919
我们详细介绍了这个API

43
00:03:06,253 --> 00:03:09,823
那么请参看去年的资料
如果你不熟悉这个API的话

44
00:03:11,625 --> 00:03:14,127
Engine管理一系列的节点

45
00:03:14,361 --> 00:03:17,130
而一个节点
就是Engine的基础组成部分

46
00:03:18,031 --> 00:03:22,135
那么这里是一个Engine组合示例
这是一个经典的卡拉OK示例

47
00:03:23,337 --> 00:03:26,240
正如你所看到的
有许多节点连接在一起

48
00:03:26,406 --> 00:03:27,908
形成了完整的处理图

49
00:03:28,942 --> 00:03:32,446
我们有InputNode
隐含地连接

50
00:03:32,513 --> 00:03:35,849
到输入硬件并捕捉用户的声音

51
00:03:36,683 --> 00:03:39,019
这是通过EffectNode处理的

52
00:03:39,086 --> 00:03:40,888
可能是 比如说EQ

53
00:03:42,256 --> 00:03:45,292
我们在InputNode上还有一个东西
当轻触时被调用

54
00:03:45,626 --> 00:03:48,462
用于分析用户的声音

55
00:03:48,529 --> 00:03:50,998
了解他表现怎么样 基于此

56
00:03:51,198 --> 00:03:54,468
我们可以通过PlayerNode
向用户播放一些提示

57
00:03:55,702 --> 00:03:57,237
我们还有另一个PlayerNode

58
00:03:57,304 --> 00:04:00,407
当用户正在唱歌时会播放背景音乐

59
00:04:01,842 --> 00:04:04,044
全部信号都被混在一起

60
00:04:04,278 --> 00:04:07,414
在MixerNode中
最终提供给OutputNode

61
00:04:07,614 --> 00:04:09,883
通过输出硬件播放出来

62
00:04:11,518 --> 00:04:14,221
这是一个引擎集合的简单示例

63
00:04:14,555 --> 00:04:18,024
但通过全部节点
和Engine实际所提供的功能

64
00:04:18,225 --> 00:04:21,195
我们可以创建更复杂的处理图

65
00:04:21,261 --> 00:04:22,896
基于你应用的需要

66
00:04:24,464 --> 00:04:26,233
那么这是关于Engine的回顾

67
00:04:26,300 --> 00:04:29,102
现在让我们来看看
今年Engine有什么新功能

68
00:04:30,304 --> 00:04:34,241
我们有一些新节点
也就是手动渲染模式

69
00:04:34,408 --> 00:04:35,642
和自动关闭模式

70
00:04:36,343 --> 00:04:39,479
同时在AVAudioPlayerNode中
也有功能增强

71
00:04:39,680 --> 00:04:42,216
与文件和缓冲区结束回调有关

72
00:04:43,250 --> 00:04:47,821
我们将逐个来看这些
从手动渲染模式开始

73
00:04:50,924 --> 00:04:53,594
那么这是我们刚才看到的
卡拉OK示例

74
00:04:54,561 --> 00:04:57,998
正如你所看到的
这里的InputNode和OutputNode

75
00:04:58,165 --> 00:05:00,300
连接到了音频硬件

76
00:05:00,634 --> 00:05:04,204
因此 Engine就会实时自动渲染

77
00:05:05,539 --> 00:05:08,041
这里的IO是由硬件驱动的

78
00:05:09,309 --> 00:05:12,112
但如果你想让Engine进行渲染

79
00:05:12,179 --> 00:05:14,214
不是对设备 而是对应用
要怎么做呢？

80
00:05:14,581 --> 00:05:17,084
比如说速度要比实时的快一些？

81
00:05:18,385 --> 00:05:21,922
那么这里有手动渲染模式
可以让你实现

82
00:05:23,190 --> 00:05:24,157
正如你所看到的

83
00:05:24,224 --> 00:05:25,392
在这种模式下

84
00:05:25,459 --> 00:05:29,696
InputNode和OutputNode
可能不会被连接到任何音频设备

85
00:05:30,397 --> 00:05:34,501
而应用将拉动Engine用于输出

86
00:05:34,801 --> 00:05:37,337
并向Engine提供输入

87
00:05:37,571 --> 00:05:39,940
可以通过InputNode

88
00:05:40,240 --> 00:05:41,842
或PlayerNode等等

89
00:05:42,843 --> 00:05:46,547
那么应用在手动渲染模式中驱动IO

90
00:05:48,682 --> 00:05:51,285
在手动渲染模式下有两个变体

91
00:05:51,752 --> 00:05:55,622
即脱机和实时手动渲染模式

92
00:05:56,089 --> 00:05:58,258
我们会具体了解每一个变体

93
00:05:58,525 --> 00:06:01,962
同时 稍后我会展示一个演示

94
00:06:02,029 --> 00:06:04,031
是脱机手动渲染模式的演示

95
00:06:07,668 --> 00:06:10,037
在脱机手动渲染模式下

96
00:06:10,337 --> 00:06:13,874
Engine和你处理图中的全部节点

97
00:06:13,941 --> 00:06:17,110
在没有截止日期
或实时约束的情况下运作

98
00:06:18,078 --> 00:06:21,982
因为这种灵活性
节点可能会选择 比如

99
00:06:22,049 --> 00:06:25,919
使用更昂贵的信号处理机制
当处于脱机时

100
00:06:26,486 --> 00:06:31,491
或者节点 比如说播放器节点
可能选择堵截渲染线程

101
00:06:31,825 --> 00:06:35,395
直到准备好全部输入数据

102
00:06:36,530 --> 00:06:38,999
但这些事可能不会 将不会发生

103
00:06:39,066 --> 00:06:41,702
当节点在进行实时渲染时

104
00:06:42,069 --> 00:06:43,036
我们稍后再看

105
00:06:44,938 --> 00:06:49,109
让我们思考一个简单的例子
我们可以在哪里使用脱机模式

106
00:06:51,478 --> 00:06:52,846
这里有个例子

107
00:06:53,146 --> 00:06:57,317
应用想要在源文件中处理音频数据

108
00:06:57,718 --> 00:07:00,020
我会给那个数据添加一些效果

109
00:07:00,187 --> 00:07:03,190
并把处理输出数据转储到目标文件中

110
00:07:04,258 --> 00:07:07,961
正如你所看到的
那里并没有涉及针对设备的渲染

111
00:07:08,195 --> 00:07:12,699
由此 应用现在可以
在脱机模式下使用Engine

112
00:07:13,467 --> 00:07:17,437
它会在Engine中设置一个
很简单的图 就像这样

113
00:07:18,005 --> 00:07:21,675
它会使用PlayerNode
从源文件中读取数据

114
00:07:22,442 --> 00:07:25,779
通过EffectNode处理它
EffectNode可能是比如说混响

115
00:07:26,380 --> 00:07:29,016
然后把数据从OutputNode中拉出来

116
00:07:29,082 --> 00:07:31,785
并把处理数据驱动到目标文件中

117
00:07:32,753 --> 00:07:38,325
我们很快就会在接下来的几张幻灯片中
看到与这个完全一样的组合的演示

118
00:07:40,327 --> 00:07:43,797
还有许多应用可以使用脱机模式

119
00:07:44,364 --> 00:07:45,799
这里列出了其中一些

120
00:07:46,533 --> 00:07:50,204
除了我刚提到过的音频文件的后处理

121
00:07:50,404 --> 00:07:54,575
你还可以使用脱机模式
比如说混合音频文件

122
00:07:55,509 --> 00:07:58,178
你可以用于脱机处理

123
00:07:58,245 --> 00:08:02,049
通过一个CPU密集型或高质量算法

124
00:08:02,115 --> 00:08:04,284
可能不适合在实时条件下使用

125
00:08:05,352 --> 00:08:07,654
或者简单地说 你可以使用脱机模式

126
00:08:07,988 --> 00:08:11,892
进行测试、调试或调整
运转中的Engine组合

127
00:08:14,228 --> 00:08:17,030
那么正如我所承诺过的总结了脱机模式

128
00:08:17,097 --> 00:08:19,333
我会实际展示一个演示

129
00:08:23,103 --> 00:08:28,242
好了 那么这是一个
能录音的Playground

130
00:08:29,176 --> 00:08:30,010
并且

131
00:08:30,210 --> 00:08:32,712
这也是我们要在源文件中后处理

132
00:08:32,779 --> 00:08:34,548
音频数据的例子

133
00:08:34,847 --> 00:08:37,351
在数据上应用一个混响效果

134
00:08:37,618 --> 00:08:40,187
把输出转储到目标文件中

135
00:08:41,121 --> 00:08:44,491
这里有一些代码片段
我提前建好了可以立即运行

136
00:08:45,859 --> 00:08:51,798
那么我要做的第一件事就是
设置Engine进行渲染

137
00:08:51,865 --> 00:08:56,570
在实时模式下针对设备进行渲染
只为了听一下源文件的声音

138
00:08:56,637 --> 00:08:59,173
当不添加任何效果时

139
00:09:01,775 --> 00:09:06,580
那么我首先打开我要读取的源文件

140
00:09:08,215 --> 00:09:11,785
然后创建并配置我的Engine

141
00:09:12,486 --> 00:09:15,589
那么我有一个Engine
和一个PlayerNode

142
00:09:16,490 --> 00:09:19,993
我要把播放器
放到Engine的主混音器节点

143
00:09:20,294 --> 00:09:23,564
它隐含地连接到了
Engine的OutputNode

144
00:09:25,632 --> 00:09:28,802
然后我要为播放器上的
那个源文件指定计划

145
00:09:28,869 --> 00:09:31,071
以便播放器可以从源文件中读取数据

146
00:09:32,239 --> 00:09:35,008
然后我就开启Engine
并开启播放器

147
00:09:35,776 --> 00:09:38,512
正如我所提到过的
Engine现在是处于实时模式

148
00:09:38,579 --> 00:09:41,315
这将会针渲染到设备中

149
00:09:41,515 --> 00:09:45,385
那么让我们听听没有任何效果的
源文件的声音如何

150
00:09:55,128 --> 00:09:56,196
好的 这是

151
00:09:56,263 --> 00:10:01,101
源文件的声音 那么现在我要做的是

152
00:10:01,168 --> 00:10:04,438
添加一个混响来处理数据

153
00:10:05,472 --> 00:10:08,375
那么我要把播放器
迁移到主混音器连接中

154
00:10:09,109 --> 00:10:10,711
我要插入混响

155
00:10:11,745 --> 00:10:13,580
那么在这里 我创建了一个混响

156
00:10:14,147 --> 00:10:16,583
并且设置了混响的参数

157
00:10:16,750 --> 00:10:19,920
在本例中
我用了一个factory预设

158
00:10:20,587 --> 00:10:23,257
而wetDryMix为70%

159
00:10:24,057 --> 00:10:27,027
然后我要在播放部分插入混响

160
00:10:27,227 --> 00:10:29,196
在播放器和主混音器之间

161
00:10:30,330 --> 00:10:35,802
那么现在如果我运行这个示例
我们就可以听到处理过的输出的声音

162
00:10:46,213 --> 00:10:49,850
好的 那么现在这点上
如果我想 我可以继续

163
00:10:49,917 --> 00:10:53,387
并调节我的混响参数
使它听起来与我想要的效果一模一样

164
00:10:53,720 --> 00:10:55,622
那么假如我对全部参数都很满意

165
00:10:55,689 --> 00:10:59,927
那么现在我要把我的源文件
完全导出到目标文件中

166
00:11:00,260 --> 00:11:02,529
并且这就是脱机模式起作用的地方

167
00:11:03,830 --> 00:11:05,566
那么我首先要做的是

168
00:11:06,900 --> 00:11:10,671
我会启动…我会把Engine
从实时模式切换为脱机模式

169
00:11:18,045 --> 00:11:19,513
那么我所要做的就是

170
00:11:20,714 --> 00:11:24,952
调用启用手动渲染模式API
然后表达

171
00:11:25,118 --> 00:11:27,254
“它需要成为脱机变体”

172
00:11:28,222 --> 00:11:32,726
我要指定我希望Engine
提供给我的一种输出格式

173
00:11:33,193 --> 00:11:36,263
那么在本例中输出格式
要与输入格式一样

174
00:11:37,764 --> 00:11:40,834
然后我会指定最大帧数

175
00:11:41,068 --> 00:11:43,203
也就是你请求Engine

176
00:11:43,270 --> 00:11:45,472
在单一渲染调用中

177
00:11:45,539 --> 00:11:47,174
进行渲染的最大帧数

178
00:11:47,641 --> 00:11:50,444
那么在本例中 这个值是4096

179
00:11:50,711 --> 00:11:53,080
但你可以随你自己的意愿进行配置

180
00:11:54,715 --> 00:11:57,618
那么现在如果我继续并运行这个示例

181
00:11:59,286 --> 00:12:02,589
什么也不会发生
因为Engine现在处于脱机模式

182
00:12:02,823 --> 00:12:03,991
它已准备好进行渲染了

183
00:12:04,324 --> 00:12:08,495
但当然了 它正在等待应用
拉动Engine用于输出

184
00:12:09,496 --> 00:12:13,767
那么我们接下来要做的其实是
拉动Engine用于输出

185
00:12:16,937 --> 00:12:22,576
那么在这里我创建了一个输出文件
我想在这个输出文件中转储处理数据

186
00:12:25,112 --> 00:12:29,483
我还创建了一个输出缓冲区
我会请求Engine

187
00:12:29,550 --> 00:12:31,885
在每个渲染调用中循序地渲染到
这个输出缓冲区中

188
00:12:32,619 --> 00:12:35,088
缓冲区的格式与我之前

189
00:12:35,155 --> 00:12:38,625
启用脱机模式时提到过的格式一样

190
00:12:40,761 --> 00:12:42,296
然后就是渲染循环

191
00:12:42,362 --> 00:12:45,532
我会持续从渲染循环中
拉动Engine用于输出

192
00:12:46,200 --> 00:12:49,870
现在 在本例中 我有一个源文件
长度约为三分钟

193
00:12:50,504 --> 00:12:52,439
那么我其实不想分配

194
00:12:52,506 --> 00:12:55,809
一个巨大的输出缓冲区
并请求Engine

195
00:12:55,876 --> 00:12:58,478
在单一渲染调用中
渲染整个三分钟的数据

196
00:12:58,946 --> 00:13:00,280
这也是为什么我要

197
00:13:00,347 --> 00:13:03,917
分配一个大小合理的输出缓冲区的原因

198
00:13:04,251 --> 00:13:07,888
但会持续地拉动Engine
并输出到同一个缓冲区中

199
00:13:07,955 --> 00:13:10,224
然后把输出数据转储到目标文件中

200
00:13:12,292 --> 00:13:15,963
那么在每个迭代中 我都要确定

201
00:13:16,029 --> 00:13:18,065
在这个渲染调用中要渲染的帧数

202
00:13:18,532 --> 00:13:21,201
并且我会在Engine上
调用渲染脱机测量仪

203
00:13:21,268 --> 00:13:23,837
要求它渲染那么多数量的帧

204
00:13:24,071 --> 00:13:26,974
并给它提供我们刚才分配的输出缓冲区

205
00:13:27,875 --> 00:13:30,544
根据状态 如果它渲染成功

206
00:13:30,944 --> 00:13:34,448
成功渲染了数据 那么我就继续

207
00:13:34,648 --> 00:13:36,850
并把数据拖拽到我的输出文件中

208
00:13:37,351 --> 00:13:39,987
万一它渲染失败
然后有哪儿发生了错误

209
00:13:40,053 --> 00:13:43,557
那么你可以查看错误代码
获取更多信息

210
00:13:44,591 --> 00:13:46,593
那么最终 当渲染完成后

211
00:13:46,660 --> 00:13:49,263
我会停止播放器并停止Engine

212
00:13:50,097 --> 00:13:53,166
那么现在如果我继续
并运行这个示例

213
00:13:53,233 --> 00:13:57,237
整个源文件都将被输出
并且数据将被转储到目标文件中

214
00:13:57,804 --> 00:13:58,805
那么让我们来试试

215
00:14:02,809 --> 00:14:06,547
好的 那么你可能观察到了
这个三分钟长的

216
00:14:06,980 --> 00:14:11,485
源文件被渲染到了一个输出文件中

217
00:14:11,552 --> 00:14:13,187
比实时模式下更快

218
00:14:13,453 --> 00:14:16,423
这是脱机渲染模式的主要应用之一

219
00:14:17,357 --> 00:14:21,795
那么我们接下来要做的就是
再一次聆听源文件

220
00:14:22,996 --> 00:14:29,570
和目标文件 确保数据确实经过了处理

221
00:14:30,037 --> 00:14:31,705
那么这是我的源文件

222
00:14:33,106 --> 00:14:35,275
这是我的目标文件

223
00:14:39,012 --> 00:14:41,248
那么首先让我们先听听源文件

224
00:14:50,057 --> 00:14:52,259
那么正如你所听到的 它很干燥

225
00:14:53,093 --> 00:14:53,927
而现在

226
00:14:54,361 --> 00:14:55,429
这是处理过的文件

227
00:15:04,204 --> 00:15:05,973
好的 那么正如我们所期待的

228
00:15:06,039 --> 00:15:08,475
处理过的数据中添加了混响效果

229
00:15:09,676 --> 00:15:13,480
那么这就对脱机渲染
演示做了一个总结

230
00:15:13,547 --> 00:15:15,616
我要切换回幻灯片

231
00:15:19,119 --> 00:15:23,657
（演示
AVAUDIOENGINE-脱机手动渲染）

232
00:15:25,025 --> 00:15:27,194
那么正如我所提到过的 脱机渲染模式

233
00:15:27,261 --> 00:15:28,529
有许多种应用方式

234
00:15:28,929 --> 00:15:34,401
那么我很高兴地告诉大家
这个例子的示例代码

235
00:15:34,468 --> 00:15:36,803
可以从我们演讲的主页上看到

236
00:15:36,970 --> 00:15:40,440
在演讲的最后
我们会提供一个主页的链接

237
00:15:42,709 --> 00:15:45,679
现在让我们进入
手动渲染模式的第二个变体

238
00:15:46,380 --> 00:15:48,015
实时手动进入模式

239
00:15:48,849 --> 00:15:51,451
正如它的名字所暗指的 在这种模式下

240
00:15:51,518 --> 00:15:54,388
你处理图中的Engine和全部节点

241
00:15:54,588 --> 00:15:57,925
将会假设它们正在实时情境下进行渲染

242
00:15:58,325 --> 00:16:01,028
由此 它们会遵从实时约束

243
00:16:01,728 --> 00:16:06,466
也就是说它们不会在渲染线程上
截断任何调用

244
00:16:06,967 --> 00:16:09,603
比如它们将不会调用任何
libdispatch

245
00:16:10,170 --> 00:16:13,407
它们将不会分配内存
或等待在mutex上进行截断

246
00:16:14,308 --> 00:16:15,876
因为这个约束

247
00:16:16,043 --> 00:16:19,446
假如节点的输入数据不能及时准备好

248
00:16:19,980 --> 00:16:22,516
节点就别无其它选择 但会表明

249
00:16:22,583 --> 00:16:25,118
“给那个渲染循环删除数据

250
00:16:25,185 --> 00:16:27,454
或假定为零并继续 ”

251
00:16:29,957 --> 00:16:31,959
现在让我们看看在实时手动渲染模式下

252
00:16:32,025 --> 00:16:33,961
要在哪里使用Engine

253
00:16:35,495 --> 00:16:38,131
假如你有一个自定义AU音频单元

254
00:16:38,966 --> 00:16:41,735
位于实时播放部分

255
00:16:42,102 --> 00:16:45,105
并且在音频单元的内部渲染组块内

256
00:16:45,372 --> 00:16:48,642
你想处理所接收的数据

257
00:16:48,709 --> 00:16:52,079
通过一台或几台其它音频单元

258
00:16:52,846 --> 00:16:57,718
在那种情况下 你可以设置Engine
使用这些其它音频装置

259
00:16:57,784 --> 00:17:01,288
并在实时手动渲染模式下处理数据

260
00:17:02,856 --> 00:17:05,659
第二个例子是假如你想处理

261
00:17:05,726 --> 00:17:08,462
电影或视频中的音频数据

262
00:17:08,694 --> 00:17:10,696
当它正在播放或回放时

263
00:17:11,498 --> 00:17:14,367
因为这是实时发生的
你可以使用Engine

264
00:17:14,434 --> 00:17:17,871
在实时手动渲染模式下
来进行音频处理

265
00:17:18,704 --> 00:17:21,008
那么现在让我们思考第二种用例

266
00:17:21,241 --> 00:17:24,111
看如何设置和使用Engine

267
00:17:24,444 --> 00:17:26,613
作为代码示例

268
00:17:28,749 --> 00:17:33,420
那么这是接收输入电影数据流的应用

269
00:17:33,854 --> 00:17:37,090
并实时显示 假如说在电视上实时显示

270
00:17:37,658 --> 00:17:40,761
但它想要做的是在输入中处理音频数据

271
00:17:40,827 --> 00:17:44,498
在它进入输出之前

272
00:17:45,732 --> 00:17:49,870
那么现在它可以使用
实时手动渲染模式下的Engine

273
00:17:50,704 --> 00:17:53,874
那么它可以设置一个这样的处理图

274
00:17:53,941 --> 00:17:56,777
它可以通过输入节点提供输入

275
00:17:57,377 --> 00:17:59,179
通过效果节点处理它

276
00:17:59,479 --> 00:18:01,648
然后从输出节点中拉动数据

277
00:18:01,715 --> 00:18:04,785
并在设备上播放

278
00:18:07,187 --> 00:18:08,889
现在让我们看一个代码示例

279
00:18:08,956 --> 00:18:11,992
关于如何设置
和使用这种模式的Engine

280
00:18:16,630 --> 00:18:17,631
那么这是代码

281
00:18:17,998 --> 00:18:21,134
请注意设置Engine自身

282
00:18:21,201 --> 00:18:25,873
是从非实时情境中发生的
只有渲染部分

283
00:18:25,939 --> 00:18:28,275
才实际发生在实时情境中

284
00:18:28,742 --> 00:18:32,713
那么这是设置代码
你首先清除Engine

285
00:18:34,248 --> 00:18:38,919
默认为Engine一经创建
即准备好渲染到设备

286
00:18:38,986 --> 00:18:41,822
除非你把它切换到手动渲染模式

287
00:18:42,756 --> 00:18:45,492
那么你清除了Engine
做了必要的连接

288
00:18:45,659 --> 00:18:48,862
然后把它切换回手动渲染模式

289
00:18:49,563 --> 00:18:51,999
这个是与我们在演示中
看到的一模一样的API

290
00:18:52,165 --> 00:18:55,969
除了我们现在要表达的内容
现在是请求Engine

291
00:18:56,203 --> 00:18:58,639
在实时手动渲染模式下运作

292
00:18:59,273 --> 00:19:03,544
指定输出格式和最大帧数

293
00:19:05,279 --> 00:19:08,048
接下来你要做的就是搜索和缓存

294
00:19:08,115 --> 00:19:10,450
这被称为放弃组块

295
00:19:10,918 --> 00:19:15,122
现在因为Engine的渲染
是从实时情境中发生的

296
00:19:15,355 --> 00:19:18,825
你将不能使用我们在刚才的演示中
所看到的渲染脱机Objective-C

297
00:19:18,892 --> 00:19:20,928
或Swift元数据

298
00:19:21,295 --> 00:19:22,329
这是因为

299
00:19:22,529 --> 00:19:26,934
在实时情境中使用Objective-C
或Swift运行时间不安全

300
00:19:27,367 --> 00:19:30,404
那么相反
Engine自己会给你提供一个渲染组块

301
00:19:30,604 --> 00:19:34,141
你可以用于搜索和缓存
然后稍后使用这个渲染组块

302
00:19:34,208 --> 00:19:36,710
在实时情境中渲染Engine

303
00:19:38,545 --> 00:19:41,849
接下来…要做的是设置你的输入节点

304
00:19:41,915 --> 00:19:44,618
从而给Engine提供输入数据

305
00:19:45,352 --> 00:19:50,390
在这里你要指定你将提供的输入格式

306
00:19:50,457 --> 00:19:53,060
输入格式可以和输出格式不一样

307
00:19:54,094 --> 00:19:57,431
并且你还要提供Engine
将要调用的组块

308
00:19:57,497 --> 00:19:59,600
无论何时当它需要输入数据时

309
00:20:01,969 --> 00:20:05,239
当调用这个组块后
Engine会告诉你

310
00:20:05,305 --> 00:20:07,941
它实际需要多少输入帧数

311
00:20:08,542 --> 00:20:14,181
在那点上 如果你有数据
你要填充输入音频缓冲区列表

312
00:20:14,414 --> 00:20:15,983
并将其返回给Engine

313
00:20:16,817 --> 00:20:20,053
但如果你没有数据
你只需要返回空即可

314
00:20:21,455 --> 00:20:24,725
现在请注意 输入节点既可用于脱机

315
00:20:24,791 --> 00:20:26,693
又可用于实时手动渲染模式

316
00:20:27,227 --> 00:20:30,330
但是当你在实时手动渲染模式中使用时

317
00:20:30,697 --> 00:20:34,401
就会在实时情境中
同时调用这个输入组块

318
00:20:34,668 --> 00:20:38,105
意思就是你需要注意

319
00:20:38,172 --> 00:20:41,575
不要在这个输入组块内截断任何调用

320
00:20:43,644 --> 00:20:46,647
接下来的设置是要清除你的输出缓冲区

321
00:20:47,314 --> 00:20:51,418
这里的不同之处在于：
你将创建一个AVAudioPCMBuffer

322
00:20:51,685 --> 00:20:55,389
并获取它的音频缓冲区列表 这将用在

323
00:20:55,455 --> 00:20:57,391
实时渲染逻辑中

324
00:20:58,692 --> 00:21:00,961
最后你要继续并启动Engine

325
00:21:01,562 --> 00:21:04,298
那么现在Engine已经设置好了
准备好进行渲染了

326
00:21:04,364 --> 00:21:08,068
它正在等待应用拉动输出数据

327
00:21:10,737 --> 00:21:13,340
现在这是实际的渲染逻辑

328
00:21:14,007 --> 00:21:17,344
请注意这部分代码使用C++写的

329
00:21:17,611 --> 00:21:22,182
这是因为正如我所提到过的
用Objective-C

330
00:21:22,249 --> 00:21:25,452
或Swift运行时间不安全
在实时情境中

331
00:21:26,520 --> 00:21:31,191
那么我们首先要做的就是：
调用我们刚才缓存的渲染组块

332
00:21:31,758 --> 00:21:34,528
并请求Engine渲染指定数量的帧

333
00:21:34,595 --> 00:21:37,064
并将其提供给
我们所创建的outputBufferList

334
00:21:38,098 --> 00:21:41,635
最终根据状态 如果成功

335
00:21:41,702 --> 00:21:45,539
意味着一切都没问题
并且数据也被渲染到了输出缓冲区中

336
00:21:46,206 --> 00:21:51,645
但你还会从输入备注中获取不完整数据
作为一个状态 意思是

337
00:21:51,712 --> 00:21:56,984
当Engine调用你的输入组块
获取输入数据时

338
00:21:57,184 --> 00:22:01,054
那个输入组块中的空对象中
没有足够的数据

339
00:22:01,922 --> 00:22:02,956
请注意

340
00:22:03,023 --> 00:22:05,425
在这种情况下
如果你在处理图中有其它资源

341
00:22:05,492 --> 00:22:08,962
比如说你有一些播放器备注

342
00:22:09,363 --> 00:22:12,065
这些备注将仍然会渲染输入数据

343
00:22:12,132 --> 00:22:16,036
那么你可能仍然会在输出缓冲区中
有输出 那么你可以

344
00:22:16,103 --> 00:22:18,338
查看输出缓冲区的大小

345
00:22:18,405 --> 00:22:20,874
从而判断是否有数据

346
00:22:22,309 --> 00:22:26,747
当然了 你要处理包含错误的其它状态

347
00:22:27,047 --> 00:22:31,318
在实时手动渲染模式下的渲染逻辑中
会有很多错误

348
00:22:34,188 --> 00:22:37,558
现在最后是渲染原因上的备注

349
00:22:38,292 --> 00:22:41,461
在脱机模式下
因为没有截止日期或实时约束

350
00:22:41,528 --> 00:22:45,599
你可以使用Objective-C
或Swift渲染

351
00:22:45,966 --> 00:22:48,802
或可以基于渲染调用使用渲染组块

352
00:22:48,869 --> 00:22:50,270
以渲染Engine

353
00:22:50,804 --> 00:22:55,309
但在实时手动渲染模式下
你必须使用基于组块的渲染调用

354
00:22:57,144 --> 00:22:59,880
那么这就是手动渲染模式

355
00:23:00,180 --> 00:23:05,285
现在让我们看看
Engine中的下一个新模式

356
00:23:05,352 --> 00:23:07,020
即自动关闭模式

357
00:23:09,356 --> 00:23:12,125
现在通常是应用要

358
00:23:12,392 --> 00:23:15,629
暂停或停止Engine 当不使用时

359
00:23:15,863 --> 00:23:17,130
以便节约电量

360
00:23:17,998 --> 00:23:22,035
比如假如说我们有一个音乐应用
使用了其中一个播放器节点

361
00:23:22,236 --> 00:23:24,638
来播放某个文件

362
00:23:25,138 --> 00:23:27,474
那么比如说 用户停止了播放

363
00:23:28,141 --> 00:23:32,312
现在应用不应该仅仅暂停
或停止播放器节点

364
00:23:32,546 --> 00:23:34,915
还应该暂停或停止Engine

365
00:23:35,315 --> 00:23:37,451
以防止它空转

366
00:23:38,418 --> 00:23:42,222
但在过去 我们看到并不是
全部应用都能实现

367
00:23:42,289 --> 00:23:44,925
尤其是在watchOS上

368
00:23:45,359 --> 00:23:48,262
由此 我们现在添加了安全网

369
00:23:48,328 --> 00:23:51,732
以通过这个自动关闭模式节约电量

370
00:23:52,866 --> 00:23:55,235
当Engine在这个模式下运作时

371
00:23:56,136 --> 00:23:59,907
它将持续监控
并且如果它检测到那个Engine

372
00:23:59,973 --> 00:24:02,643
空转了一段指定的时间

373
00:24:02,709 --> 00:24:05,746
它将停止音频硬件并删除

374
00:24:06,280 --> 00:24:09,483
那么稍后 假如任意资源再次被激活

375
00:24:09,550 --> 00:24:11,785
它将动态地开启音频硬件

376
00:24:12,352 --> 00:24:14,087
全部这些都是在底层发生的

377
00:24:15,155 --> 00:24:17,724
这是在watchOS上的强制性行为

378
00:24:17,891 --> 00:24:21,094
但它还可以在其它平台上选择启动

379
00:24:23,564 --> 00:24:27,467
现在下一个增强功能
是在AV音频播放器节点中

380
00:24:29,736 --> 00:24:32,739
AV音频播放器节点
是Engine中的一个资源节点

381
00:24:33,073 --> 00:24:36,476
通过这个你可以
安排缓冲区或文件用于播放

382
00:24:37,911 --> 00:24:41,882
现有的安排方法是通过一个结束处理器

383
00:24:42,216 --> 00:24:44,718
它们调用结束处理器
当你所提供的数据

384
00:24:44,785 --> 00:24:47,554
被播放器消耗完之后

385
00:24:48,989 --> 00:24:53,093
我们现在添加了新的结束处理器
和回调的新类型

386
00:24:53,360 --> 00:24:57,231
以便让你了解结束的各种不同阶段

387
00:24:58,999 --> 00:25:03,370
第一个新回调类型是数据消耗类型

388
00:25:03,604 --> 00:25:06,773
这与现有的结束处理器一模一样

389
00:25:07,207 --> 00:25:10,644
也就是说 当调用结束处理器时

390
00:25:10,711 --> 00:25:13,247
意味着数据已经被播放器消耗完了

391
00:25:13,714 --> 00:25:17,618
那么在那点上 如果你想
你就可以回收那个缓冲区

392
00:25:17,885 --> 00:25:21,922
或者如果你在播放器上安排了更多数据
你就可以继续执行

393
00:25:23,223 --> 00:25:26,059
第二种回调类型是数据渲染回调

394
00:25:26,393 --> 00:25:29,162
意思就是你所提供的数据

395
00:25:29,229 --> 00:25:32,199
已经被渲染完成了
当调用结束处理器时

396
00:25:33,033 --> 00:25:37,271
并且这不会导致信号处理延迟后移

397
00:25:37,337 --> 00:25:39,273
在你的处理图中

398
00:25:40,541 --> 00:25:43,110
最后一种类型是数据播放类型

399
00:25:43,177 --> 00:25:44,878
也是最有意思的一个

400
00:25:45,179 --> 00:25:48,215
它的意思是当你调用结束处理器时

401
00:25:48,515 --> 00:25:50,851
缓冲区或你所安排的文件

402
00:25:50,918 --> 00:25:54,188
实际已经完成了播放
从听者的角度来说

403
00:25:54,988 --> 00:25:56,890
只有当Engine

404
00:25:56,957 --> 00:25:58,392
渲染到设备时才可适用

405
00:25:59,059 --> 00:26:02,029
这会导致全部信号处理延迟

406
00:26:02,229 --> 00:26:04,965
你处理图中的播放器后移

407
00:26:05,032 --> 00:26:08,335
以及音频播放设备的延迟

408
00:26:09,837 --> 00:26:14,007
那么作为一个代码示例
让我们看一个安排文件方法

409
00:26:14,074 --> 00:26:16,543
通过这个你可以为播放安排文件

410
00:26:17,277 --> 00:26:19,746
那么在这里 我正在给播放安排文件

411
00:26:19,813 --> 00:26:23,917
我指明我有兴趣了解何时播放数据

412
00:26:25,152 --> 00:26:27,521
那个空对象 我要提供一个结束处理器

413
00:26:28,021 --> 00:26:31,258
然后当调用结束处理器时
意味着我的文件

414
00:26:31,325 --> 00:26:34,127
已经完成播放 那么在这点上
我可以表明

415
00:26:34,194 --> 00:26:36,530
“通知我的UI线程更新UI”

416
00:26:36,597 --> 00:26:38,699
或者我可以通知我的主线程

417
00:26:38,765 --> 00:26:41,168
停止Engine 如果适用的话

418
00:26:43,170 --> 00:26:44,905
那么这就是

419
00:26:44,972 --> 00:26:47,774
AVAudio Engine中的
全部增强功能

420
00:26:48,475 --> 00:26:50,477
在这点上 我还想提一下

421
00:26:50,544 --> 00:26:54,448
我们很快就会在2018年弃用

422
00:26:54,515 --> 00:26:58,051
音频工具箱框架中的
AU Graph API

423
00:26:58,118 --> 00:27:01,989
所以请着手使用AVAudio Engine
而别再用AUGraph了

424
00:27:02,055 --> 00:27:03,557
如果你还没有这样做的话

425
00:27:06,727 --> 00:27:10,564
现在让我们谈谈
AV基础框架中的第二个API集合：

426
00:27:10,631 --> 00:27:11,698
AV音频会话

427
00:27:13,967 --> 00:27:17,838
在今年发布的iOS、tvOS
和macOS中

428
00:27:17,905 --> 00:27:19,806
AirPlay 2是一个全新的技术

429
00:27:20,641 --> 00:27:25,112
通过装有AirPlay 2的设备
你可以执行多房间播放音频

430
00:27:25,345 --> 00:27:26,980
比如说Homepod

431
00:27:29,116 --> 00:27:32,953
那么有一个单独的专题演讲
叫作“AirPlay 2访谈”

432
00:27:33,020 --> 00:27:35,088
时间为本周四下午4:10

433
00:27:35,589 --> 00:27:39,259
那场演讲将具体分析
这种技术的全部功能

434
00:27:39,326 --> 00:27:42,529
那么你可以考虑参加那场演讲
如果你有兴趣了解更多详情的话

435
00:27:44,331 --> 00:27:47,935
同时与AirPlay 2共同发布的
有一个长格式音频

436
00:27:48,435 --> 00:27:53,273
这是一系列的内容 比如音乐或播客

437
00:27:53,540 --> 00:27:56,710
一般只有几分钟的长度

438
00:27:57,077 --> 00:27:59,980
可以与其它人共享播放

439
00:28:00,914 --> 00:28:03,217
比如 假如说你在家办聚会

440
00:28:03,450 --> 00:28:06,086
你通过AirPlay设备播放一个
音乐播放列表

441
00:28:06,153 --> 00:28:09,590
现在这将被分类为…

442
00:28:09,656 --> 00:28:10,924
那将被分类

443
00:28:10,991 --> 00:28:12,426
为长格式音频内容

444
00:28:13,861 --> 00:28:16,430
现在通过AirPlay 2
和长格式音频

445
00:28:16,663 --> 00:28:21,668
我们现在获得了一个独立的共享路径
从长格式音频应用

446
00:28:21,902 --> 00:28:23,303
到AirPlay 2设备

447
00:28:23,871 --> 00:28:27,708
我稍后再详细解释更多

448
00:28:28,909 --> 00:28:32,946
好的…现在我们在AV音频会话中
有一个新API

449
00:28:33,013 --> 00:28:36,416
应用可以将自己识别为长格式

450
00:28:36,583 --> 00:28:40,654
并利用这个独立的共享音频路径

451
00:28:42,689 --> 00:28:45,692
那么让我们思考一下我刚提到的例子

452
00:28:45,759 --> 00:28:49,530
那么假如我们在家里办聚会
并且你正在

453
00:28:49,596 --> 00:28:50,764
AirPlay设备上播放音乐

454
00:28:51,431 --> 00:28:55,502
我们将对比当前行为

455
00:28:55,569 --> 00:28:57,271
与长格式音频路径行为
看是如何变更的

456
00:28:57,638 --> 00:28:59,239
那么这是当前行为

457
00:28:59,840 --> 00:29:03,911
那么你…现在正在通过
AirPlay设备播放音乐

458
00:29:04,311 --> 00:29:06,480
假如现在有人打电话给你

459
00:29:08,048 --> 00:29:09,383
那么

460
00:29:09,783 --> 00:29:13,787
在这点上 你的音乐播放就会被中断
停止播放

461
00:29:14,354 --> 00:29:17,424
而电话会占用系统音频

462
00:29:17,491 --> 00:29:19,226
可能是接收器或比如说扬声器

463
00:29:20,227 --> 00:29:24,364
那么只有电话结束后
音乐收到了一个可恢复中断

464
00:29:24,431 --> 00:29:27,034
它才会恢复播放

465
00:29:28,135 --> 00:29:31,672
那么正如你所看到的
电话打断了你聚会的音乐

466
00:29:31,738 --> 00:29:34,408
这并不是一个很理想的情景

467
00:29:35,075 --> 00:29:39,713
那么我们现在要看通过长格式音频路径
这种行为会发生怎样的变化

468
00:29:41,548 --> 00:29:43,550
那么让我们来看同一个例子

469
00:29:43,617 --> 00:29:48,555
那么现在我们正在通过
支持AirPlay 2的设备播放音乐

470
00:29:49,790 --> 00:29:51,959
然后有电话进来了

471
00:29:52,693 --> 00:29:55,963
现在因为电话不是长格式音频

472
00:29:56,296 --> 00:29:58,866
它不会中断你的音乐播放

473
00:29:59,233 --> 00:30:02,569
它获得了一个单独的路径进入系统音频

474
00:30:03,036 --> 00:30:03,971
没有导致任何问题

475
00:30:04,538 --> 00:30:06,940
那么通过长格式音频路径

476
00:30:07,241 --> 00:30:11,578
两个会话可以共存 而不会相互中断

477
00:30:11,912 --> 00:30:15,749
并且正如你所看到的
这当然提升了用户体验

478
00:30:18,118 --> 00:30:19,019
那么…

479
00:30:23,190 --> 00:30:26,026
那么总的来说 通过长格式音频路径

480
00:30:26,393 --> 00:30:30,030
将自己识别为长格式的所有应用

481
00:30:30,097 --> 00:30:35,169
比如说音乐、播客
或任意其它音乐流应用

482
00:30:35,769 --> 00:30:39,306
都会获得专用…都会获得独立共享路径

483
00:30:39,373 --> 00:30:41,275
在支持AirPlay 2的设备上

484
00:30:42,009 --> 00:30:44,745
现在请注意这两者之间有一个调停会话

485
00:30:45,112 --> 00:30:47,381
那会确保

486
00:30:47,447 --> 00:30:51,385
同一时间只有一个这样的应用
在AirPlay设备上播放

487
00:30:51,652 --> 00:30:54,221
那么这些应用不能相互穿插

488
00:30:55,022 --> 00:30:59,960
而其它使用系统路径的非长格式应用

489
00:31:00,027 --> 00:31:01,562
则可以相互中断

490
00:31:01,628 --> 00:31:05,699
或相互穿插 它们会获得进入
系统音频的另一条路径

491
00:31:05,866 --> 00:31:08,735
并不会中断你的长格式音频播放

492
00:31:11,438 --> 00:31:15,976
现在让我们看看应用是如何
将自己识别为长格式应用

493
00:31:16,043 --> 00:31:17,811
以及是如何利用这个路径的

494
00:31:19,446 --> 00:31:22,649
在iOS和tvOS上 代码非常简单

495
00:31:22,950 --> 00:31:26,119
你获得AVAudio会话的共享实例

496
00:31:26,353 --> 00:31:29,323
且你使用这个新API把你的类别设置

497
00:31:29,389 --> 00:31:33,393
为播放
并将路径共享政策设置为长格式

498
00:31:36,230 --> 00:31:38,498
现在再看看macOS

499
00:31:39,333 --> 00:31:43,036
路径与iOS和tvOS上非常相似

500
00:31:43,370 --> 00:31:45,272
全部长格式音频应用

501
00:31:45,339 --> 00:31:49,009
比如 你的iTunes和
其它音乐流应用

502
00:31:49,076 --> 00:31:52,813
会获得进入支持AirPlay 2的
设备的路径

503
00:31:53,747 --> 00:31:55,983
当然了 这之间有一个调停会话

504
00:31:57,017 --> 00:32:02,256
而其它系统应用 比如GarageBand、
Safari或Game App

505
00:32:02,523 --> 00:32:05,192
将不会中断你的长格式音频应用

506
00:32:05,259 --> 00:32:09,696
它们总是会相互穿插
并获得进入默认设备的路径

507
00:32:10,831 --> 00:32:14,601
要在macOS上启用
长格式音频路径的支持

508
00:32:14,668 --> 00:32:19,072
我们现在向macOS引入了
AVAudio会话的一个非常小的子集

509
00:32:19,640 --> 00:32:22,409
那么作为应用 为了将自己识别为

510
00:32:22,476 --> 00:32:25,112
长格式 你要再次获取
AVAudio会话的

511
00:32:25,179 --> 00:32:27,748
共享和感觉

512
00:32:27,814 --> 00:32:30,817
将路径共享政策设置为长格式

513
00:32:33,086 --> 00:32:34,054
那么这就是

514
00:32:34,121 --> 00:32:35,722
AVAudio会话的改进

515
00:32:35,789 --> 00:32:39,326
让我们现在看看AV基础框架中
最后一个部分

516
00:32:39,393 --> 00:32:41,195
即watchOS上的改进

517
00:32:43,897 --> 00:32:48,468
那么我们引入了AV…
我们在watchOS 3.1SDK中

518
00:32:48,535 --> 00:32:50,337
也可以用AVAudio Player API了

519
00:32:50,904 --> 00:32:53,607
那么这是我们第一次
在WWDC提到这个API

520
00:32:54,141 --> 00:32:57,244
使用AVAudio Player
播放的一个好处是

521
00:32:57,477 --> 00:33:00,614
它与其AVAudio会话相关联

522
00:33:00,681 --> 00:33:03,317
因此你可以使用会话类别选项

523
00:33:03,383 --> 00:33:07,654
比如解码器或与其它穿插
来描述你应用的行为

524
00:33:08,789 --> 00:33:12,893
现在从watchOS 4开始
我们会开始披露更多的API

525
00:33:12,960 --> 00:33:17,898
以帮助你录制音频 即我们正在开发
AVAudio Recorder

526
00:33:18,198 --> 00:33:21,502
以及AVAudio Input Note
和AVAudio Engine

527
00:33:22,769 --> 00:33:25,639
有了这些
就有了AVAudio录音许可

528
00:33:25,806 --> 00:33:29,276
通过这个许可 应用就可以请求
用户允许并进行录音

529
00:33:30,444 --> 00:33:34,248
现在[听不清]这个
你可以使用watchKit框架

530
00:33:34,314 --> 00:33:35,482
来进行录音

531
00:33:35,549 --> 00:33:36,917
使用Apple UI

532
00:33:37,217 --> 00:33:41,989
但现在通过这些API
你可以进行录音

533
00:33:42,055 --> 00:33:43,223
用你自己的UI

534
00:33:44,424 --> 00:33:46,927
通过AVAudio Recorder
你可以录音到一个文件

535
00:33:47,227 --> 00:33:50,297
或如果你想直接获取麦克风数据

536
00:33:50,364 --> 00:33:53,467
你可以使用AVAudio输入节点
同时也可以选择

537
00:33:53,634 --> 00:33:54,635
把它写入到文件中

538
00:33:55,536 --> 00:33:58,539
这是watchOS上支持的格式

539
00:33:58,605 --> 00:34:00,541
无论是播放还是录音

540
00:34:02,276 --> 00:34:04,578
关于录音政策的最后一个提醒

541
00:34:05,245 --> 00:34:08,248
录音只能当应用处于前台时开始

542
00:34:08,849 --> 00:34:11,818
但它允许在后台继续录音

543
00:34:11,885 --> 00:34:15,322
但是…将会在顶部显示麦克风图标

544
00:34:15,489 --> 00:34:17,157
以便用户知道他们正在录音

545
00:34:18,292 --> 00:34:21,428
在后台录音是CPU受限的

546
00:34:21,495 --> 00:34:23,429
类似于锻炼会话

547
00:34:23,496 --> 00:34:26,266
你可以参考这个URL获取更多信息

548
00:34:28,268 --> 00:34:30,704
现在让我们进入音频工具箱的世界

549
00:34:30,771 --> 00:34:34,842
看看AUAudio单元和音频格式
有哪些改进

550
00:34:36,376 --> 00:34:39,346
在AUAudio单元中有两个主要改进

551
00:34:39,947 --> 00:34:40,848
在本场演讲的最后

552
00:34:40,914 --> 00:34:43,116
我们将进行一次演示

553
00:34:43,183 --> 00:34:45,219
在实际操作中了解这两个新功能

554
00:34:48,222 --> 00:34:52,326
现在音频单元托管
选择了各种策略的应用

555
00:34:52,592 --> 00:34:55,896
以便推荐如何为AU显示UI

556
00:34:56,429 --> 00:35:00,834
它们可以决定
比如说在自己的UI中嵌入AU的UI

557
00:35:01,168 --> 00:35:05,138
或它们可以为AU呈现一个全屏、
独立的UI

558
00:35:06,139 --> 00:35:09,376
现在这主要呈现为iOS设备上的挑战

559
00:35:09,443 --> 00:35:12,379
因为目前没有定义视图尺寸

560
00:35:12,713 --> 00:35:16,650
而音频单元要采用

561
00:35:16,717 --> 00:35:18,652
托管实际所选择的任意UI尺寸

562
00:35:19,853 --> 00:35:23,524
为了克服这种限制
我们添加了一种方式

563
00:35:23,590 --> 00:35:25,158
托管和AU

564
00:35:25,225 --> 00:35:29,730
可以相互协商
并且AU可以通知托管

565
00:35:29,796 --> 00:35:32,833
关于它实际所支持的全部视图配置

566
00:35:33,634 --> 00:35:36,770
现在让我们看这种协商是如何发生的

567
00:35:38,338 --> 00:35:43,977
托管首先为AU编译一个
全部可用视图配置列表

568
00:35:44,411 --> 00:35:49,283
然后把音频传递给AU

569
00:35:50,017 --> 00:35:53,587
然后AU可以通过全部这些
可用配置进行播放

570
00:35:53,654 --> 00:35:56,924
然后告诉托管

571
00:35:56,990 --> 00:35:59,059
它实际所支持的配置

572
00:35:59,660 --> 00:36:03,430
然后托管可以选择其中一个
所支持的配置

573
00:36:03,497 --> 00:36:07,267
然后它会告诉AU它最终选择的配置

574
00:36:08,335 --> 00:36:09,203
现在让我们来看看

575
00:36:09,269 --> 00:36:12,539
代码示例 关于这种协商是如何发生的

576
00:36:12,906 --> 00:36:16,009
我们首先要看音频单元扩展

577
00:36:17,878 --> 00:36:19,713
AU要做的第一件事

578
00:36:20,347 --> 00:36:21,281
就是覆盖

579
00:36:21,448 --> 00:36:25,018
基类中的所支持的视频配置方法

580
00:36:25,519 --> 00:36:27,921
这是由托管通过

581
00:36:27,988 --> 00:36:31,158
全部可用配置进行调用的

582
00:36:32,292 --> 00:36:36,163
然后AU会在每个配置中进行迭代

583
00:36:36,230 --> 00:36:38,966
并决定它实际支持哪个

584
00:36:40,000 --> 00:36:43,804
现在配置自身包含一个宽和一个高

585
00:36:44,071 --> 00:36:46,840
也就是推荐视图尺寸

586
00:36:47,040 --> 00:36:49,743
同时它还有一个托管测试控制器旗标

587
00:36:50,611 --> 00:36:54,848
并且那个旗标指明了托管是否代表

588
00:36:54,915 --> 00:36:58,519
它自己的控制器 在这个视图配置中

589
00:36:59,586 --> 00:37:01,622
那么根据全部这些因素

590
00:37:01,688 --> 00:37:05,526
AU可以选择它是否支持那个配置

591
00:37:06,860 --> 00:37:10,998
请注意有一个通配符配置是0x0

592
00:37:11,265 --> 00:37:15,068
意思是…那代表AU可以

593
00:37:15,302 --> 00:37:17,070
支持默认全屏尺寸

594
00:37:17,804 --> 00:37:23,243
在macOS上 这其实是转化到一个
独立的、可调整大小的窗口…

595
00:37:23,310 --> 00:37:27,447
全尺寸、可调整大小的窗口
作为AU的UI

596
00:37:29,216 --> 00:37:33,453
那么AU有自己的逻辑
来决定它支持哪个配置

597
00:37:33,520 --> 00:37:36,590
那么最终它对比列有各种指标的列表

598
00:37:38,225 --> 00:37:40,127
和它所支持的配置

599
00:37:40,194 --> 00:37:43,864
并将这个指标组合写入到托管中

600
00:37:45,399 --> 00:37:49,937
AU要做的最后一件事就是
覆盖选择方法

601
00:37:50,170 --> 00:37:52,873
由托管通过

602
00:37:52,940 --> 00:37:54,775
它最终所选择的配置进行调用

603
00:37:54,942 --> 00:37:56,310
然后AU

604
00:37:56,376 --> 00:37:58,178
可以让其视图控制器

605
00:37:58,245 --> 00:38:00,414
了解最终所选择的配置

606
00:38:02,316 --> 00:38:06,086
现在让我们进入托管看一下代码

607
00:38:07,621 --> 00:38:11,859
托管必须编译可用配置的列表

608
00:38:11,925 --> 00:38:13,694
在本例中 它指明

609
00:38:13,861 --> 00:38:17,097
它拥有一个大配置和一个小配置

610
00:38:18,031 --> 00:38:20,801
在最后一个配置中 托管表明

611
00:38:20,868 --> 00:38:25,672
它没有呈现其控制器
那么托管的控制器旗子标记就为假

612
00:38:25,973 --> 00:38:29,910
在小配置中 托管呈现了它的控制器

613
00:38:29,977 --> 00:38:31,111
那么旗标就为真

614
00:38:32,913 --> 00:38:37,918
然后托管调用AU上
所支持的视图配置方法

615
00:38:37,985 --> 00:38:40,521
并提供这份配置列表

616
00:38:40,888 --> 00:38:44,224
根据所返回的指标组合

617
00:38:44,291 --> 00:38:46,426
它会选择其中一项配置

618
00:38:46,627 --> 00:38:49,596
在本例中 托管只是

619
00:38:49,663 --> 00:38:52,099
在大配置和小配置之间切换

620
00:38:53,700 --> 00:38:54,868
那么这就是

621
00:38:54,935 --> 00:38:57,271
优选视图配置协商

622
00:38:57,704 --> 00:39:00,874
现在让我们看看第二个主要的新功能

623
00:39:01,208 --> 00:39:04,611
即在音频单元扩展中支持MIDI输出

624
00:39:05,913 --> 00:39:09,416
我们现在支持AU进行

625
00:39:09,483 --> 00:39:11,385
通过其音频输出同步后的MIDI输出

626
00:39:12,019 --> 00:39:16,223
主要用处是如果托管想记录和编辑

627
00:39:16,290 --> 00:39:19,193
AU中的MIDI性能

628
00:39:19,259 --> 00:39:20,327
以及音频输出

629
00:39:20,961 --> 00:39:24,398
那么托管安装了MIDI输出事件组块

630
00:39:24,464 --> 00:39:25,332
在AU上

631
00:39:25,399 --> 00:39:28,902
AU应该在每个渲染循环中
调用这个组块

632
00:39:29,069 --> 00:39:32,472
并为该渲染循坏提供MIDI输出

633
00:39:34,842 --> 00:39:37,344
我们还有一些其它改进

634
00:39:37,411 --> 00:39:39,046
在音频工具箱框架中

635
00:39:39,112 --> 00:39:42,382
第一个是与隐私权有关

636
00:39:43,083 --> 00:39:48,589
那么从iOS 11 SDK开始
全部音频单元扩展托管应用

637
00:39:48,655 --> 00:39:52,459
都需要应用间音频权利来与

638
00:39:52,526 --> 00:39:54,328
音频单元扩展进行通讯

639
00:39:55,162 --> 00:39:59,933
我们还有一个新API
可以让AU发布

640
00:40:00,000 --> 00:40:04,204
一个非常有意义的短名称
因此托管比如说

641
00:40:04,271 --> 00:40:07,708
可以使用这个短名称 如果它要显示

642
00:40:08,308 --> 00:40:13,547
AU名称列表的话 在空间约束列表中

643
00:40:15,182 --> 00:40:19,720
那么这就是音频工具箱框架中的
全部改进

644
00:40:19,786 --> 00:40:24,424
正如我所承诺的
我们要实际操作演示一下这些新功能

645
00:40:24,491 --> 00:40:26,560
让我们邀请Bela上台

646
00:40:37,004 --> 00:40:40,507
谢谢Akshatha 大家下午好
我叫Bela Balazs

647
00:40:40,574 --> 00:40:42,609
我是Core Audio团队的一名工程师

648
00:40:43,177 --> 00:40:46,880
今天我要给大家展示

649
00:40:46,947 --> 00:40:50,651
一个应用
主要了解我们新引入的API

650
00:40:51,118 --> 00:40:54,221
为此 我们开发了一个示例音频单元

651
00:40:54,288 --> 00:40:56,123
有如下功能

652
00:40:57,157 --> 00:40:59,860
它支持其优选视图配置

653
00:40:59,927 --> 00:41:01,695
通过音频单元托管应用

654
00:41:02,563 --> 00:41:04,798
它支持多视图配置

655
00:41:04,865 --> 00:41:09,036
并使用新桥接的MIDI输出API

656
00:41:09,102 --> 00:41:10,938
来传递MIDI数据

657
00:41:11,405 --> 00:41:14,808
到音频单元托管应用 用于录制音频

658
00:41:15,742 --> 00:41:18,612
那么在这里我有一个
GarageBand的升级版

659
00:41:19,346 --> 00:41:22,249
我已经把我的示例音频单元
加载到了一个音轨中

660
00:41:23,350 --> 00:41:26,486
在这里你可以看到我的
音频单元的自定义视图

661
00:41:26,553 --> 00:41:28,355
和GarageBand键盘

662
00:41:28,655 --> 00:41:30,524
在这个重配置中

663
00:41:30,591 --> 00:41:33,594
我依赖GarageBand键盘
播放我的伴奏

664
00:41:34,228 --> 00:41:37,030
我在键盘上制定了三种鼓声示例

665
00:41:37,097 --> 00:41:40,868
有底鼓、军鼓和踩镲

666
00:41:41,502 --> 00:41:44,571
除了这些 在我的音频单元视图上

667
00:41:44,638 --> 00:41:48,542
我还有一个音量滑块
用于控制这些示例的音量

668
00:41:51,478 --> 00:41:55,482
然而 我的音频单元
还有一个不同的视图配置

669
00:41:55,549 --> 00:41:59,353
我可以通过右边这个新添加的按钮
进行切换…

670
00:41:59,419 --> 00:42:01,588
在屏幕的右下方

671
00:42:02,122 --> 00:42:06,560
当我激活那个按钮后
我会进入音频单元的大视图中

672
00:42:06,627 --> 00:42:08,662
而GarageBand键盘就消失了

673
00:42:09,329 --> 00:42:13,534
当我再次激活它时
我又返回到了音频单元的小视图中

674
00:42:14,101 --> 00:42:16,770
这是通过GarageBand

675
00:42:17,571 --> 00:42:21,175
把全部可用视图配置
发布到我的音频单元实现的

676
00:42:21,675 --> 00:42:24,711
并且我的音频单元查看那份列表
并把每一项标记为

677
00:42:24,778 --> 00:42:26,980
支持或不支持

678
00:42:27,047 --> 00:42:30,417
在这个过程的最后
GarageBand就会了解我的音频单元

679
00:42:30,484 --> 00:42:34,354
支持两种视图配置
并且它可以在这两者之间进行切换

680
00:42:35,189 --> 00:42:38,692
如果我的音频单元只支持一种视图配置

681
00:42:38,759 --> 00:42:41,495
GarageBand就会
隐藏这个按钮

682
00:42:41,562 --> 00:42:45,599
但我的音频组件仍然会利用协商过程

683
00:42:45,666 --> 00:42:49,536
来协商该视图的优选视图配置

684
00:42:50,804 --> 00:42:55,275
在这个小视图中
托管把控制器旗标设为真

685
00:42:56,009 --> 00:42:58,345
这也是GarageBand键盘
可见的原因

686
00:42:58,579 --> 00:43:00,614
在较大的视图配置中

687
00:43:00,681 --> 00:43:04,718
GarageBand键盘是隐藏的
因为那个旗标被设为假

688
00:43:05,252 --> 00:43:09,590
在这个视图配置中
我的音频单元有自己的播放界面

689
00:43:09,990 --> 00:43:11,825
我可以用于播放我的伴奏

690
00:43:12,059 --> 00:43:15,162
有底鼓、军鼓和踩镲

691
00:43:15,996 --> 00:43:19,433
除了这三个按钮 我还有

692
00:43:20,767 --> 00:43:23,837
一个新按钮 位于右手边
叫作重复音符

693
00:43:24,071 --> 00:43:25,138
这个按钮可以让我

694
00:43:25,205 --> 00:43:28,208
以指定速率重复每个示例

695
00:43:28,709 --> 00:43:32,713
我可以通过滑块分别设置每一个的速率

696
00:43:36,617 --> 00:43:39,553
我还可以把每个示例从鼓声循环中
拿出来或放进去

697
00:43:51,265 --> 00:43:53,767
这就允许我很简便地构建鼓声循环

698
00:43:53,834 --> 00:43:56,670
使其遵从我的音轨的速度

699
00:43:57,738 --> 00:44:01,275
让我们使用MIDI输出API来录制

700
00:44:01,341 --> 00:44:03,410
这个音频单元扩展的输出

701
00:44:04,044 --> 00:44:09,049
这里有一个同步速率按钮
把我的速率设置为110 BPM

702
00:44:09,750 --> 00:44:13,287
首先我将录制一个底鼓
和军鼓的鼓声循环

703
00:44:13,387 --> 00:44:16,790
然后当循环录制完成后 我再添加踩镲

704
00:44:17,224 --> 00:44:20,928
这是通过GarageBand的
合并录音功能实现的

705
00:44:21,662 --> 00:44:23,130
那么让我们来试试

706
00:44:27,000 --> 00:44:30,003
对于前两个 我只录制四条

707
00:44:35,175 --> 00:44:36,510
然后添加踩镲

708
00:44:45,152 --> 00:44:47,321
踩镲已经被添加到了录音中

709
00:44:49,456 --> 00:44:53,193
现在我们可以进入音轨视图
并查看录音媒体输出

710
00:44:57,264 --> 00:44:59,233
我可以量化音轨

711
00:45:02,803 --> 00:45:04,438
然后我可以播放它

712
00:45:06,640 --> 00:45:10,277
我们拥有GarageBand的全部
MIDI编辑功能

713
00:45:10,344 --> 00:45:13,614
可任由我们用来构建我们的鼓声音轨

714
00:45:14,381 --> 00:45:16,984
这就是我的演示 谢谢大家的关注

715
00:45:17,050 --> 00:45:19,453
我要把舞台交还给我的同事Akshatha

716
00:45:19,853 --> 00:45:20,687
谢谢

717
00:45:26,026 --> 00:45:27,027
谢谢Bela

718
00:45:29,897 --> 00:45:33,634
现在让我们讲音频工具箱框架中的
最后一组改进

719
00:45:33,700 --> 00:45:35,269
与音频格式有关

720
00:45:36,637 --> 00:45:39,740
我们目前支持两种流行格式

721
00:45:39,806 --> 00:45:42,409
分别是FLAC和Opus格式

722
00:45:43,110 --> 00:45:47,147
关于FLAC 我们有编解码器、
文件和数据流支持

723
00:45:47,214 --> 00:45:50,450
关于Opus
我们有编解码器和文件I/O支持

724
00:45:50,517 --> 00:45:52,419
通过编码音频格式容器

725
00:45:54,621 --> 00:45:57,157
从音频格式到空间音频格式

726
00:45:57,658 --> 00:46:00,260
你们中对[听不清]音频、

727
00:46:00,327 --> 00:46:05,566
AR和VR应用感兴趣的人 可能很高兴
听到我们支持环绕声的消息

728
00:46:06,099 --> 00:46:10,204
对于不熟悉环绕声的人 就像我一样

729
00:46:10,737 --> 00:46:12,773
环绕声还是一种

730
00:46:12,840 --> 00:46:15,909
多波段格式 但不同点是

731
00:46:15,976 --> 00:46:21,548
我们所了解的传统环绕立体声格式
比如5.1或7.1

732
00:46:21,915 --> 00:46:25,118
其信号实际上代表着扬声器布局

733
00:46:25,586 --> 00:46:27,087
而环绕声

734
00:46:27,421 --> 00:46:31,658
提供一个独立于扬声器之外的
声音种子代表

735
00:46:32,259 --> 00:46:35,495
它们实质上[听不清]来自播放系统

736
00:46:36,163 --> 00:46:37,898
渲染时

737
00:46:37,965 --> 00:46:41,835
就是它们被解码到
听者的扬声器组合中时

738
00:46:42,336 --> 00:46:45,439
这就为内容创建者提供了更多的灵活性

739
00:46:46,440 --> 00:46:48,041
我们现在支持第一级

740
00:46:48,108 --> 00:46:53,647
环绕声叫作B-格式
而更高级的环绕声

741
00:46:53,714 --> 00:46:57,885
为序号N 范围可以从1到254

742
00:46:58,318 --> 00:47:02,422
根据顺序 波段自身可以从零…

743
00:47:02,489 --> 00:47:07,127
环绕声波段编号可以从零
一直到65,024

744
00:47:07,828 --> 00:47:11,965
我们支持两种流行的标准化数据流

745
00:47:12,032 --> 00:47:14,768
分别是SN3D和N3D数据流

746
00:47:14,835 --> 00:47:20,707
并且我们支持把环绕声
解码到任意扬声器布局

747
00:47:20,774 --> 00:47:24,278
并在B-格式和这些标准化数据流
之间进行转换

748
00:47:26,280 --> 00:47:27,481
最后一个改进

749
00:47:27,548 --> 00:47:30,317
是关于AU空间混音器

750
00:47:30,517 --> 00:47:33,687
那么这是Apple内置空间混音器

751
00:47:33,754 --> 00:47:36,423
用于3D音频空间化

752
00:47:37,057 --> 00:47:39,526
并且AVAudio环境节点

753
00:47:39,593 --> 00:47:44,464
即AVAudio Engine中的
一个节点也在底层使用空间混音器

754
00:47:44,865 --> 00:47:48,969
并且我们现在有一个新的渲染算法
在这个空间混音器中

755
00:47:49,036 --> 00:47:52,239
叫作HRTFHQ 高品质

756
00:47:52,573 --> 00:47:57,077
这与当前现有的HRTF算法不同

757
00:47:57,144 --> 00:47:59,713
它拥有更好的频率响应

758
00:47:59,780 --> 00:48:02,583
和更好的资源本地化 在3D空间中

759
00:48:03,817 --> 00:48:07,588
那么这就是
音频工具箱框架中的全部改进

760
00:48:07,654 --> 00:48:10,958
现在我要把舞台交给Torrey

761
00:48:11,024 --> 00:48:14,361
让他给大家讲讲
设备间音频模式的更新

762
00:48:20,567 --> 00:48:21,535
谢谢Akshatha

763
00:48:21,835 --> 00:48:24,338
我是Torrey Holbrook Walker
今天我很高兴跟大家谈谈

764
00:48:24,404 --> 00:48:25,973
设备间音频模式

765
00:48:26,039 --> 00:48:28,909
或者如果你想酷点儿
你可以简称它为IDAM

766
00:48:29,276 --> 00:48:30,511
你要记住这个词IDAM

767
00:48:30,611 --> 00:48:34,748
你拿着你的iOS设备 把它查到Mac上
打开Audio MIDI组合

768
00:48:34,815 --> 00:48:37,584
然后它会在音频设备窗口中显示

769
00:48:37,651 --> 00:48:41,088
你可以 它旁边有一个启动按钮
如果你点击它 梆

770
00:48:41,255 --> 00:48:44,224
你会立即获得录制数字音频的功能

771
00:48:44,291 --> 00:48:47,628
通过设备自带的USB闪电数据线

772
00:48:47,694 --> 00:48:51,498
它看起来就像是对Mac托管的一个
USB音频输入

773
00:48:51,565 --> 00:48:53,166
那么它使用了同样的驱动器

774
00:48:53,233 --> 00:48:55,769
同样的低延迟驱动器
与在MacOS 4上所使用的一样

775
00:48:55,836 --> 00:48:57,337
class-compliant音频设备

776
00:48:57,571 --> 00:48:59,239
你自El Capitan和iOS 9

777
00:48:59,306 --> 00:49:01,642
起就可以实现了

778
00:49:02,276 --> 00:49:03,410
嗯 今天我们要

779
00:49:03,477 --> 00:49:06,046
向IDAM依依惜别了

780
00:49:06,113 --> 00:49:08,982
那么再见了IDAM 拜拜了IDAM

781
00:49:09,449 --> 00:49:12,252
当你挥手告别时
记得跟IDAM打招呼

782
00:49:12,319 --> 00:49:13,720
设备间音频和MIDI

783
00:49:14,388 --> 00:49:18,125
今年我们给IDAM配置
添加了MIDI

784
00:49:18,192 --> 00:49:22,496
那会允许你发送或接收音乐伴奏数据

785
00:49:22,563 --> 00:49:25,966
到你的iOS设备
通过设备自带的同一根线

786
00:49:26,200 --> 00:49:31,638
又是class-compliant
那么在iOS上 你会看到

787
00:49:31,705 --> 00:49:33,874
MIDI源和目标代表Mac

788
00:49:33,941 --> 00:49:37,845
在Mac上 你会看到源
和目标代表你的iOS设备

789
00:49:38,445 --> 00:49:43,050
现在这需要iOS 11才行
但你可以

790
00:49:43,116 --> 00:49:46,453
在老MacOS El Capitan或更新版本上
实现 因为它是个class-compliant实施

791
00:49:46,854 --> 00:49:48,789
并且你不需要做什么就能获取MIDI

792
00:49:48,856 --> 00:49:50,357
你会自动获取它

793
00:49:50,424 --> 00:49:53,193
无论何时你通过点击启用进入项配置时

794
00:49:53,360 --> 00:49:55,863
你需要做什么来让你的应用支持它吗？

795
00:49:55,929 --> 00:49:58,131
不 如果它支持MIDI
它就那么起作用了

796
00:49:58,999 --> 00:50:01,768
当你在IDAM配置中时 你的设备

797
00:50:01,835 --> 00:50:03,303
将可以充电和同步

798
00:50:03,370 --> 00:50:06,907
但你将暂时失去图片导入和渲染的功能

799
00:50:07,074 --> 00:50:09,076
你可以通过点击禁用按钮
再次获得这两个功能

800
00:50:09,142 --> 00:50:10,911
或在Mac上带电插拔设备

801
00:50:11,545 --> 00:50:14,715
那个输入 它的音频输入可能是聚合的

802
00:50:14,781 --> 00:50:18,085
所以如果你拥有多台iOS设备
就像我一样 假如

803
00:50:18,151 --> 00:50:22,155
你的iPhone和iPad以及你孩子的iPad
你可以比如说

804
00:50:22,222 --> 00:50:25,659
在全部三台设备上
启用IDAM配置并将其聚合到

805
00:50:25,726 --> 00:50:29,496
你的数字音频工作站可以查找到的
单一的六声道音频输入设备

806
00:50:30,197 --> 00:50:32,533
因为MIDI通讯是双向的

807
00:50:32,599 --> 00:50:35,102
你可以把它用作…你可以指明比如说

808
00:50:35,169 --> 00:50:41,208
“发送MIDI到合成器应用”
并记录返回的音频

809
00:50:41,441 --> 00:50:44,611
或者你可以为iPad设计一个
MIDI控制器应用

810
00:50:44,678 --> 00:50:48,048
那块神奇的玻璃
你可以用它来控制你的[听不清]

811
00:50:48,382 --> 00:50:50,617
但演讲总是说得简单
而演示会身临其境

812
00:50:50,851 --> 00:50:52,352
那么让我们实际操作一下

813
00:50:54,788 --> 00:50:57,224
那么在我实际打开我的演示机器之前

814
00:50:57,291 --> 00:51:01,061
我想给你们展示一个
我即将要使用的应用

815
00:51:02,296 --> 00:51:04,565
它叫作Feud Machine

816
00:51:05,065 --> 00:51:07,501
我已打开了Feud Machine

817
00:51:07,668 --> 00:51:12,039
在Feud Machine上
有一个多播放头MIDI音序器

818
00:51:12,372 --> 00:51:14,508
那意味着你可以使用

819
00:51:14,575 --> 00:51:16,710
一个MIDI音序和不同的播放头

820
00:51:16,777 --> 00:51:20,948
也许在不同的时间移动
朝不同的方向移动

821
00:51:21,014 --> 00:51:26,887
并使用它来创建复杂的和音
通过定相和定时关联

822
00:51:27,287 --> 00:51:29,590
那么我要播放这里的这个模型

823
00:51:32,993 --> 00:51:36,230
并且有大量播放头 我要停止其中一些

824
00:51:36,964 --> 00:51:38,165
那么只有一个了

825
00:51:40,133 --> 00:51:41,068
我要再添加一个

826
00:51:42,536 --> 00:51:43,403
再添加一个

827
00:51:44,605 --> 00:51:47,941
再添加一个 正如你所看到的
我们可以用这种方式很便利地创建和音

828
00:51:48,642 --> 00:51:51,678
那么还有其它模型可以使用 比如

829
00:51:51,745 --> 00:51:52,880
这个叫作“Dotted”

830
00:51:55,415 --> 00:51:56,450
这个叫“Triplet”

831
00:51:57,985 --> 00:52:01,555
但我们仍然采用这个 我们要用这个

832
00:52:01,622 --> 00:52:04,091
实际控制一个
我们正在Logic中使用的项目

833
00:52:04,358 --> 00:52:06,126
那么现在我要返回我的演示机器

834
00:52:06,793 --> 00:52:08,128
我要点击这里的启用

835
00:52:09,496 --> 00:52:11,765
我将看到它作为USB音频输入出现

836
00:52:11,832 --> 00:52:15,802
如果我查看MIDI工作室窗口
我还会看到它在这里显示

837
00:52:15,869 --> 00:52:18,572
为MIDI源和目标
我可以在Logic中使用

838
00:52:18,939 --> 00:52:21,375
那么如果我启动一个
我正在这里使用的项目…

839
00:52:26,747 --> 00:52:28,682
现在这是一个很短的四条循环

840
00:52:28,749 --> 00:52:33,120
是用于游戏得分屏幕上的
那么在这个视频游戏等级结束后

841
00:52:33,187 --> 00:52:36,623
玩家可以查看他们的得分
并且他们会听这个循环

842
00:52:37,758 --> 00:52:41,195
而这个循环现在 在我向里边添加
任何东西之前 听起来是这样的

843
00:52:52,873 --> 00:52:55,375
现在我想在其中一部分中添加和音

844
00:52:55,576 --> 00:52:58,679
那么我所要做的就是
双击这里添加另一个音轨

845
00:52:58,946 --> 00:53:03,417
我要选择一个和音
也许是某些像方波一样的东西

846
00:53:06,653 --> 00:53:09,289
好了 我要在这里执行打击乐方波

847
00:53:09,389 --> 00:53:11,825
在声波条上 你实际可以看到一个和音

848
00:53:11,892 --> 00:53:14,561
我不需要那个
因为我要用Feud Machine播放

849
00:53:14,928 --> 00:53:19,533
那么如果我启用这个
并配置好这里的音序

850
00:53:19,933 --> 00:53:25,439
我就能听到Feud Machine
在Logic中播放软音源

851
00:53:26,039 --> 00:53:27,140
那么我要独奏

852
00:53:30,244 --> 00:53:32,779
这个是四个播放头同时移动

853
00:53:32,946 --> 00:53:36,517
我要把它们关掉 我只留下一个播放头
如果我想的话

854
00:53:37,618 --> 00:53:39,152
或最多可以有四个

855
00:53:39,620 --> 00:53:42,456
那么我要把这个录到我的音轨中

856
00:53:42,523 --> 00:53:44,925
我们要看看在情境中听起来如何

857
00:53:51,565 --> 00:53:56,303
哎呦 抱歉 我要先记录配置并播放

858
00:54:07,581 --> 00:54:08,949
好的那么

859
00:54:10,083 --> 00:54:13,086
我已经记录了这里的自动操作

860
00:54:13,620 --> 00:54:16,790
并且我可以使用这个自动操作 并能

861
00:54:18,325 --> 00:54:21,328
从这里的iPad中播放 那么如果
我在情境中听 听起来是这样的

862
00:54:24,498 --> 00:54:26,300
那么现在我已经搞好了MIDI…

863
00:54:26,366 --> 00:54:28,435
一个MIDI开始命令
进入了Feud Machine

864
00:54:28,502 --> 00:54:30,571
Feud Machine
正在播放我们的软音源

865
00:54:30,904 --> 00:54:33,607
在这里对于录音 我有一些自动操作

866
00:54:34,741 --> 00:54:39,546
这就是IDAM之上的
MIDI配置的演示

867
00:54:40,714 --> 00:54:42,015
让我们返回看幻灯片

868
00:54:46,720 --> 00:54:48,288
好的 我们今天讲了很多内容

869
00:54:48,355 --> 00:54:50,290
我们讲了AVAudio Engine的改进

870
00:54:50,357 --> 00:54:53,160
包括手动渲染 你现在可以脱机进行了

871
00:54:53,227 --> 00:54:54,394
或者也可以实时进行

872
00:54:54,728 --> 00:54:58,332
有了AirPlay 2支持 关于AirPlay 2
将有另一场完全不同的演讲

873
00:54:58,398 --> 00:55:00,167
即将举办

874
00:55:00,234 --> 00:55:02,169
请确保参看那场演讲
如果你感兴趣的话

875
00:55:02,369 --> 00:55:05,906
Watch OS 4 你现在可以录音了
我们谈了功能

876
00:55:05,973 --> 00:55:08,342
和限制还有政策

877
00:55:08,742 --> 00:55:11,411
对于AUAudio单元
你现在可以协商视图配置了

878
00:55:11,478 --> 00:55:15,249
并且你可以与你的音频
输出同步AU的MIDI输出

879
00:55:15,716 --> 00:55:18,585
我们谈了一些其它的音频改进 包括

880
00:55:18,652 --> 00:55:22,723
支持的新格式、环绕声、
与播放头相关的转换功能

881
00:55:22,789 --> 00:55:24,825
并且我们还谈了IDAM
圆满结束了演讲

882
00:55:24,892 --> 00:55:27,160
IDAM代表设备间音频和MIDI

883
00:55:27,761 --> 00:55:30,664
关于本场演讲的主要信息URL

884
00:55:30,731 --> 00:55:32,132
在这里

885
00:55:33,934 --> 00:55:37,938
如果你对音频有兴趣
你可能还有兴趣参加

886
00:55:38,005 --> 00:55:41,708
本周稍后举办的这些相关演讲

887
00:55:43,443 --> 00:55:45,345
谢谢大家花时间
参加和关注我们的演讲

888
00:55:45,412 --> 00:55:47,080
祝你们在大会期间有一段美妙时光

