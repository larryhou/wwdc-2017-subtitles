1
00:00:27,060 --> 00:00:27,995
大家早上好

2
00:00:28,629 --> 00:00:31,765
我是Krishna
我来自Core ML工程师团队

3
00:00:32,031 --> 00:00:35,602
今天我要跟大家谈谈
Core ML的深度解析

4
00:00:37,504 --> 00:00:39,506
今年我们引入了Core ML

5
00:00:39,806 --> 00:00:43,343
这是整合机器学习模型最简单的方式

6
00:00:43,544 --> 00:00:44,578
在你的应用中

7
00:00:45,579 --> 00:00:48,615
Core ML在macOS、

8
00:00:49,049 --> 00:00:52,953
iOS、watchOS
和tvOS上均可用

9
00:00:53,987 --> 00:00:56,924
周二有一场介绍Core ML的演讲

10
00:00:57,357 --> 00:00:59,126
如果你们没参加那场演讲

11
00:00:59,226 --> 00:01:00,594
那就让我们花一点时间

12
00:01:00,661 --> 00:01:03,397
回顾一下那场演讲中的一些关键点

13
00:01:04,765 --> 00:01:09,603
关于Core ML
首先也是最重要的就是你可以

14
00:01:09,670 --> 00:01:12,606
把你的机器学习模型当作代码一样

15
00:01:13,173 --> 00:01:16,743
你与它们的交互
也就像与其它Swift类一样

16
00:01:18,278 --> 00:01:20,514
你的流程应该是这样的

17
00:01:21,148 --> 00:01:25,586
你从机器学习模型开始
把那个模型拖拽到Xcode中

18
00:01:26,320 --> 00:01:30,924
Xcode将自动生成一个Swift
或Objective-C界面

19
00:01:31,158 --> 00:01:32,960
你可以针对那个模型进行编程

20
00:01:33,727 --> 00:01:36,163
你写好应用代码并创建它

21
00:01:37,030 --> 00:01:41,134
Xcode将在你的应用中
把代码和模型捆绑到一起

22
00:01:43,337 --> 00:01:47,040
在那场演讲中
我们还看了一个花语的小演示

23
00:01:47,841 --> 00:01:50,110
那是一个应用 给你一张图片

24
00:01:50,277 --> 00:01:54,081
比如 这种粉红玫瑰
应用就会告诉你

25
00:01:54,147 --> 00:01:56,950
它是什么花以及它所代表的含义

26
00:01:58,519 --> 00:02:01,755
我们看那个演示的目的是为了
使用或创建那个应用

27
00:02:01,889 --> 00:02:03,457
它只有几行代码

28
00:02:04,124 --> 00:02:06,693
一行代码是实例化模型

29
00:02:07,394 --> 00:02:10,229
一行代码是从那个模型中进行阐释

30
00:02:11,832 --> 00:02:14,735
那么在本场演讲中
我们要接着上次的演讲继续讲

31
00:02:15,135 --> 00:02:17,938
我们要更多地谈谈不同的用例

32
00:02:18,005 --> 00:02:20,307
你们可以通过Core ML
实现的一些很酷的功能

33
00:02:21,975 --> 00:02:24,745
然后谈谈Core ML是如何

34
00:02:24,811 --> 00:02:27,314
针对它所运行或它所在运行的硬件
进行优化的

35
00:02:27,514 --> 00:02:29,683
以及这对于你们开发人员
来说意味着什么

36
00:02:31,351 --> 00:02:34,555
最后我们谈谈如何获取Core ML模型

37
00:02:34,621 --> 00:02:36,123
以在你的应用中使用

38
00:02:37,324 --> 00:02:40,761
那么这将是一场很有意思的演讲
有一些演示 让我们开始吧

39
00:02:42,262 --> 00:02:45,599
那么你已经看到了一个
使用图片的应用示例

40
00:02:45,799 --> 00:02:46,767
花语应用

41
00:02:47,301 --> 00:02:49,837
但通过Core ML你可以实现
更多功能而不只是图片

42
00:02:50,637 --> 00:02:52,139
你可以处理手势

43
00:02:52,472 --> 00:02:54,641
比如说手表上的手写检测

44
00:02:55,776 --> 00:02:56,877
你还可以处理视频

45
00:02:56,977 --> 00:02:58,745
比如说你想实现信用卡检测

46
00:02:59,880 --> 00:03:04,084
你可以处理音频并且甚至可以处理文本

47
00:03:05,352 --> 00:03:08,155
现在通过输入全部这些不同的类型

48
00:03:08,488 --> 00:03:10,891
你可以创建各种各样的应用

49
00:03:12,292 --> 00:03:15,462
比如你想创建一个应用
当你键入某些文本时

50
00:03:15,796 --> 00:03:18,665
它会告诉你是令人高兴的文本
还是令人沮丧的文本

51
00:03:18,765 --> 00:03:21,168
或者消极的有侵略性的文本
或令人愤怒的文本

52
00:03:22,202 --> 00:03:25,739
你可以通过情感分析来实现
我们今天要看一个这样的小演示

53
00:03:27,708 --> 00:03:30,844
你可以…通过风格转换
你甚至可以让你的家庭照片

54
00:03:30,911 --> 00:03:32,546
看起来像是Vincent Van Gogh的油画

55
00:03:34,081 --> 00:03:35,516
而通过手势识别

56
00:03:35,616 --> 00:03:38,752
你可以有一种全新的方式
把数据输入到你的应用中

57
00:03:39,620 --> 00:03:42,256
现在这些都是可以实现的

58
00:03:42,689 --> 00:03:46,560
因为你可以通过Core ML
使用大量的模型

59
00:03:47,661 --> 00:03:49,630
你可以使用经典的机器学习模型

60
00:03:49,696 --> 00:03:53,200
比如广义线性模型、树和支持向量机

61
00:03:53,901 --> 00:03:57,037
这些都很棒 因为它们很小

62
00:03:57,204 --> 00:04:00,174
你可以通过它们进行快速预测
并且它们可以在任何设备上运行

63
00:04:00,941 --> 00:04:05,012
但你还可以处理各种神经网络

64
00:04:05,579 --> 00:04:07,948
我们支持30多种层类型

65
00:04:08,015 --> 00:04:09,283
这已经是很多了

66
00:04:09,983 --> 00:04:12,886
你可以做很多事
比如正反馈和卷积线性网络

67
00:04:12,953 --> 00:04:15,455
图片和基于视频的应用都可以实现

68
00:04:15,956 --> 00:04:18,257
并且你还可以做比如说循环神经网络

69
00:04:18,325 --> 00:04:21,094
或LSDM
基于文本的应用都可以实现

70
00:04:22,029 --> 00:04:24,264
我们要看一下循环神经网络

71
00:04:24,331 --> 00:04:26,066
在今天的演讲中

72
00:04:27,835 --> 00:04:31,905
事实上通过Core ML
你还可以结合不同类型的模型

73
00:04:32,239 --> 00:04:34,441
那么你可以利用比如神经网络

74
00:04:34,508 --> 00:04:38,278
并把它与树结合在一起
然后你可以得到一个大模型

75
00:04:38,612 --> 00:04:40,380
这个概念就叫作管道

76
00:04:42,649 --> 00:04:45,452
但最重要的是 对于你们开发人员

77
00:04:45,552 --> 00:04:48,488
我们希望你们注重
你们写进应用中的代码

78
00:04:48,622 --> 00:04:51,925
而不是应用所运行的模型的复杂程度

79
00:04:54,294 --> 00:04:58,232
我们通过为你们提供
功能性抽象化查看器模型实现

80
00:04:58,799 --> 00:05:02,202
所以你们需要关注的是
你的模型有预测功能

81
00:05:02,269 --> 00:05:05,272
获取一些输入并给出一些输出

82
00:05:06,507 --> 00:05:09,710
并且这些输入和输出有五种不同类型：

83
00:05:10,410 --> 00:05:16,149
数值型、分类型、图片型、
数组型和词典型

84
00:05:17,484 --> 00:05:20,954
现在让我们分别看一下每种类型

85
00:05:22,489 --> 00:05:25,559
那么数值型和分类型在Swift中有

86
00:05:25,759 --> 00:05:28,095
双精度浮点型、整型或字符串型

87
00:05:28,295 --> 00:05:29,296
所以它很自然

88
00:05:30,030 --> 00:05:32,399
我们有一个小应用示例

89
00:05:32,466 --> 00:05:35,135
用了这两种类型
在developer.apple.com上

90
00:05:35,569 --> 00:05:38,205
那是一个预测房屋价格的应用

91
00:05:39,006 --> 00:05:41,542
所以这个模型的某些输入是数值型

92
00:05:41,675 --> 00:05:44,711
就是指明它们是连续的
所以你可以从零到无限大

93
00:05:45,612 --> 00:05:48,515
而某些是分类型或不连续型

94
00:05:48,749 --> 00:05:50,884
所以是0、1、2、3、4

95
00:05:53,153 --> 00:05:55,589
我们已经看过了一个使用图片的示例了

96
00:05:55,956 --> 00:05:59,026
现在这些图片是作为CVPixelBuffers
展示给你的

97
00:06:01,328 --> 00:06:04,264
要获取更复杂的功能
比如手势、音频和视频

98
00:06:04,331 --> 00:06:10,270
我们有一个新类型 叫作MLMultiArray
可以封装一个多维数组

99
00:06:11,805 --> 00:06:14,241
对于大量基于文本的应用

100
00:06:14,474 --> 00:06:16,143
你可以通过词典进行交互

101
00:06:16,877 --> 00:06:17,711
这是词典

102
00:06:17,778 --> 00:06:21,615
键是字符串型或整型
而值是双精度浮点型

103
00:06:23,383 --> 00:06:26,787
让我们看看输入文本
并通过词典起作用的功能

104
00:06:28,956 --> 00:06:32,960
我们要通过一个情感分析应用来看

105
00:06:33,894 --> 00:06:36,330
现在我想让这个应用
实现当我键入文本时

106
00:06:36,396 --> 00:06:39,600
我希望UI弹出并反映我现在的情绪

107
00:06:40,033 --> 00:06:42,936
如果我说“Core ML真棒！
我喜欢用它！”

108
00:06:43,136 --> 00:06:45,572
我希望它变成绿色且我想要一个
比如 一个笑脸

109
00:06:46,473 --> 00:06:50,143
如果我跟你说比如午餐是如何糟糕

110
00:06:50,277 --> 00:06:51,912
“今天午餐真令人失望和伤心”

111
00:06:51,979 --> 00:06:54,648
我希望它变成红色
所以这是我希望它能实现的功能

112
00:06:55,916 --> 00:06:59,186
那么需要如何创建一个这样的应用呢？

113
00:06:59,520 --> 00:07:03,223
那么我要从应用shell开始
用户可以键入一些文本

114
00:07:04,691 --> 00:07:08,462
一旦用户点击空格键
它就会获取那些文本

115
00:07:08,996 --> 00:07:12,566
把它提交给机器学习模型
并返回一个情感预测

116
00:07:13,600 --> 00:07:14,635
那么情感预测

117
00:07:14,701 --> 00:07:17,538
可以是高兴、中性或伤心

118
00:07:18,739 --> 00:07:20,340
一旦我得到这个预测

119
00:07:20,474 --> 00:07:23,243
我就返回并快速更新UI

120
00:07:23,377 --> 00:07:24,578
以反映我现在的情绪

121
00:07:25,612 --> 00:07:27,481
但你需要注意一个最重要的事

122
00:07:27,614 --> 00:07:32,019
就是这些全部会在设备上实时发生
当用户正在键入时

123
00:07:32,352 --> 00:07:34,154
所以它打造了一个很棒的体验

124
00:07:36,056 --> 00:07:38,492
让我们看看如何着手创建那个应用

125
00:07:39,326 --> 00:07:41,461
嗯 最重要的东西就是模型

126
00:07:42,196 --> 00:07:44,665
对于这个应用
我们要使用一个情感分析模型

127
00:07:45,399 --> 00:07:48,969
但这个情感分析模型要根据字数来操作

128
00:07:49,236 --> 00:07:51,205
不是原始文本 而是字数统计

129
00:07:52,072 --> 00:07:54,942
那么这些字数将被呈现为词典

130
00:07:55,876 --> 00:07:57,611
键就是字

131
00:07:57,845 --> 00:08:01,014
而值就是那个字在句子中的出现次数

132
00:08:01,882 --> 00:08:04,218
“Core ML很棒 我很喜欢用”

133
00:08:04,284 --> 00:08:06,587
将会被翻译到词典中
看起来应该是这样的

134
00:08:07,821 --> 00:08:09,256
一旦我获取到了字数统计

135
00:08:09,389 --> 00:08:11,792
我就可以把它传到我的情感分析模型中

136
00:08:12,292 --> 00:08:13,560
然后就会得到一个预测

137
00:08:13,961 --> 00:08:15,095
在本例中是“高兴”

138
00:08:16,563 --> 00:08:19,900
但你可能会想 “好吧 我该如何
把原始文本转成字数统计呢？”

139
00:08:20,501 --> 00:08:23,136
嗯 你可能已经看了NLP的演讲

140
00:08:23,403 --> 00:08:27,641
但你可以使用NSLinguisticTagger中
现有的工具

141
00:08:27,774 --> 00:08:29,843
来标记字符串并统计字数

142
00:08:30,210 --> 00:08:35,249
我要使用NLP来预处理我的文本
尤其是NSLinguisticTagger

143
00:08:35,883 --> 00:08:37,518
然后我要获取字数统计

144
00:08:37,951 --> 00:08:40,821
然后我就传给模型 并获取预测

145
00:08:41,822 --> 00:08:44,458
然后你就可以创建一个像这样的应用了

146
00:08:44,758 --> 00:08:47,194
但让我们不只是说说而已
让我们实际来做一下

147
00:08:48,362 --> 00:08:52,466
我要打开Xcode

148
00:08:57,337 --> 00:09:00,174
我这里有Xcode和一个模拟器

149
00:09:00,741 --> 00:09:02,743
模拟器现在正在运行应用shell

150
00:09:03,143 --> 00:09:04,711
应用shell并没有包含模型

151
00:09:04,778 --> 00:09:11,185
所以如果我键入某些文本 比如
“Core ML很棒很有趣”

152
00:09:11,552 --> 00:09:13,253
那么UI不会有任何事情发生

153
00:09:13,820 --> 00:09:16,356
我们现在要做的就是继续

154
00:09:16,423 --> 00:09:18,492
并在这里整合机器学习模型

155
00:09:18,759 --> 00:09:21,228
从而使应用变得更有活力

156
00:09:23,230 --> 00:09:26,466
那么我要做的第一件事就是
打开Finder

157
00:09:27,234 --> 00:09:30,337
并把情感分析模型拖到Xcode中

158
00:09:32,639 --> 00:09:34,541
让我们看看这是个什么模型

159
00:09:36,743 --> 00:09:39,479
那么正如你所看到的
这是一个情感分析模型

160
00:09:40,214 --> 00:09:42,516
这个模型的类型是管道分类器

161
00:09:42,683 --> 00:09:45,519
那么它在提供最终预测之前
会做一些不同的操作

162
00:09:46,687 --> 00:09:50,023
它只有167KB 所以很小

163
00:09:51,325 --> 00:09:54,294
这个模型的输入是字数统计

164
00:09:54,494 --> 00:09:56,363
字数统计是词典…

165
00:09:57,998 --> 00:10:01,235
…键是字 值是那个字出现的次数

166
00:10:01,902 --> 00:10:03,637
我从这个模型中可以获得两种输出

167
00:10:04,004 --> 00:10:06,440
一个是情感标签 包括两个东西

168
00:10:06,640 --> 00:10:08,809
要么是好 要么是坏

169
00:10:09,343 --> 00:10:12,446
还有一个情感得分 很可能

170
00:10:12,579 --> 00:10:15,716
与好情绪或坏情绪相关联

171
00:10:16,750 --> 00:10:17,818
所以这是个预测

172
00:10:18,385 --> 00:10:20,187
那么我要在这个应用中使用的是

173
00:10:20,254 --> 00:10:24,958
我要用情感得分来决定 范围从0到1

174
00:10:25,092 --> 00:10:26,360
这个文本所表达的内容有多积极

175
00:10:26,493 --> 00:10:29,062
并且根据这个来更新我的UI

176
00:10:31,031 --> 00:10:35,435
那么让我继续 把这个模型
包含到我应用的目标中

177
00:10:36,703 --> 00:10:40,073
然后Xcode将会自动生成一个
很漂亮的界面

178
00:10:40,607 --> 00:10:44,478
我可以返回到ViewController
并且现在我要实施逻辑

179
00:10:44,878 --> 00:10:47,014
以整合这个机器学习模型

180
00:10:47,581 --> 00:10:49,716
接下来我要实施这个函数

181
00:10:49,883 --> 00:10:51,952
predictSentimentScoreFromRawText

182
00:10:52,252 --> 00:10:54,421
这个函数会获取文本

183
00:10:54,488 --> 00:10:55,822
就是整个字符串

184
00:10:56,523 --> 00:10:59,059
每次用户键入空格时都会调用它

185
00:10:59,526 --> 00:11:02,796
并且它返回的是一个
介于0和1之间的双精度浮点值

186
00:11:02,863 --> 00:11:06,033
0代表非常非常伤心
1代表非常非常开心

187
00:11:07,901 --> 00:11:10,637
那么我要做的第一件事就是
实例化这个模型

188
00:11:10,704 --> 00:11:14,274
我可以很简单地实现
通过let model = SentimentAnalysis

189
00:11:15,809 --> 00:11:19,680
然后我要进行预测
使用这个模型做出这种预测

190
00:11:19,980 --> 00:11:22,049
但正如你所看到的
这里输入的是一个句子

191
00:11:22,249 --> 00:11:24,184
但我真正想要的是字数统计

192
00:11:24,985 --> 00:11:28,222
我已经实施了这个
叫作tokenizeAndCountWords的函数

193
00:11:28,555 --> 00:11:31,592
这个函数使用NSLinguisticTagger
来标记句子中的字符串

194
00:11:31,658 --> 00:11:34,461
然后计算那个句子中的令牌个数

195
00:11:34,628 --> 00:11:35,796
我要跳过这个过程

196
00:11:37,531 --> 00:11:38,866
我只需要调用那个函数即可

197
00:11:38,932 --> 00:11:40,901
我要指明let wordCounts

198
00:11:40,968 --> 00:11:43,770
= tokenizeAndCountWords (sentence)

199
00:11:44,905 --> 00:11:47,908
然后我要使用这个字数统计
并提供给我的模型

200
00:11:48,642 --> 00:11:54,314
那么我要指明if let prediction =
try model.prediction (wordCounts)

201
00:11:54,748 --> 00:11:56,650
并简便地把wordCount传到那儿

202
00:11:57,451 --> 00:12:00,487
如果成功了 我就要使用预测对象

203
00:12:01,188 --> 00:12:02,856
来获取情感得分

204
00:12:03,090 --> 00:12:05,025
因为我想要一个0到1之间的值

205
00:12:05,092 --> 00:12:06,793
我要获取得分而不是标签

206
00:12:07,160 --> 00:12:10,063
那么我要获取与情绪“良好”
相关联的情感得分

207
00:12:10,464 --> 00:12:11,899
并将其返回到我的UI

208
00:12:12,466 --> 00:12:15,502
如果失败 万一失败
得分就会小于0.5

209
00:12:15,636 --> 00:12:17,404
这里的处理就发生了错误

210
00:12:19,840 --> 00:12:22,042
我要继续并创建那个应用

211
00:12:22,342 --> 00:12:24,545
在此过程中 你可能会意识到

212
00:12:24,611 --> 00:12:27,214
模型和代码都是打包的

213
00:12:27,314 --> 00:12:29,249
只是搬到设备上来

214
00:12:29,783 --> 00:12:32,719
另一件需要注意的就是编译器

215
00:12:32,786 --> 00:12:36,390
Core ML编译器是作为Xcode
池链的一部分被搬过来的

216
00:12:36,557 --> 00:12:39,426
随意如果你想自己编译
或自己运行代码生成器

217
00:12:39,526 --> 00:12:41,195
你可以直接使用编译器

218
00:12:42,930 --> 00:12:46,066
那么现在让我们使用这个应用
让我们先试试积极的文本

219
00:12:46,700 --> 00:12:49,636
让我们键入“Core ML很有趣”

220
00:12:49,703 --> 00:12:50,571
这是我要输入的内容

221
00:12:51,205 --> 00:12:54,575
“Core ML很有趣 我喜欢用”

222
00:12:55,542 --> 00:12:58,111
那么你立即看到UI弹出
我得到了绿色

223
00:12:58,178 --> 00:13:00,280
我很开心 并且你知道的 这很棒

224
00:13:04,218 --> 00:13:06,053
但现在我想键入一些消极的内容

225
00:13:06,119 --> 00:13:08,455
但我其实不想对任何东西
或任何人开玩笑

226
00:13:08,522 --> 00:13:11,358
我要谈谈如果没有CoreML
我的生活会如何糟糕

227
00:13:12,526 --> 00:13:15,462
“没有CoreML的生活很慵懒、

228
00:13:16,496 --> 00:13:20,334
很糟糕、令人伤心”

229
00:13:20,801 --> 00:13:23,437
那么很明显 UI表示现在很伤心
因为你知道的

230
00:13:23,504 --> 00:13:25,339
没有Core ML的生活
令人非常伤心

231
00:13:26,039 --> 00:13:27,140
那么我们其实看到的

232
00:13:27,207 --> 00:13:31,278
是一个NLP和Core ML
的无缝整合

233
00:13:31,545 --> 00:13:33,313
那么我创建了这个情感分析模型

234
00:13:33,380 --> 00:13:36,517
并能使我的应用更有活力

235
00:13:36,617 --> 00:13:40,787
这全都是实时发生的
当用户在设备上键入文本时

236
00:13:41,955 --> 00:13:42,990
非常酷

237
00:13:46,193 --> 00:13:49,897
让我们回顾一下这个演示中的
两个主要的东西

238
00:13:50,430 --> 00:13:53,233
首先是预处理文本

239
00:13:53,400 --> 00:13:55,135
我们使用了NSLinguisticTagger

240
00:13:56,670 --> 00:13:59,706
第二个是一旦我获得这些字数统计

241
00:13:59,840 --> 00:14:02,543
我就可以把它提供给模型
并获得一个预测

242
00:14:03,844 --> 00:14:06,513
这种模式你会

243
00:14:06,613 --> 00:14:08,148
在基于文本的应用中多次遇到

244
00:14:08,448 --> 00:14:13,287
因为绝大部分文本空间应用
都不能直接读取原始文本

245
00:14:13,654 --> 00:14:16,456
总是不得不需要

246
00:14:16,523 --> 00:14:17,558
一点预处理

247
00:14:19,493 --> 00:14:22,563
但是那真的是个简单的例子
它只是用于介绍的例子

248
00:14:22,963 --> 00:14:25,032
让我们继续讲 进入下一个层级

249
00:14:25,666 --> 00:14:26,667
让我们谈谈

250
00:14:26,733 --> 00:14:29,303
你每天都能与之交互的东西

251
00:14:29,903 --> 00:14:31,638
就是Apple键盘

252
00:14:32,506 --> 00:14:34,741
现当你在Apple键盘中键入文本时

253
00:14:34,908 --> 00:14:36,076
你们可能都意识到了

254
00:14:36,176 --> 00:14:38,178
你会获得一个非常情境化的预测

255
00:14:38,345 --> 00:14:40,848
关于你下一个即将要输入的最可能的词

256
00:14:41,415 --> 00:14:46,353
那么如果我说“我不确定奥利弗
是否吃牡蛎 但他会”

257
00:14:46,653 --> 00:14:48,956
键盘会告诉你“那么”、“完全地”
和“喜欢”

258
00:14:49,022 --> 00:14:51,558
这三个你接下来很可能要键入的词

259
00:14:53,160 --> 00:14:56,430
如何创建像这样复杂的东西呢？

260
00:14:57,264 --> 00:14:58,932
这是一个预测性键盘

261
00:14:59,700 --> 00:15:04,471
这里的机器学习任务是预测下一个词

262
00:15:06,139 --> 00:15:07,641
这里所使用的模型

263
00:15:07,774 --> 00:15:10,310
或像这样的应用所要使用的模型

264
00:15:10,611 --> 00:15:14,248
其实是一个可以读取一系列词的模型

265
00:15:14,381 --> 00:15:17,518
“我不确定奥利弗是否吃牡蛎
但他会”

266
00:15:17,584 --> 00:15:18,886
是一系列的词

267
00:15:19,286 --> 00:15:22,122
我把这个句子作为模型的输入
并获取一个预测

268
00:15:23,490 --> 00:15:24,324
那么你可能在想

269
00:15:24,391 --> 00:15:27,728
“好吧 那这个模型
与我们刚看到的那个

270
00:15:27,794 --> 00:15:30,564
情感分析模型有什么不一样呢？
它们看起来是一样的”

271
00:15:31,498 --> 00:15:36,403
主要不同点是在这里
输入的是一系列的词

272
00:15:36,870 --> 00:15:39,506
那么如果你把这些词打乱并提供给模型

273
00:15:39,773 --> 00:15:41,842
你将会得到一个完全不同的预测

274
00:15:43,544 --> 00:15:46,380
要实现这样的功能
绝大部分机器学习模型

275
00:15:46,446 --> 00:15:49,316
都有一个与之相关联的状态的概念

276
00:15:49,917 --> 00:15:51,552
这是它们获得此性能的方式

277
00:15:52,152 --> 00:15:55,822
做出每个预测时会传递一个状态

278
00:15:56,256 --> 00:15:58,492
那么这就像是接力赛中的接力棒一样

279
00:15:58,625 --> 00:16:00,894
每次你做出预测时
都要读取状态并传递它

280
00:16:03,063 --> 00:16:06,667
我们通常使用LSDM
来实现这样的功能

281
00:16:06,967 --> 00:16:09,269
尤其像[听不清]网络

282
00:16:09,703 --> 00:16:12,139
但通过Core ML
这些都会变得异常简单

283
00:16:12,606 --> 00:16:14,208
那么让我们看看你要做什么

284
00:16:15,375 --> 00:16:17,411
但我们会用一个更有趣的应用来实现

285
00:16:17,744 --> 00:16:19,246
我们要用莎士比亚键盘来实现

286
00:16:19,780 --> 00:16:21,348
那么这不是一个常规的键盘

287
00:16:21,582 --> 00:16:23,584
这个键盘会让我输入的文本
变得像是莎士比亚的话一样

288
00:16:24,184 --> 00:16:27,387
如果我说“我是否应该对比”
它应该会指明

289
00:16:27,454 --> 00:16:30,190
“尔”、“夏”、“天”是我接下来
很可能要输入的三个词

290
00:16:32,025 --> 00:16:33,527
那么

291
00:16:33,594 --> 00:16:36,530
莎士比亚键盘和常规键盘之间
到底有什么区别呢？

292
00:16:36,763 --> 00:16:39,967
其实是模型在预测下一个词

293
00:16:40,501 --> 00:16:43,737
那么其中一个模型
被训练读取莎士比亚数据

294
00:16:44,137 --> 00:16:47,140
而另一个被训练读取常规的英文数据

295
00:16:47,941 --> 00:16:49,743
那么这个概念就是语言模型

296
00:16:50,577 --> 00:16:52,980
那么我刚跟你们提到了许多新概念

297
00:16:53,046 --> 00:16:56,383
语言模型、序列、LSDM
但你不要担心

298
00:16:56,483 --> 00:16:58,385
通过Core ML
这些应该简单多了

299
00:16:58,785 --> 00:17:00,387
让我们看看你如何实现像这样的功能

300
00:17:01,855 --> 00:17:05,492
那么我从一个模型开始
我要给它键入第一个词

301
00:17:05,692 --> 00:17:07,327
在本例中比如“是否应该”

302
00:17:08,262 --> 00:17:09,730
这是当前的词

303
00:17:10,864 --> 00:17:13,800
我从模型中得到的是两个东西

304
00:17:14,635 --> 00:17:16,670
下一个词的一组可选词

305
00:17:16,869 --> 00:17:17,804
那么在本例中

306
00:17:18,005 --> 00:17:22,376
我主要获得了与下一个词的全部
可选词相关联的概率

307
00:17:23,777 --> 00:17:27,146
并且我还要获取与此预测相关联的状态

308
00:17:28,615 --> 00:17:32,152
那么我会读取下一个词的可选词
并提供给用户

309
00:17:32,953 --> 00:17:35,122
用户会从三个词中选择一个

310
00:17:35,189 --> 00:17:36,924
或也许他们会键入自己的词

311
00:17:37,024 --> 00:17:38,959
无论哪种方式我都能得到下一个词

312
00:17:40,427 --> 00:17:41,528
我将使用那个下一个词

313
00:17:41,595 --> 00:17:43,630
把它传回给模型用于下一次预测

314
00:17:44,131 --> 00:17:45,465
并且我还要读取状态

315
00:17:45,532 --> 00:17:47,401
并将其传回模型用于下一次预测

316
00:17:48,402 --> 00:17:51,238
那么在稳定状态下
每次你都要做两件事

317
00:17:51,638 --> 00:17:53,240
你要读取状态中的当前词

318
00:17:53,307 --> 00:17:55,776
并把它提供给模型 然后你将会获得

319
00:17:55,843 --> 00:17:58,345
下一个词的一组可选词以及某个状态

320
00:17:59,246 --> 00:18:00,314
你要做的第二件事

321
00:18:00,380 --> 00:18:02,983
是把那些全部传回到模型
用于下一次预测

322
00:18:03,250 --> 00:18:04,418
非常简单

323
00:18:05,085 --> 00:18:07,454
让我们看看代码是什么样的

324
00:18:09,189 --> 00:18:12,826
一开始我要指明let output =
model.prediction (input)

325
00:18:14,294 --> 00:18:17,130
我会读取与下一个词相关联的概率

326
00:18:17,264 --> 00:18:20,067
并且我会把它提供给一个函数
假设是displayTopPredictions

327
00:18:20,133 --> 00:18:23,036
这个函数表明的是
选择前三个并提供给用户

328
00:18:24,905 --> 00:18:27,241
用户选择三个词中的一个

329
00:18:27,307 --> 00:18:28,609
或键入自己的词

330
00:18:28,842 --> 00:18:32,079
无论哪种方式我都能从这个
getWordFromUser函数中获取下一个词

331
00:18:32,479 --> 00:18:35,048
并且我会把这个词作为当前词
传回给输入

332
00:18:35,382 --> 00:18:38,819
我将读取状态并把它再次传给模型

333
00:18:39,253 --> 00:18:41,421
那么只需要几行代码

334
00:18:41,722 --> 00:18:44,324
你就可以整合一个像LSDM一样
不那么复杂的模型

335
00:18:44,458 --> 00:18:47,261
其中包括状态、语言模型、键盘

336
00:18:47,327 --> 00:18:49,363
全部这些东西只通过几行代码就实现了

337
00:18:49,830 --> 00:18:51,932
这就是几个不同的用例

338
00:18:51,999 --> 00:18:53,400
还有关于文本的用例

339
00:18:54,401 --> 00:18:56,069
让我们谈谈Core ML是如何

340
00:18:56,170 --> 00:18:58,839
针对它所运行的硬件进行优化的

341
00:18:58,972 --> 00:19:02,709
最重要的是当你创建应用时
这对于你们来说意味着什么

342
00:19:04,411 --> 00:19:05,412
那么我们要通过一个

343
00:19:05,479 --> 00:19:08,448
实时对象探测的小视频
来激励一下我们自己

344
00:19:09,349 --> 00:19:14,922
这里有一个重点要注意 就是相机种子
是实时的 将会进入一个模型

345
00:19:15,355 --> 00:19:16,957
一个相对强大的模型

346
00:19:17,057 --> 00:19:20,427
你将会获得精确的预测
正如你所看到的 实时的

347
00:19:21,195 --> 00:19:22,663
这之所以可能实现

348
00:19:22,963 --> 00:19:26,667
是因为Core ML针对它所在运行
的硬件进行了超级优化

349
00:19:27,134 --> 00:19:30,971
在本例中 模型的运行大概
比如说是不到50毫秒

350
00:19:34,141 --> 00:19:35,342
我希望没有人会笑

351
00:19:35,409 --> 00:19:37,845
因为这个笑话早已经讲了七遍了

352
00:19:42,382 --> 00:19:44,785
那么对于你来说
真正重要的是Core ML

353
00:19:44,852 --> 00:19:47,020
是建在性能基元之上的

354
00:19:47,287 --> 00:19:49,256
加速和MPS

355
00:19:49,623 --> 00:19:50,524
但更重要的是

356
00:19:50,591 --> 00:19:52,659
它完全隐藏了硬件

357
00:19:52,726 --> 00:19:55,562
所以你不必担心它是否运行于CPU

358
00:19:55,863 --> 00:19:56,763
或GPU上

359
00:19:57,231 --> 00:19:58,899
那么你看到的那个演示 你可能会提问

360
00:19:58,966 --> 00:20:01,635
“好吧 我要拐几个弯
才能实现那个功能？”

361
00:20:01,835 --> 00:20:02,903
嗯 零个

362
00:20:03,136 --> 00:20:05,272
这是一个开箱即用的性能

363
00:20:12,279 --> 00:20:14,815
那么特别是你刚看到的那个演示

364
00:20:14,882 --> 00:20:17,417
和你之前看到的那个花语的演示

365
00:20:17,584 --> 00:20:20,187
这两个都需要进行复杂的计算

366
00:20:20,387 --> 00:20:23,357
我们知道这一点
所以我们在GPU上为你显示

367
00:20:24,057 --> 00:20:25,893
然而一些基于文本的演示

368
00:20:25,993 --> 00:20:28,595
比如情感分析和下一个词预测

369
00:20:28,829 --> 00:20:30,497
这些真用了许多内存

370
00:20:30,564 --> 00:20:33,000
这也是我们为什么在CPU上
为你显示的原因

371
00:20:33,901 --> 00:20:36,336
但最重要的是它们都运行在
Core ML上

372
00:20:36,403 --> 00:20:38,405
所以你不必担心它运行在哪儿

373
00:20:38,505 --> 00:20:39,439
我们又把你们带回来了

374
00:20:40,674 --> 00:20:43,844
这种抽象概念
让我们做一些强大的功能吧

375
00:20:44,478 --> 00:20:46,647
那么对于给图像添加字幕 比如说

376
00:20:46,880 --> 00:20:51,351
那个模型的一部分是需要复杂的计算
而还有一部分是占用许多内存

377
00:20:51,518 --> 00:20:54,888
我们会自动联系从GPU切换到CPU

378
00:20:55,055 --> 00:20:57,124
从而你可以获得最好的性能

379
00:20:59,159 --> 00:21:02,129
这些都是用例和性能相关的

380
00:21:02,863 --> 00:21:05,232
但Core ML并不只是一个框架

381
00:21:05,699 --> 00:21:07,901
它是一个文件格式 还是工具的组合

382
00:21:07,968 --> 00:21:11,171
可以帮助你获取越来越多的模型
你可以用在你自己的应用中

383
00:21:11,705 --> 00:21:14,875
让我们邀请我的朋友兼同事Zach
上台来讲讲与之相关的内容

384
00:21:23,250 --> 00:21:24,084
谢谢Krishna

385
00:21:25,052 --> 00:21:30,324
嗨 我叫Zach
Core ML工程师团队的工程师

386
00:21:34,494 --> 00:21:36,496
今天我很激动能跟大家谈谈

387
00:21:36,797 --> 00:21:38,832
Core ML模型格式

388
00:21:38,999 --> 00:21:42,736
以及你从哪里可以获取这种格式的模型
以用在你自己的应用中

389
00:21:44,505 --> 00:21:47,541
那么到目前为止
你已经多次看到这个图表了吧

390
00:21:47,674 --> 00:21:51,211
它展示了使用机器学习模型是如何简单

391
00:21:51,311 --> 00:21:54,915
只需要把它拖拽到Xcode中
然后你就会获得一个代码界面

392
00:21:55,315 --> 00:21:58,986
但是现在 你很可能会想
“这些模型是从哪儿来的呢？”

393
00:22:00,854 --> 00:22:02,623
嗯 其实有两个地方

394
00:22:02,689 --> 00:22:06,193
你可以找到Core ML模型
格式的机器学习模型

395
00:22:06,927 --> 00:22:11,098
第一个是developer.apple.com
上的示例模型

396
00:22:11,932 --> 00:22:14,368
这些是预训练过的各种模型

397
00:22:14,535 --> 00:22:16,937
已经是Core ML模型格式了

398
00:22:17,137 --> 00:22:20,340
并且如果你是机器学习的新手
这也是最简单的上手方式了

399
00:22:21,441 --> 00:22:22,543
但我们也知道

400
00:22:22,743 --> 00:22:25,445
那儿是机器学习的广阔世界

401
00:22:26,013 --> 00:22:28,682
有大量现有的流行训练工具

402
00:22:28,782 --> 00:22:32,152
以及大量模型 它们已是这种格式的了

403
00:22:32,619 --> 00:22:35,923
所以我们把获取机器学习模型

404
00:22:36,089 --> 00:22:38,125
使用最流行的工具进行训练

405
00:22:38,592 --> 00:22:40,594
并把它们应用在你的应用中变成了可能

406
00:22:42,162 --> 00:22:45,799
那么为了那个目的
我们已经创建了Core ML工具

407
00:22:46,233 --> 00:22:50,737
它是一个转化器包
把模型转化成各种流行格式

408
00:22:50,938 --> 00:22:53,574
把它们转化成Core ML模型格式

409
00:22:54,074 --> 00:22:56,543
并且它是开源的

410
00:23:03,250 --> 00:23:06,587
我们已经发布了遵从BSD许可协议
的Core ML工具

411
00:23:06,720 --> 00:23:08,755
因此采用这些工具没有任何障碍

412
00:23:12,426 --> 00:23:15,629
要在因特网之外的某个地方获取模型

413
00:23:15,696 --> 00:23:18,065
你必须从一个不同格式的模型开始

414
00:23:18,131 --> 00:23:19,366
比如说Caffe

415
00:23:19,933 --> 00:23:22,736
那么Caffe是一个
很流行的深度学习训练库

416
00:23:23,937 --> 00:23:26,340
如果你从它的一个模型开始
比如Caffe格式

417
00:23:26,607 --> 00:23:28,709
你把它放到Core ML模型格式

418
00:23:28,775 --> 00:23:30,344
和在你应用中使用它的方式

419
00:23:30,511 --> 00:23:33,614
是通过Core ML工具中的一个
转换器运行它

420
00:23:34,214 --> 00:23:37,551
或者如果你在那儿没有找到
你想要的模型

421
00:23:37,918 --> 00:23:39,786
你可以使用自己的训练数据

422
00:23:39,887 --> 00:23:42,556
并使用任何一种流行工具

423
00:23:42,623 --> 00:23:44,458
来训练该种格式的你自己的模型

424
00:23:44,825 --> 00:23:46,927
然后运行转换器

425
00:23:46,994 --> 00:23:49,363
生产Core ML模型格式的模型

426
00:23:49,563 --> 00:23:52,232
然后接下来的流程就完全一样了

427
00:23:52,399 --> 00:23:55,869
只需要把模型拖拽到Xcode
然后你就得到一个编码界面

428
00:24:03,944 --> 00:24:08,348
使用Core ML工具
与Pip安装coremltools一样简单

429
00:24:08,482 --> 00:24:10,417
只需要下载并安装文件包即可

430
00:24:10,884 --> 00:24:15,556
这是一个Python包
有适合各种流行训练工具的转换器

431
00:24:15,722 --> 00:24:18,392
其中大部分工具已是Python格式

432
00:24:18,592 --> 00:24:20,861
作为那个机器学习生态系统的一部分

433
00:24:20,961 --> 00:24:22,796
这也是一个Python库

434
00:24:25,232 --> 00:24:28,302
让我们具体看一下这个包里都有什么

435
00:24:28,869 --> 00:24:31,271
最顶部时每种转换器

436
00:24:31,338 --> 00:24:33,240
这是一组转换器

437
00:24:33,340 --> 00:24:35,375
每种转换器对应一种流行的训练库

438
00:24:36,610 --> 00:24:40,647
接下来我们有Core ML捆绑
和一个转换器库

439
00:24:40,881 --> 00:24:43,884
这是我们用于创建全部转换器的东西

440
00:24:43,984 --> 00:24:49,356
在这里 Core ML捆绑允许你
直接从Python调用到Core ML

441
00:24:49,456 --> 00:24:52,893
并获得后备推测 这真的很有帮助
对于验证

442
00:24:52,960 --> 00:24:55,395
你获得某种转换模型的预测

443
00:24:55,462 --> 00:24:57,998
是否与你原始训练框架的

444
00:24:58,065 --> 00:24:59,867
预测结果一模一样

445
00:25:00,501 --> 00:25:02,135
我们还有一个转换器库

446
00:25:02,336 --> 00:25:05,038
是一个用于创建转换器的高等级API

447
00:25:05,105 --> 00:25:07,307
它的共享代码全部这些转换器都能使用

448
00:25:07,374 --> 00:25:11,111
使得创建新格式的新转换器
变得非常简单

449
00:25:12,212 --> 00:25:15,382
下边是Core ML规范

450
00:25:15,983 --> 00:25:21,054
这是一个直接面向
Core ML模型格式的读写API

451
00:25:21,455 --> 00:25:24,591
那么全部单一字段都能从这里获取

452
00:25:27,027 --> 00:25:31,665
我们已经以这种方式设计了程序包
所以它可兼容 并且可扩展

453
00:25:32,132 --> 00:25:33,200
在最高层级

454
00:25:33,267 --> 00:25:38,505
转换器为Core ML兼容性
提供了各种流行工具

455
00:25:39,773 --> 00:25:42,209
下边是Core ML捆绑

456
00:25:42,309 --> 00:25:45,112
转换器库以及Core ML规范

457
00:25:45,279 --> 00:25:49,082
使得这个程序包变得可扩展
很容易创建新转换器

458
00:25:49,483 --> 00:25:52,419
并为还没有转换器的大量现有格式

459
00:25:52,486 --> 00:25:53,520
创建转换器

460
00:25:53,921 --> 00:25:55,856
因为这是开源的

461
00:25:56,190 --> 00:25:57,991
很容易获取这个程序包

462
00:25:58,158 --> 00:26:01,128
甚至将它整合到另一个开源库中

463
00:26:01,228 --> 00:26:02,596
并在顶层创建新转换器

464
00:26:02,663 --> 00:26:05,532
并且这里没有任何限制
它有BSD许可证

465
00:26:13,240 --> 00:26:16,677
Core ML模型格式
是单一文档格式

466
00:26:17,010 --> 00:26:20,414
它既封装了模型的功能性描述

467
00:26:20,581 --> 00:26:22,649
关于其输入和输出

468
00:26:22,950 --> 00:26:25,285
又封装了模型受训练的参数

469
00:26:25,853 --> 00:26:27,254
看一个例子

470
00:26:27,454 --> 00:26:29,823
是一个像线性回归一样的简单模型

471
00:26:29,990 --> 00:26:31,859
这是在训练时间内

472
00:26:31,925 --> 00:26:33,260
学习到的权重和补偿的集合

473
00:26:33,660 --> 00:26:36,263
对于一个更复杂的模型 比如神经网络

474
00:26:36,463 --> 00:26:39,800
它实际上封装了网络的结构

475
00:26:39,900 --> 00:26:42,503
及其在训练时间内学习到的权重

476
00:26:43,237 --> 00:26:45,072
这是一个公共文件格式

477
00:26:45,138 --> 00:26:48,075
在developer.apple.com上有完整说明

478
00:26:51,745 --> 00:26:54,348
当你在Xcode中
查看机器学习模型时

479
00:26:54,615 --> 00:26:56,517
你会看到一个这样的视图

480
00:26:56,583 --> 00:26:59,152
你得到了全部元数据和功能性界面

481
00:26:59,520 --> 00:27:03,090
并且我们现在可以看到的是这完全

482
00:27:03,257 --> 00:27:04,691
由这种Core ML模型格式所支持

483
00:27:04,758 --> 00:27:08,562
那么这种单一文档格式包含Xcode
所需要的全部信息

484
00:27:08,629 --> 00:27:10,631
用于给你提供这种模型之上的UI

485
00:27:10,731 --> 00:27:14,268
让你的代码调用它
然后在设备上执行它

486
00:27:16,904 --> 00:27:19,673
Core ML转换器
都以同一种方式运行

487
00:27:20,107 --> 00:27:22,776
它们从在源格式中获取模型开始

488
00:27:22,943 --> 00:27:27,181
比如Caffe
并将其转换为Core ML模型格式

489
00:27:28,248 --> 00:27:30,150
有一组统一的API

490
00:27:30,284 --> 00:27:32,886
用于将这些模型从各种格式

491
00:27:32,953 --> 00:27:34,154
转换为Core ML格式

492
00:27:34,454 --> 00:27:36,356
如果你知道如何从一种格式
转为另一种格式

493
00:27:36,523 --> 00:27:38,592
你就明白如何转换全部格式

494
00:27:40,928 --> 00:27:42,162
让我们看一个例子

495
00:27:42,229 --> 00:27:46,466
近距离地了解如何转换Caffe模型

496
00:27:47,501 --> 00:27:49,002
Caffe大概是这样的

497
00:27:49,136 --> 00:27:51,705
它用几个文件来代表模型

498
00:27:52,239 --> 00:27:56,643
.caffemodel文件代表在该模型中
所学习到的权重

499
00:27:57,010 --> 00:28:01,448
而.prototxt文件
代表神经网络的结构

500
00:28:02,516 --> 00:28:04,318
当Caffe执行推理时

501
00:28:04,818 --> 00:28:08,388
你应该从获取图片开始
比如像这样的一朵玫瑰

502
00:28:08,722 --> 00:28:11,491
并且你把图片和这两个文件
传给Caffe

503
00:28:11,792 --> 00:28:16,396
Caffe会返回一个类标签的索引
比如74

504
00:28:17,197 --> 00:28:20,200
然后有一个第三个文件
一个labels.txt

505
00:28:20,400 --> 00:28:24,471
那会把这些索引映射到字符串类标签
比如“玫瑰”

506
00:28:25,072 --> 00:28:27,841
那么有一点很重要 就是这三个文件

507
00:28:27,941 --> 00:28:30,978
把模型中的全部文件封装在一起

508
00:28:31,111 --> 00:28:34,381
这就是转换到Core ML格式
所需要的一切

509
00:28:35,883 --> 00:28:39,987
现在让我们看一个例子
将Caffe模型转换为

510
00:28:40,087 --> 00:28:41,255
Core ML模型

511
00:28:41,321 --> 00:28:43,323
（演示）

512
00:28:50,964 --> 00:28:54,368
一开始我要在这里打开一个
交互式的Python提示框

513
00:28:54,701 --> 00:28:58,105
所以我们可以键入Python代码
并实时查看输出

514
00:28:59,840 --> 00:29:02,576
那么我要导入coremltools

515
00:29:02,643 --> 00:29:04,978
这是那个Python格式的
程序包的名称

516
00:29:07,648 --> 00:29:11,118
一旦完成后
我只需要键入coremltools

517
00:29:11,451 --> 00:29:13,720
当我处理新Python程序包时

518
00:29:13,987 --> 00:29:16,490
我要做的第一件事就是
把它标记为完成

519
00:29:16,557 --> 00:29:18,425
并查看API中有哪些可用

520
00:29:19,459 --> 00:29:24,198
在这里标记完成时 我们可以看到
coremltools包含转换器

521
00:29:24,364 --> 00:29:27,100
是每个高等级转换器从另一种格式

522
00:29:27,167 --> 00:29:28,468
都转换为了Core ML格式

523
00:29:28,969 --> 00:29:32,940
以及模型 规范版本和通用版本

524
00:29:33,207 --> 00:29:35,909
就是框架捆绑和转换器库

525
00:29:36,076 --> 00:29:38,545
以及那些你可以创建新转换器

526
00:29:38,612 --> 00:29:40,480
并测试它是否能提供

527
00:29:40,547 --> 00:29:42,115
与原始训练框架所提供的一样的预测

528
00:29:42,649 --> 00:29:46,720
而.protonamesbase包含

529
00:29:46,887 --> 00:29:48,355
Core ML模型格式的读写API

530
00:29:49,122 --> 00:29:51,024
今天我们主要关注转换器

531
00:29:51,558 --> 00:29:54,862
若我再次执行.converters
然后标记为完成

532
00:29:55,128 --> 00:29:57,197
我可以立即看到这个命名空间中

533
00:29:57,264 --> 00:29:58,465
全部可用的转换器

534
00:29:58,532 --> 00:30:02,769
这里有caffe、keras、libsvn、
scikit-learn和xgboost

535
00:30:03,036 --> 00:30:04,905
今天我们主要讲Caffe

536
00:30:05,439 --> 00:30:08,108
我再次执行.caffe并标记完成

537
00:30:08,175 --> 00:30:09,009
查看有哪些可用

538
00:30:09,076 --> 00:30:11,278
它就通过.convert
替我标记为已完成

539
00:30:11,545 --> 00:30:12,379
就是那么简单

540
00:30:12,446 --> 00:30:14,515
这里只有一个函数叫作convert

541
00:30:15,082 --> 00:30:16,617
让我们看看要如何使用它

542
00:30:19,086 --> 00:30:23,724
首先我要设置输入

543
00:30:23,991 --> 00:30:27,728
我知道我有一个Caffe模型
由两个文件定义

544
00:30:27,928 --> 00:30:30,797
我要指明caffemodel =
并且我提供两个字符串的

545
00:30:31,064 --> 00:30:34,201
两个极点指向
那个Caffe模型的文件名

546
00:30:34,468 --> 00:30:38,972
分别是“flowers.caffemodel”
和“flowers.prototxt”

547
00:30:39,039 --> 00:30:42,476
这两者共同代表了
在网络中学习到的权重

548
00:30:42,543 --> 00:30:44,044
以及网络结构

549
00:30:46,146 --> 00:30:49,483
接下来我要设置一个类标签文件

550
00:30:49,983 --> 00:30:51,852
那么我要指明labels =

551
00:30:52,486 --> 00:30:54,555
并且这里有"labels.txt"

552
00:30:55,355 --> 00:30:59,593
那代表从数值型索引
到字符串类标签的映射

553
00:31:00,894 --> 00:31:02,229
现在运行转换器…

554
00:31:02,496 --> 00:31:03,497
很简单

555
00:31:03,697 --> 00:31:07,201
我要做的就是
指明coreml_model

556
00:31:08,135 --> 00:31:13,173
= coremltools
.converters.caffe.convert

557
00:31:13,373 --> 00:31:14,908
全部标记为完成

558
00:31:15,142 --> 00:31:21,448
我只需要传递那个caffemodel
并且classlabels = labels

559
00:31:21,849 --> 00:31:23,851
那么我只需要传递这三个文件

560
00:31:26,119 --> 00:31:29,890
当转换器完成时
我得到的是Core ML模型

561
00:31:30,524 --> 00:31:33,160
就在这里 是Python格式的
我可以打印模型

562
00:31:33,527 --> 00:31:36,830
并查看通过那个模型所得到的界面

563
00:31:37,297 --> 00:31:40,000
那么我可以看到它有一个输入
叫作“数据”

564
00:31:40,767 --> 00:31:42,803
那个输入是一个多数组

565
00:31:43,003 --> 00:31:47,007
格式为3乘227乘227
类型为双精度浮点型

566
00:31:48,242 --> 00:31:49,610
我们稍后再返回来看

567
00:31:50,177 --> 00:31:51,712
它还有两种输出

568
00:31:52,112 --> 00:31:55,816
一个叫作“prob”
它是一个带字符串键的词典

569
00:31:55,949 --> 00:31:58,218
所以它代表

570
00:31:58,385 --> 00:32:00,120
每种可能的类标签的可能性

571
00:32:00,954 --> 00:32:03,757
另一种输出叫作“classLabel”
是一个字符串

572
00:32:03,824 --> 00:32:06,426
它肯定是最可能的类标签

573
00:32:06,527 --> 00:32:09,029
为了方便 你不必查看全部可能性

574
00:32:09,096 --> 00:32:10,764
以找出最可能的那个

575
00:32:13,567 --> 00:32:16,370
但你可以查看这个
我知道这种输入类型

576
00:32:16,436 --> 00:32:20,107
并不完全是我们希望模型所拥有的界面

577
00:32:20,374 --> 00:32:23,744
所以我可以返回并以另一个参数
运行转换器

578
00:32:24,077 --> 00:32:27,214
修改输入类型
因为我不希望使用多数组

579
00:32:27,614 --> 00:32:30,450
我希望这个模型将图片作为输入

580
00:32:31,885 --> 00:32:36,523
我要返回并添加
image_input_names = "data"

581
00:32:36,924 --> 00:32:38,792
再次让我们看一下这里的输出

582
00:32:38,859 --> 00:32:40,761
你可以看到输入名称为data

583
00:32:41,061 --> 00:32:43,430
它就会知道将data用作输入

584
00:32:45,165 --> 00:32:46,867
如果我再次运行转换器

585
00:32:47,134 --> 00:32:49,369
然后再一次打印模型界面

586
00:32:49,570 --> 00:32:51,839
现在我看到叫作data的输入

587
00:32:51,905 --> 00:32:56,910
是一个图片 宽227 高227
色彩空间是RGB

588
00:33:04,218 --> 00:33:05,919
现在我们有Core ML模型

589
00:33:06,320 --> 00:33:09,189
让我们查看并确保转换已成功

590
00:33:09,389 --> 00:33:12,593
从而通过这个Core ML模型
获取正确预测

591
00:33:13,827 --> 00:33:16,630
我先导入Python图库

592
00:33:16,697 --> 00:33:20,334
以便我可以使用图片
并直接向模型中传递图片

593
00:33:20,868 --> 00:33:25,439
那么我要指明from PIL import Image

594
00:33:26,507 --> 00:33:31,111
然后Rose = Image.open ("rose.jpg")

595
00:33:32,713 --> 00:33:35,516
为了证明我没隐藏什么

596
00:33:36,583 --> 00:33:38,585
我要调用rose.show

597
00:33:39,653 --> 00:33:42,923
并给你展示
这的确是一张玫瑰的图片

598
00:33:48,829 --> 00:33:49,963
你不必为此而鼓掌

599
00:33:53,500 --> 00:33:57,037
现在我已经表明
这张图片真的代表的是玫瑰

600
00:33:57,104 --> 00:33:58,872
让我们来看看模型是否同意我们的意见

601
00:33:59,473 --> 00:34:05,012
查看预测跟调用
coremlmodel.predict一样简单

602
00:34:06,013 --> 00:34:09,949
这是我们之前谈到过的
Core ML框架捆绑

603
00:34:10,984 --> 00:34:14,987
我们要传递命名为“data”、
值为“玫瑰”的输入

604
00:34:16,422 --> 00:34:20,360
我们立即就得到了一个
类标签玫瑰的预测

605
00:34:26,699 --> 00:34:27,967
但让我们确保模型…

606
00:34:28,034 --> 00:34:29,203
让我们能确保它不是一个侥幸

607
00:34:29,268 --> 00:34:31,871
并确保模型真的知道这是一朵玫瑰

608
00:34:31,938 --> 00:34:34,675
我们要向下滚动类可能性

609
00:34:34,975 --> 00:34:36,577
知道我们在这里看到玫瑰

610
00:34:37,945 --> 00:34:40,246
我们可以看到模型其实非常自信

611
00:34:40,313 --> 00:34:41,148
它认为这就是一朵玫瑰

612
00:34:41,215 --> 00:34:45,918
那么有.991比1的可能性表明
这是一朵玫瑰

613
00:34:45,985 --> 00:34:49,790
那么我要推断很可能是模型
执行了正确的转换

614
00:34:50,424 --> 00:34:52,659
如果我在一个真实的应用中
执行以上程序

615
00:34:52,793 --> 00:34:54,995
我会希望执行更严格的检测

616
00:34:55,062 --> 00:34:56,964
所以我要提供一个以上的例子

617
00:34:57,130 --> 00:34:58,866
我还想要查看

618
00:34:58,932 --> 00:35:01,201
我从Core ML中获取的预测

619
00:35:01,301 --> 00:35:04,905
与Caffe会给我提供的预测
是一模一样的

620
00:35:04,972 --> 00:35:08,509
如果输入一样的话
这也是我们了解转换是否成功的方式

621
00:35:09,176 --> 00:35:12,713
但对于这个演示来说会花太长时间
所以让我们继续

622
00:35:13,180 --> 00:35:16,149
我现在要做的是保存模型

623
00:35:16,216 --> 00:35:17,985
然后在Xcode中进行查看

624
00:35:18,385 --> 00:35:20,787
我会表明
coremlmodel.save

625
00:35:21,088 --> 00:35:24,024
并且我要把它命名为
FlowerPredictor.mlmodel

626
00:35:26,426 --> 00:35:29,263
然后我要打开当前目录和Finder

627
00:35:29,530 --> 00:35:32,266
并双击那个模型以在Xcode中打开

628
00:35:33,800 --> 00:35:35,202
那么我们在这里可以看到

629
00:35:35,903 --> 00:35:39,273
一个叫作FlowerPredictor的
机器学习模型

630
00:35:39,339 --> 00:35:44,545
类型是神经网络分类器
大小为229.1MB

631
00:35:45,345 --> 00:35:47,147
但也有许多遗失的信息

632
00:35:47,214 --> 00:35:49,183
它不知道作者、证书、

633
00:35:49,249 --> 00:35:53,420
描述或输入和输出描述

634
00:35:53,887 --> 00:35:55,856
那么当它作为模型时

635
00:35:55,956 --> 00:35:58,692
这就是一个我没有必要提供给同事

636
00:35:58,759 --> 00:35:59,826
或放到因特网上的一个模型

637
00:36:00,093 --> 00:36:02,963
因为它并没有那么有用
如果它没有声明

638
00:36:03,030 --> 00:36:04,731
它应该干什么以及如何使用它

639
00:36:05,332 --> 00:36:06,733
那么我要成为一个好公民

640
00:36:06,800 --> 00:36:08,969
我要返回并赋予这个模型一些元数据

641
00:36:14,074 --> 00:36:17,611
返回到Python提示框
我可以改变这里的模型

642
00:36:17,978 --> 00:36:20,347
通过在顶层分配给字段

643
00:36:20,414 --> 00:36:24,451
我可以说
author = "Zach Nation"

644
00:36:26,153 --> 00:36:30,490
coremlmodel.license = "BSD"

645
00:36:31,925 --> 00:36:37,865
coremlmodel.shortdescription
= "A flower classifier"

646
00:36:39,299 --> 00:36:43,003
让我们同时设置输入和输出的帮助文本

647
00:36:43,136 --> 00:36:46,106
因为它不仅是在Xcode视图中显示

648
00:36:46,340 --> 00:36:49,676
还会当所生成的代码出现时
以及当你调用它时显示

649
00:36:49,877 --> 00:36:52,446
其实成为了文档评论

650
00:36:52,513 --> 00:36:53,647
在所生成的代码中

651
00:36:53,914 --> 00:36:55,949
那么当你标记完成一个Xcode时

652
00:36:56,049 --> 00:36:58,318
这就是其他人期待
能从这个模式中看到的东西

653
00:37:00,087 --> 00:37:04,191
我要给data表明
coremlmodel.inputdescription

654
00:37:04,691 --> 00:37:07,961
是“一张花朵的图片”

655
00:37:09,463 --> 00:37:12,666
并且coremlmodel.outputdescription
"prob"

656
00:37:12,866 --> 00:37:18,405
是“根据所输入内容

657
00:37:19,740 --> 00:37:21,108
判断每种花朵类型的可能性”

658
00:37:23,610 --> 00:37:27,447
并且coremlmodel.outputdescription
"classLabel"

659
00:37:28,415 --> 00:37:33,587
是“根据所输入内容判断
最有可能的花朵类型”

660
00:37:35,889 --> 00:37:40,093
现在 我们只需要再次保存那个模型

661
00:37:41,094 --> 00:37:43,130
我要覆盖那里的那个文件

662
00:37:43,397 --> 00:37:45,232
因为我希望得到带有元数据的文件

663
00:37:45,999 --> 00:37:47,768
所以我要把它保存在文件顶部

664
00:37:47,835 --> 00:37:51,438
并再次打开那个目录和Finder

665
00:37:52,039 --> 00:37:54,908
现在当我双击模型
并在Xcode中打开它时

666
00:37:55,309 --> 00:37:57,878
我们可以看到它包含有用的元数据

667
00:37:57,978 --> 00:38:00,447
描述了模型如何使用

668
00:38:00,781 --> 00:38:02,983
以及输入和输出是什么

669
00:38:13,660 --> 00:38:19,233
回顾一下 我们了解了
使用Core ML工具来转换模型

670
00:38:19,600 --> 00:38:24,338
与导入coremltools
设置输入

671
00:38:24,638 --> 00:38:27,708
然后调用一个高层级转换函数

672
00:38:27,808 --> 00:38:30,577
并获得一个Core ML
模型格式的模型一样简单

673
00:38:31,111 --> 00:38:32,312
最棒的是

674
00:38:32,412 --> 00:38:35,849
假如你将框架从Caffe
切换到Keras

675
00:38:36,250 --> 00:38:39,853
切换转换器与更新命名空间一样简单

676
00:38:40,020 --> 00:38:44,124
因为所有转换函数
都共享同一个高层级API

677
00:38:52,299 --> 00:38:57,204
Core ML工具支持Caffe和
Keras模型作为神经网络

678
00:38:57,471 --> 00:38:59,273
为Pipeline支持
Scikit-Learn

679
00:39:00,140 --> 00:39:03,143
为Tree Ensemble
支持Scikit-Learn和XGBoost

680
00:39:03,544 --> 00:39:06,914
并且为线性模型
支持LIBSVM和Scikit-Learn

681
00:39:06,980 --> 00:39:08,615
以及支持向量机

682
00:39:09,149 --> 00:39:13,387
还有一点值得注意就是Keras
真的是一个很强大的高层级界面

683
00:39:13,453 --> 00:39:15,822
对于许多流行的深度学习训练工具来说

684
00:39:15,889 --> 00:39:17,257
包括TensorFlow

685
00:39:17,658 --> 00:39:20,093
如果你正在Keras中
训练TensorFlow

686
00:39:20,227 --> 00:39:24,031
你可以使用Core ML工具
将其转换为Core ML模型格式

687
00:39:32,039 --> 00:39:35,509
获取模型 你可以在两个地方查看

688
00:39:35,809 --> 00:39:39,646
一个是developer.apple.com上的
示例模型集合

689
00:39:39,847 --> 00:39:43,817
再一次 这些是预训练的模型
已经是Core ML模型格式了

690
00:39:43,917 --> 00:39:46,019
所以这是最简单的上手方式

691
00:39:46,086 --> 00:39:47,321
如果你是机器学习方面的新手

692
00:39:47,454 --> 00:39:51,258
或者如果这些模型中的一个能实现
你在应用中尝试实现的功能的话

693
00:39:51,959 --> 00:39:54,494
但是因为那儿有许多机器学习

694
00:39:54,561 --> 00:39:57,931
用例中的各种模型有各种模式

695
00:39:58,165 --> 00:39:59,800
我们创建了Core ML工具

696
00:39:59,967 --> 00:40:03,036
允许你将其中任意一种流行格式转换为

697
00:40:03,170 --> 00:40:04,705
Core ML模型格式

698
00:40:11,812 --> 00:40:14,281
那么总结一下 我们今天学习到了

699
00:40:14,615 --> 00:40:17,084
Core ML使得在

700
00:40:17,184 --> 00:40:19,686
你的应用中整合机器学习模型
变得非常简单

701
00:40:19,887 --> 00:40:21,822
只需要拖拽到Xcode中

702
00:40:21,889 --> 00:40:23,924
然后就会获得一个模型的代码接口

703
00:40:25,492 --> 00:40:29,496
Core ML对于各种各样的用例
拥有丰富的数据类型支持

704
00:40:29,630 --> 00:40:32,065
它可以处理你已经很熟悉的

705
00:40:32,132 --> 00:40:33,066
应用代码中的数据类型

706
00:40:35,636 --> 00:40:37,271
Core ML针对硬件执行了优化

707
00:40:37,437 --> 00:40:39,439
它创建在性能基元之上

708
00:40:39,506 --> 00:40:41,909
比如Metal性能着色器
和Accelerate

709
00:40:42,276 --> 00:40:45,245
你可以在设备上获得最佳性能

710
00:40:47,014 --> 00:40:49,082
通过Core ML工具

711
00:40:49,349 --> 00:40:53,487
Core ML与最流行的机器学习
格式一致

712
00:40:53,720 --> 00:40:55,322
并且随着时间会添加更多格式

713
00:40:57,591 --> 00:41:00,594
要获取更多信息
请访问developer.apple.com

714
00:41:00,694 --> 00:41:02,663
我们的演讲编号为710

715
00:41:04,231 --> 00:41:06,033
之后还有一些相关的演讲

716
00:41:06,099 --> 00:41:07,467
你们可能会有兴趣参加

717
00:41:07,534 --> 00:41:08,869
要查看一些次要的具体信息

718
00:41:08,936 --> 00:41:11,505
关于我们如何在硬件上
获得如此好的性能

719
00:41:11,738 --> 00:41:14,141
请参看“Accelerate和Metal 2”演讲

720
00:41:14,374 --> 00:41:15,209
谢谢大家

