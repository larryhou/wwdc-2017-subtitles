1
00:00:22,516 --> 00:00:26,546
[ Applause ]

2
00:00:27,046 --> 00:00:28,206
I hope you're have a great time

3
00:00:28,206 --> 00:00:29,006
at WWDC so far.

4
00:00:29,776 --> 00:00:31,216
Allow me to introduce myself.

5
00:00:31,906 --> 00:00:33,646
My name is Brett Keating and I'm

6
00:00:33,646 --> 00:00:34,896
here with my colleague Frank

7
00:00:34,896 --> 00:00:36,386
Doepke and we're here to tell

8
00:00:36,386 --> 00:00:38,256
you about Apple's new Vision

9
00:00:38,256 --> 00:00:39,836
framework, so let's get started.

10
00:00:39,836 --> 00:00:42,506
We're going to begin by showing

11
00:00:42,506 --> 00:00:43,716
you what Vision can do for your

12
00:00:43,716 --> 00:00:44,266
apps.

13
00:00:44,746 --> 00:00:45,686
We're going to go through a few

14
00:00:45,686 --> 00:00:47,126
visual examples of the

15
00:00:47,126 --> 00:00:48,106
algorithms that are going to be

16
00:00:48,106 --> 00:00:49,016
made available in the Vision

17
00:00:49,016 --> 00:00:49,766
framework this year.

18
00:00:50,936 --> 00:00:51,976
At which point I'll hand it off

19
00:00:51,976 --> 00:00:54,636
to Frank to talk about the

20
00:00:54,636 --> 00:00:56,116
concepts behind the Vision

21
00:00:56,116 --> 00:00:57,566
framework, why we designed

22
00:00:57,566 --> 00:00:58,836
things the way we did, what the

23
00:00:58,836 --> 00:01:00,696
mental model behind our API is.

24
00:01:00,796 --> 00:01:02,516
And then we'll go a little

25
00:01:02,516 --> 00:01:04,906
deeper and go through a code

26
00:01:04,906 --> 00:01:05,446
example.

27
00:01:06,006 --> 00:01:08,796
This code example brings

28
00:01:08,796 --> 00:01:09,716
together a few different

29
00:01:09,716 --> 00:01:11,386
technologies in our SDK,

30
00:01:11,496 --> 00:01:14,566
including Core Image, as well as

31
00:01:14,566 --> 00:01:16,406
the brand-new Core ML framework

32
00:01:16,656 --> 00:01:17,516
that we're offering this year

33
00:01:18,386 --> 00:01:19,926
which enables you to put in your

34
00:01:19,926 --> 00:01:21,226
own custom models and have them

35
00:01:21,276 --> 00:01:23,746
be accelerated using our

36
00:01:24,896 --> 00:01:26,106
hardware.

37
00:01:26,896 --> 00:01:28,716
So, let's begin with what you

38
00:01:28,716 --> 00:01:29,326
can do with Vision.

39
00:01:30,596 --> 00:01:32,986
Let's start off with face

40
00:01:32,986 --> 00:01:33,466
detection.

41
00:01:33,466 --> 00:01:35,676
Now face detection is something

42
00:01:35,676 --> 00:01:37,136
we already have in our SDK, but

43
00:01:37,866 --> 00:01:39,126
we're offering in the Vision

44
00:01:39,126 --> 00:01:40,536
framework new this year a face

45
00:01:40,536 --> 00:01:41,646
detection that's based on deep

46
00:01:41,646 --> 00:01:42,036
learning.

47
00:01:43,246 --> 00:01:44,776
And you may already know that

48
00:01:44,986 --> 00:01:46,006
deep learning has made

49
00:01:46,496 --> 00:01:47,786
groundbreaking changes in the

50
00:01:47,786 --> 00:01:49,916
accuracy in what we can do with

51
00:01:49,986 --> 00:01:51,296
Vision technologies and face

52
00:01:51,296 --> 00:01:53,386
detection is no exception.

53
00:01:54,136 --> 00:01:54,886
We're going to have higher

54
00:01:54,886 --> 00:01:56,086
precision which means fewer

55
00:01:56,086 --> 00:01:57,686
false positives, but we are also

56
00:01:57,686 --> 00:01:58,906
going to have dramatically

57
00:01:59,326 --> 00:02:01,806
higher recall which means we'll

58
00:02:01,806 --> 00:02:02,796
miss less faces.

59
00:02:03,286 --> 00:02:04,156
So, let's look at some of the

60
00:02:04,156 --> 00:02:05,986
examples of faces that we will

61
00:02:05,986 --> 00:02:07,446
now be able to detect with the

62
00:02:07,446 --> 00:02:08,106
Vision framework.

63
00:02:08,936 --> 00:02:10,515
For one thing, we'll be able to

64
00:02:10,515 --> 00:02:11,756
detect smaller faces.

65
00:02:13,936 --> 00:02:15,536
We'll also be doing a better job

66
00:02:15,536 --> 00:02:17,066
of detecting strong profiles.

67
00:02:19,516 --> 00:02:21,386
We'll also do a better job

68
00:02:21,846 --> 00:02:23,036
detecting more partially

69
00:02:23,036 --> 00:02:25,616
occluded faces and that includes

70
00:02:25,826 --> 00:02:27,276
things like hats and glasses.

71
00:02:27,276 --> 00:02:30,506
Sticking with the faces theme

72
00:02:30,506 --> 00:02:33,496
for a little longer, we now are

73
00:02:33,496 --> 00:02:34,786
offering in the Vision framework

74
00:02:34,786 --> 00:02:37,006
new this year face landmarks,

75
00:02:37,656 --> 00:02:38,596
what are face landmarks?

76
00:02:39,496 --> 00:02:41,026
This is a constellation of

77
00:02:41,026 --> 00:02:42,006
points that we detect on the

78
00:02:42,006 --> 00:02:43,696
facer, things like the corners

79
00:02:43,696 --> 00:02:45,036
of the eyes, the outline of the

80
00:02:45,036 --> 00:02:47,036
mouth, the contour of the chin.

81
00:02:48,316 --> 00:02:50,876
Here's an example, here's

82
00:02:50,876 --> 00:02:53,766
another example, and one more

83
00:02:53,766 --> 00:02:54,266
example.

84
00:02:55,396 --> 00:02:56,946
We're really excited about this

85
00:02:56,946 --> 00:02:57,956
I think there's going to be some

86
00:02:57,956 --> 00:02:59,056
great apps created with this

87
00:02:59,056 --> 00:02:59,596
technology.

88
00:03:01,996 --> 00:03:03,696
Next, also new this year in the

89
00:03:03,696 --> 00:03:04,776
Vision framework is image

90
00:03:04,776 --> 00:03:05,426
registration.

91
00:03:06,026 --> 00:03:07,176
If you don't know what image

92
00:03:07,176 --> 00:03:08,546
registration is it's basically

93
00:03:08,876 --> 00:03:11,126
aligning two images based on the

94
00:03:11,126 --> 00:03:12,226
features that are present in

95
00:03:12,226 --> 00:03:12,876
those images.

96
00:03:13,496 --> 00:03:15,826
You can use this for stitching

97
00:03:15,826 --> 00:03:17,166
together used for panorama kind

98
00:03:17,286 --> 00:03:18,686
of like this example or image

99
00:03:18,686 --> 00:03:19,726
stacking applications.

100
00:03:20,536 --> 00:03:22,406
We have two different kinds, one

101
00:03:22,406 --> 00:03:24,126
that's translation only and one

102
00:03:24,126 --> 00:03:24,756
that gives you for full

103
00:03:24,756 --> 00:03:27,176
homography for greater accuracy.

104
00:03:28,656 --> 00:03:31,136
We're also offering a few

105
00:03:31,136 --> 00:03:32,276
technologies that are already in

106
00:03:32,276 --> 00:03:33,566
our SDK through CIDetector

107
00:03:33,566 --> 00:03:34,406
interface.

108
00:03:34,796 --> 00:03:35,706
We're making them available in

109
00:03:35,706 --> 00:03:36,776
the Vision API as well.

110
00:03:36,896 --> 00:03:38,766
That includes rectangle

111
00:03:38,766 --> 00:03:40,516
detection as you can see, we

112
00:03:40,516 --> 00:03:42,296
detect the sign in the picture.

113
00:03:43,816 --> 00:03:45,566
We're also doing barcode

114
00:03:45,566 --> 00:03:46,776
detection and recognition in the

115
00:03:46,776 --> 00:03:50,726
Vision API and text detection as

116
00:03:52,256 --> 00:03:52,376
well.

117
00:03:53,766 --> 00:03:54,866
Another new technology,

118
00:03:55,116 --> 00:03:55,976
brand-new in the Vision

119
00:03:55,976 --> 00:03:56,986
framework this year is object

120
00:03:56,986 --> 00:03:57,366
tracking.

121
00:03:58,186 --> 00:04:00,576
You can use this to track a face

122
00:04:00,576 --> 00:04:01,726
if you've detected a face.

123
00:04:01,726 --> 00:04:03,246
You can use that face rectangle

124
00:04:03,246 --> 00:04:05,186
as an initial condition to the

125
00:04:05,186 --> 00:04:06,436
tracking and then the Vision

126
00:04:06,436 --> 00:04:07,756
framework will track that square

127
00:04:08,196 --> 00:04:09,056
throughout the rest of your

128
00:04:09,056 --> 00:04:09,446
video.

129
00:04:10,246 --> 00:04:12,486
Will also track rectangles and

130
00:04:12,486 --> 00:04:14,036
you can also define the initial

131
00:04:14,036 --> 00:04:15,036
condition yourself.

132
00:04:15,846 --> 00:04:17,426
So that's what I mean by general

133
00:04:17,426 --> 00:04:19,456
templates, if you decide to for

134
00:04:19,456 --> 00:04:21,305
example, put a square around

135
00:04:21,305 --> 00:04:24,656
this wakeboarder as I have, you

136
00:04:24,656 --> 00:04:27,116
can then go ahead and track

137
00:04:27,116 --> 00:04:27,376
that.

138
00:04:29,336 --> 00:04:30,556
You can see that we handle

139
00:04:30,556 --> 00:04:32,666
pretty large changes in scale,

140
00:04:32,756 --> 00:04:34,396
pretty large deformations fairly

141
00:04:34,396 --> 00:04:35,746
robustly with this technology.

142
00:04:39,096 --> 00:04:40,266
Another really exciting

143
00:04:41,296 --> 00:04:42,956
technology that's new in Apple's

144
00:04:42,956 --> 00:04:45,086
SDK this year Core ML and you

145
00:04:45,086 --> 00:04:46,126
can integrate your Core ML

146
00:04:46,126 --> 00:04:47,576
models directly into Vision.

147
00:04:48,236 --> 00:04:50,916
As I've mentioned, machine

148
00:04:50,916 --> 00:04:52,496
learning does great things for

149
00:04:52,496 --> 00:04:55,336
Computer Vision and you can use

150
00:04:55,336 --> 00:04:57,066
Core ML if you want to create

151
00:04:57,066 --> 00:04:58,166
your own models, do your own

152
00:04:58,166 --> 00:04:58,716
solution.

153
00:04:59,416 --> 00:05:01,246
Perhaps for example, you want to

154
00:05:01,246 --> 00:05:02,476
create a wedding application

155
00:05:02,986 --> 00:05:06,316
where you're able to detect this

156
00:05:06,316 --> 00:05:08,006
part of the wedding is the

157
00:05:08,006 --> 00:05:08,956
reception, this part of the

158
00:05:08,956 --> 00:05:09,816
wedding is where the bride is

159
00:05:09,816 --> 00:05:10,686
walking down the aisle.

160
00:05:11,416 --> 00:05:12,456
If you want to train your own

161
00:05:12,456 --> 00:05:14,706
model and you have the data to

162
00:05:14,706 --> 00:05:17,196
train your own model you can do

163
00:05:17,196 --> 00:05:17,476
that.

164
00:05:18,556 --> 00:05:20,766
Core ML as I mentioned, provides

165
00:05:20,766 --> 00:05:22,116
native acceleration for custom

166
00:05:22,116 --> 00:05:23,206
models so they'll run really

167
00:05:23,206 --> 00:05:25,706
fast and Vision provides the

168
00:05:25,706 --> 00:05:26,876
imaging pipeline to support

169
00:05:26,876 --> 00:05:28,776
these models, so you won't have

170
00:05:28,776 --> 00:05:30,276
to do any rescaling or anything

171
00:05:30,276 --> 00:05:31,156
like that we'll take care of all

172
00:05:31,156 --> 00:05:31,586
that for you.

173
00:05:31,586 --> 00:05:33,106
We know what your model is

174
00:05:33,106 --> 00:05:34,476
expecting and we'll put the

175
00:05:34,476 --> 00:05:35,626
image in the right format.

176
00:05:37,596 --> 00:05:38,826
If you're interested in Core ML

177
00:05:38,826 --> 00:05:40,396
there's some sessions that you

178
00:05:40,396 --> 00:05:42,576
can go to, we've listed the labs

179
00:05:42,576 --> 00:05:43,306
down here for you.

180
00:05:43,866 --> 00:05:45,346
One of them will be tomorrow

181
00:05:45,346 --> 00:05:47,356
morning and then another one on

182
00:05:47,356 --> 00:05:47,986
Friday afternoon.

183
00:05:49,446 --> 00:05:51,386
So that's basically the features

184
00:05:51,386 --> 00:05:52,346
that are in the Vision

185
00:05:52,346 --> 00:05:52,796
framework.

186
00:05:54,236 --> 00:05:56,526
Overall, what Apple's new Vision

187
00:05:56,526 --> 00:05:58,306
framework provides are

188
00:05:58,766 --> 00:06:00,776
high-level on-device solutions

189
00:06:01,306 --> 00:06:02,586
to Computer Vision problems

190
00:06:02,706 --> 00:06:03,756
through one simple API.

191
00:06:03,756 --> 00:06:06,576
Now let me break this statement

192
00:06:06,576 --> 00:06:08,266
down just a little bit.

193
00:06:09,276 --> 00:06:10,586
What do I mean by high-level

194
00:06:10,586 --> 00:06:11,186
solutions?

195
00:06:11,896 --> 00:06:14,336
Well we don't want you to have

196
00:06:14,336 --> 00:06:15,556
to be a Computer Vision expert

197
00:06:15,556 --> 00:06:16,876
to put the magic of Computer

198
00:06:16,876 --> 00:06:18,066
Vision into your applications.

199
00:06:18,656 --> 00:06:20,706
You don't want to necessarily

200
00:06:20,706 --> 00:06:22,216
have to know which feature

201
00:06:22,216 --> 00:06:23,636
detector you want to use in

202
00:06:23,636 --> 00:06:25,086
combination with what classifier

203
00:06:25,086 --> 00:06:26,966
or set of classifiers, we're

204
00:06:26,966 --> 00:06:28,366
going to handle that for you or

205
00:06:28,366 --> 00:06:29,196
whether or not you want to use

206
00:06:29,196 --> 00:06:30,216
machine learning for example.

207
00:06:30,976 --> 00:06:31,946
If you're a developer you're

208
00:06:32,326 --> 00:06:33,426
probably thinking I just want to

209
00:06:33,426 --> 00:06:34,246
know where the faces are.

210
00:06:35,626 --> 00:06:36,506
And so, we're going to handle

211
00:06:36,506 --> 00:06:37,666
all that complexity for you.

212
00:06:38,926 --> 00:06:42,166
Depending on your use case we'll

213
00:06:42,166 --> 00:06:43,896
be doing either traditional

214
00:06:43,896 --> 00:06:45,206
approach if that's what's needed

215
00:06:45,206 --> 00:06:46,846
for maybe real-time applications

216
00:06:46,846 --> 00:06:49,166
or deep learning algorithms for

217
00:06:49,416 --> 00:06:50,176
higher accuracy.

218
00:06:50,846 --> 00:06:54,186
Now I also mentioned that we're

219
00:06:54,186 --> 00:06:55,666
doing all these algorithms on

220
00:06:55,666 --> 00:06:58,116
the device, let's talk a little

221
00:06:58,116 --> 00:07:00,436
bit about why we'd want to do

222
00:07:00,436 --> 00:07:01,916
things on device versus provided

223
00:07:01,916 --> 00:07:02,906
a cloud-based solution.

224
00:07:03,396 --> 00:07:05,766
First of all, it's privacy.

225
00:07:06,866 --> 00:07:08,836
As you know, Apple cares a lot

226
00:07:08,836 --> 00:07:11,006
about privacy, I care a lot

227
00:07:11,006 --> 00:07:12,476
about privacy working at Apple,

228
00:07:12,796 --> 00:07:14,136
sometimes it makes my job a

229
00:07:14,136 --> 00:07:16,446
little harder, but nonetheless

230
00:07:17,026 --> 00:07:18,056
keeping all your data on the

231
00:07:18,056 --> 00:07:20,156
device is the best way to

232
00:07:20,156 --> 00:07:21,736
protect your user's data

233
00:07:21,736 --> 00:07:22,176
privacy.

234
00:07:24,456 --> 00:07:26,276
Furthermore, with certain

235
00:07:26,276 --> 00:07:28,036
cloud-based solutions there's a

236
00:07:28,036 --> 00:07:29,106
cost associated with it.

237
00:07:29,306 --> 00:07:31,616
If you're a developer maybe

238
00:07:31,616 --> 00:07:32,796
you're paying usage fees to use

239
00:07:32,796 --> 00:07:33,976
a cloud-based solution.

240
00:07:35,316 --> 00:07:37,146
Your users will have to transfer

241
00:07:37,146 --> 00:07:38,066
the data to that cloud.

242
00:07:39,516 --> 00:07:41,306
All these costs they can add up

243
00:07:41,306 --> 00:07:42,296
for both the developers and the

244
00:07:42,296 --> 00:07:42,726
users.

245
00:07:42,956 --> 00:07:44,506
So, when everything's on the

246
00:07:44,506 --> 00:07:45,586
device it's free.

247
00:07:45,816 --> 00:07:50,566
And you can support real-time

248
00:07:50,566 --> 00:07:51,966
use cases like the tracking

249
00:07:51,966 --> 00:07:52,756
example I showed you.

250
00:07:53,566 --> 00:07:54,616
Imagine trying to track

251
00:07:54,616 --> 00:07:55,536
something through a video by

252
00:07:55,536 --> 00:07:56,416
sending every frame to the

253
00:07:56,416 --> 00:07:57,926
cloud, I don't think that's

254
00:07:57,926 --> 00:07:58,556
going to work too well.

255
00:07:59,226 --> 00:08:01,456
So, no latency, fast execution

256
00:08:01,926 --> 00:08:02,946
that's what we're offering with

257
00:08:03,226 --> 00:08:03,896
the Vision framework.

258
00:08:04,786 --> 00:08:07,696
So, I hope you enjoyed that

259
00:08:08,076 --> 00:08:09,186
introduction, now we're going to

260
00:08:09,186 --> 00:08:12,226
go a little deeper and talk

261
00:08:12,226 --> 00:08:13,446
about the Vision concepts.

262
00:08:13,446 --> 00:08:13,996
For this part of the

263
00:08:13,996 --> 00:08:15,356
presentation I'm going to hand

264
00:08:15,356 --> 00:08:15,976
it off to Frank.

265
00:08:16,516 --> 00:08:19,566
[ Applause ]

266
00:08:20,066 --> 00:08:20,556
>> Thank you Brett.

267
00:08:22,956 --> 00:08:24,406
Hi, good afternoon, my name is

268
00:08:24,406 --> 00:08:25,466
Frank Doepke and I'm going to

269
00:08:25,466 --> 00:08:26,996
talk about more of the technical

270
00:08:26,996 --> 00:08:28,876
details what is part of our

271
00:08:28,876 --> 00:08:29,496
Vision framework.

272
00:08:29,796 --> 00:08:33,916
So, what do we want to do, when

273
00:08:33,916 --> 00:08:35,726
we want to analyze an image we

274
00:08:35,916 --> 00:08:37,836
have three major tasks that we

275
00:08:37,836 --> 00:08:39,816
actually want to perform.

276
00:08:40,275 --> 00:08:41,416
So, we [inaudible] finding out

277
00:08:41,416 --> 00:08:42,576
what is in the image and what do

278
00:08:42,576 --> 00:08:43,456
I want to know about it.

279
00:08:44,316 --> 00:08:45,356
There's the machinery,

280
00:08:46,076 --> 00:08:47,166
somebody's got to do the work

281
00:08:47,676 --> 00:08:48,956
and we get some results out of

282
00:08:48,956 --> 00:08:50,316
it, at least we hope that's

283
00:08:50,316 --> 00:08:51,206
what's going to happen.

284
00:08:52,106 --> 00:08:54,036
So, in terminology for Vision

285
00:08:54,036 --> 00:08:56,046
that means the asks these are

286
00:08:56,046 --> 00:08:56,836
requests.

287
00:08:57,476 --> 00:08:59,166
And I just did a few examples

288
00:08:59,166 --> 00:09:01,016
here like the barcode detection

289
00:09:01,016 --> 00:09:03,936
or face detection and we feed

290
00:09:03,936 --> 00:09:07,016
them into our request handler.

291
00:09:07,926 --> 00:09:08,996
That's the one in this case to

292
00:09:08,996 --> 00:09:10,106
be an image request and

293
00:09:10,176 --> 00:09:11,136
[inaudible] hold on to the image

294
00:09:11,446 --> 00:09:12,576
and it's going to do all the

295
00:09:12,576 --> 00:09:13,416
work for us.

296
00:09:14,066 --> 00:09:16,486
And as a result, we get back

297
00:09:16,526 --> 00:09:18,086
what we call observations, what

298
00:09:18,086 --> 00:09:19,446
did we observe in this image.

299
00:09:20,046 --> 00:09:21,666
And these observations depend on

300
00:09:21,666 --> 00:09:22,726
what you asked us to do.

301
00:09:23,006 --> 00:09:24,516
So, we have classification

302
00:09:24,516 --> 00:09:26,956
observation or detected objects.

303
00:09:27,786 --> 00:09:28,986
Now when you want to track

304
00:09:28,986 --> 00:09:30,076
something in the sequence like

305
00:09:30,076 --> 00:09:32,656
the wakeboarder it's basically

306
00:09:32,656 --> 00:09:33,466
the same concept.

307
00:09:33,566 --> 00:09:34,796
We have some asks, we have the

308
00:09:34,796 --> 00:09:36,376
machinery, and we get some

309
00:09:36,376 --> 00:09:39,096
results out of it in the end.

310
00:09:39,096 --> 00:09:40,826
Again, the asks are requests.

311
00:09:41,426 --> 00:09:42,936
Now since this changes with

312
00:09:42,936 --> 00:09:44,686
every frame the image actually

313
00:09:44,686 --> 00:09:45,926
travels with the request.

314
00:09:47,736 --> 00:09:48,936
Our machinery is again

315
00:09:48,936 --> 00:09:49,826
[inaudible] request handler it's

316
00:09:49,826 --> 00:09:52,506
the sequence request handler and

317
00:09:52,506 --> 00:09:54,046
we get results which are

318
00:09:54,046 --> 00:09:55,906
observations that go with our

319
00:09:55,976 --> 00:09:56,546
requests.

320
00:09:58,086 --> 00:10:00,866
So, let me talk a little bit

321
00:10:00,866 --> 00:10:02,066
more about these two image

322
00:10:02,066 --> 00:10:02,966
request handlers that I

323
00:10:02,966 --> 00:10:03,806
mentioned so far.

324
00:10:04,476 --> 00:10:05,606
So, we have the image request

325
00:10:05,606 --> 00:10:07,666
handler that is mostly if you

326
00:10:07,666 --> 00:10:09,026
want to do something interactive

327
00:10:09,026 --> 00:10:09,736
with the image.

328
00:10:10,166 --> 00:10:11,386
You want to do multiple Vision

329
00:10:11,386 --> 00:10:13,466
tasks on an image, sometimes you

330
00:10:13,466 --> 00:10:15,116
actually do one and then based

331
00:10:15,116 --> 00:10:16,566
on the results you then kick off

332
00:10:16,566 --> 00:10:17,926
the next one and that's what you

333
00:10:17,926 --> 00:10:19,016
want to use the image request

334
00:10:19,016 --> 00:10:19,486
handler for.

335
00:10:19,966 --> 00:10:21,296
It'll hold on to the image that

336
00:10:21,296 --> 00:10:22,446
it's set up with for its

337
00:10:22,446 --> 00:10:25,366
lifecycle and that allows us

338
00:10:25,396 --> 00:10:26,856
under the cover to do

339
00:10:26,856 --> 00:10:28,856
performance optimizations by

340
00:10:28,856 --> 00:10:30,996
holding on to intermediates to

341
00:10:31,146 --> 00:10:32,866
make these requests perform

342
00:10:32,866 --> 00:10:33,276
faster.

343
00:10:34,016 --> 00:10:36,466
On the flipside, if I want to

344
00:10:36,466 --> 00:10:37,726
track something we use the

345
00:10:37,726 --> 00:10:38,816
sequence request handler.

346
00:10:39,276 --> 00:10:40,516
The sequence request handler

347
00:10:40,516 --> 00:10:42,086
allows us to keep tracking

348
00:10:42,186 --> 00:10:42,996
[inaudible] in the sequence

349
00:10:42,996 --> 00:10:43,666
request handler.

350
00:10:44,316 --> 00:10:46,136
And it will not hold on to all

351
00:10:46,136 --> 00:10:47,466
the images that gets fed into it

352
00:10:47,466 --> 00:10:48,626
over its lifecycle so they get

353
00:10:48,626 --> 00:10:49,326
released earlier.

354
00:10:50,056 --> 00:10:51,536
But that means on the flipside,

355
00:10:51,776 --> 00:10:53,486
you cannot do the same

356
00:10:53,486 --> 00:10:54,866
optimizations if you want to do

357
00:10:54,866 --> 00:10:56,146
multiple requests on the same

358
00:10:56,146 --> 00:10:56,486
image.

359
00:10:57,716 --> 00:10:59,836
So how does this look in the

360
00:10:59,836 --> 00:11:01,596
code, we are developers that's

361
00:11:01,596 --> 00:11:02,526
what we want to see.

362
00:11:04,116 --> 00:11:05,606
So, we start as a blank slate

363
00:11:05,606 --> 00:11:07,316
that's always good and then we

364
00:11:07,316 --> 00:11:08,446
create a request.

365
00:11:08,886 --> 00:11:10,466
In this case, it's a face

366
00:11:10,466 --> 00:11:12,746
detection request.

367
00:11:13,346 --> 00:11:14,496
Now we create the request

368
00:11:14,496 --> 00:11:16,546
handler, what I'm choosing here

369
00:11:16,546 --> 00:11:17,816
is a request handler based on

370
00:11:17,816 --> 00:11:19,216
the files so I have a file on

371
00:11:19,216 --> 00:11:22,556
disk that I want to use.

372
00:11:22,556 --> 00:11:24,026
Now I ask myRequestHandler to

373
00:11:24,026 --> 00:11:25,646
perform my request and this in

374
00:11:25,646 --> 00:11:27,566
this case [inaudible] it's just

375
00:11:27,566 --> 00:11:29,716
one request I have my array, but

376
00:11:29,716 --> 00:11:32,746
it could be many and I get my

377
00:11:32,906 --> 00:11:33,836
observations back.

378
00:11:34,996 --> 00:11:36,606
And this can be many faces that

379
00:11:36,606 --> 00:11:37,266
I detected.

380
00:11:37,836 --> 00:11:39,216
Now the one thing I would like

381
00:11:39,216 --> 00:11:42,746
to highlight here is the results

382
00:11:43,006 --> 00:11:45,036
come back as part of the request

383
00:11:45,156 --> 00:11:46,076
that we actually set up

384
00:11:46,076 --> 00:11:46,526
initially.

385
00:11:46,526 --> 00:11:49,916
How does it look when we want to

386
00:11:51,076 --> 00:11:52,506
track something?

387
00:11:52,616 --> 00:11:53,946
We create a sequence request

388
00:11:54,966 --> 00:11:55,776
handler [inaudible] of course

389
00:11:55,776 --> 00:11:57,026
not set up as an image because

390
00:11:57,216 --> 00:11:58,236
we have to [inaudible] all the

391
00:11:58,236 --> 00:11:59,656
frames of the sequence.

392
00:12:01,476 --> 00:12:02,326
So, I started with an

393
00:12:02,326 --> 00:12:03,906
observation that I got from the

394
00:12:03,906 --> 00:12:05,906
previous detection or I mark

395
00:12:05,956 --> 00:12:07,596
something up and I create my

396
00:12:07,596 --> 00:12:09,276
tracking request.

397
00:12:09,826 --> 00:12:12,026
And I simply have to run the

398
00:12:12,026 --> 00:12:12,596
request.

399
00:12:13,246 --> 00:12:15,036
And I feed in in this case as a

400
00:12:15,036 --> 00:12:17,026
pixel buffer the frame that is

401
00:12:17,026 --> 00:12:17,996
currently being dragged.

402
00:12:19,406 --> 00:12:21,176
And out of it again I get some

403
00:12:21,176 --> 00:12:21,616
results.

404
00:12:22,246 --> 00:12:25,246
So, now that we have talked

405
00:12:25,246 --> 00:12:26,816
about how this API is kind of

406
00:12:26,816 --> 00:12:28,426
structured I would like to guide

407
00:12:28,426 --> 00:12:30,066
you through some best practices

408
00:12:30,066 --> 00:12:31,616
so that you get, you know, the

409
00:12:31,616 --> 00:12:33,006
best experience out of Vision.

410
00:12:33,666 --> 00:12:37,456
So, when we want to put together

411
00:12:37,456 --> 00:12:38,936
a Computer Vision task you have

412
00:12:38,936 --> 00:12:39,886
to think about a few things.

413
00:12:41,476 --> 00:12:43,216
Number one, what is the right

414
00:12:43,216 --> 00:12:44,586
image type that I want to use.

415
00:12:45,826 --> 00:12:47,876
Number two, what am I going to

416
00:12:47,876 --> 00:12:48,746
do with the image.

417
00:12:50,396 --> 00:12:52,196
And number three, what

418
00:12:52,226 --> 00:12:53,496
performance do I need or want.

419
00:12:53,496 --> 00:12:54,346
Of course, you always want

420
00:12:54,346 --> 00:12:55,346
fastest, but there are some

421
00:12:55,346 --> 00:12:56,356
tradeoffs that you have to think

422
00:12:56,356 --> 00:12:56,636
about.

423
00:12:57,026 --> 00:12:59,136
So, let's talk about the image

424
00:13:00,526 --> 00:13:00,636
type.

425
00:13:00,846 --> 00:13:02,346
Vision supports a number of

426
00:13:02,346 --> 00:13:04,196
image types and they range from

427
00:13:04,196 --> 00:13:06,836
CVPixelBuffer, CGIImage or even

428
00:13:06,836 --> 00:13:09,276
as we saw in the previous

429
00:13:09,276 --> 00:13:11,446
example just from data that I

430
00:13:11,446 --> 00:13:12,606
use in NSURL.

431
00:13:13,216 --> 00:13:15,776
And we go over all these types

432
00:13:15,886 --> 00:13:17,076
in the following slides so that

433
00:13:17,076 --> 00:13:18,396
you know what to choose when.

434
00:13:20,556 --> 00:13:22,756
Which to choose depends a lot of

435
00:13:22,756 --> 00:13:23,916
like what you want to do.

436
00:13:23,916 --> 00:13:25,836
If you run from a camera stream

437
00:13:25,836 --> 00:13:27,596
or if you run from files on disk

438
00:13:28,346 --> 00:13:29,996
you have to look at that

439
00:13:30,096 --> 00:13:31,396
[inaudible] kind of which type

440
00:13:31,396 --> 00:13:32,856
of image you want to use.

441
00:13:33,246 --> 00:13:34,536
Now two important things to

442
00:13:34,536 --> 00:13:37,576
remember is we already have an

443
00:13:37,666 --> 00:13:39,206
imaging pipeline in the Vision

444
00:13:39,206 --> 00:13:41,076
framework you don't need to

445
00:13:41,076 --> 00:13:42,006
scale the images.

446
00:13:42,406 --> 00:13:43,616
So, unless you already have a

447
00:13:43,616 --> 00:13:44,926
very small representation that

448
00:13:44,926 --> 00:13:45,806
you absolutely want to use,

449
00:13:45,856 --> 00:13:47,136
please don't pre-scale because

450
00:13:47,246 --> 00:13:50,476
we'll just do the work twice.

451
00:13:50,666 --> 00:13:52,356
And mind the orientation.

452
00:13:52,526 --> 00:13:54,086
Computer Vision algorithms are

453
00:13:54,086 --> 00:13:57,056
mostly not, you know, sensitive

454
00:13:57,056 --> 00:13:59,226
to orientation or sorry, they

455
00:13:59,356 --> 00:14:01,176
are sensitive to orientation so

456
00:14:01,236 --> 00:14:02,576
you have to pass that in.

457
00:14:03,166 --> 00:14:04,586
And that is an important part

458
00:14:04,586 --> 00:14:05,676
because if you pass in a

459
00:14:05,816 --> 00:14:07,206
portrait image that's actually

460
00:14:07,206 --> 00:14:08,166
lying on its side we will not

461
00:14:08,166 --> 00:14:09,616
find the faces and that's one of

462
00:14:09,616 --> 00:14:10,996
the common mistakes that usually

463
00:14:10,996 --> 00:14:11,386
happens.

464
00:14:12,906 --> 00:14:14,396
So, I promised to go over the

465
00:14:14,396 --> 00:14:14,846
types.

466
00:14:15,796 --> 00:14:16,696
When you want to do something

467
00:14:16,696 --> 00:14:17,866
streaming we want to use the

468
00:14:17,866 --> 00:14:18,746
CVPixelBuffer.

469
00:14:19,906 --> 00:14:21,956
When you create a VideoDataOut

470
00:14:21,956 --> 00:14:23,206
[inaudible] capture you will get

471
00:14:23,286 --> 00:14:24,776
CMSampleBuffers and through

472
00:14:24,916 --> 00:14:25,806
those we get your

473
00:14:25,866 --> 00:14:26,686
CVPixelBuffers.

474
00:14:27,856 --> 00:14:29,456
It's also a pretty good format

475
00:14:29,456 --> 00:14:30,886
if you already have something

476
00:14:30,886 --> 00:14:32,806
where you keep your image data

477
00:14:32,806 --> 00:14:34,376
raw in memory like it's LGB

478
00:14:34,376 --> 00:14:35,936
pixels and wrap them into a

479
00:14:35,936 --> 00:14:37,276
CVPixelBuffer this is a great

480
00:14:37,276 --> 00:14:38,496
format to pass into Vision.

481
00:14:40,666 --> 00:14:41,976
When you get files from disk

482
00:14:42,146 --> 00:14:44,046
please use the URL or if it

483
00:14:44,046 --> 00:14:44,996
comes from the web use the

484
00:14:44,996 --> 00:14:46,926
NSData path.

485
00:14:47,126 --> 00:14:48,886
The great thing about that is it

486
00:14:48,886 --> 00:14:50,796
really allows us to reduce the

487
00:14:50,796 --> 00:14:51,936
memory for print in your

488
00:14:51,936 --> 00:14:52,616
application.

489
00:14:53,086 --> 00:14:54,566
Vision will only read what it

490
00:14:54,566 --> 00:14:56,236
needs to perform the task.

491
00:14:57,156 --> 00:14:58,256
If you think about you want to

492
00:14:58,256 --> 00:14:59,226
do face detection on a

493
00:14:59,226 --> 00:15:02,376
64-megapixel panorama Vision

494
00:15:02,376 --> 00:15:03,526
will actually reduce your memory

495
00:15:03,526 --> 00:15:04,936
for it, but not reading the full

496
00:15:04,936 --> 00:15:06,436
file actually into the memory

497
00:15:06,436 --> 00:15:07,466
and that is an important thing

498
00:15:07,466 --> 00:15:08,136
to keep in mind.

499
00:15:10,396 --> 00:15:12,006
We will read in this case the

500
00:15:12,006 --> 00:15:13,336
EXIF Orientation out of the

501
00:15:13,336 --> 00:15:15,336
file, but you can override it if

502
00:15:15,336 --> 00:15:17,266
you have to for those formats

503
00:15:17,266 --> 00:15:18,706
that don't support it.

504
00:15:20,666 --> 00:15:22,126
If you're already using Core

505
00:15:22,166 --> 00:15:24,076
Image in your application by all

506
00:15:24,076 --> 00:15:25,286
means process the CI image.

507
00:15:25,286 --> 00:15:27,366
This is also important when you

508
00:15:27,366 --> 00:15:28,096
want to actually do some

509
00:15:28,096 --> 00:15:28,806
preprocessing.

510
00:15:28,806 --> 00:15:29,786
If you have some domain

511
00:15:29,786 --> 00:15:30,946
knowledge of what you want to do

512
00:15:30,946 --> 00:15:33,286
in your Computer Vision task you

513
00:15:33,286 --> 00:15:34,506
can do some preprocessing and

514
00:15:34,506 --> 00:15:35,766
try and enhance the image and,

515
00:15:35,806 --> 00:15:37,186
therefore, enhance the Vision

516
00:15:37,186 --> 00:15:37,716
results.

517
00:15:39,346 --> 00:15:40,316
If you want to learn a bit more

518
00:15:40,316 --> 00:15:42,706
about Core Image, there's a

519
00:15:42,706 --> 00:15:45,006
session on Thursday at 1:50 and

520
00:15:45,396 --> 00:15:46,146
they will also show the

521
00:15:46,146 --> 00:15:47,836
integration with our Vision

522
00:15:47,836 --> 00:15:48,286
framework.

523
00:15:48,756 --> 00:15:52,276
Last but not least, if you have

524
00:15:52,276 --> 00:15:54,246
all the images in your UI you

525
00:15:54,426 --> 00:15:56,346
can use the CG image [inaudible]

526
00:15:56,716 --> 00:15:59,116
out of the NS image or the UI

527
00:15:59,226 --> 00:16:01,196
images let's say it comes from

528
00:16:01,196 --> 00:16:02,906
the UI image picker and pass

529
00:16:02,976 --> 00:16:03,686
those into Vision.

530
00:16:03,806 --> 00:16:07,006
Now what am I going to do with

531
00:16:07,096 --> 00:16:08,496
the image and that's where we

532
00:16:08,496 --> 00:16:10,236
have to decide if I want to do

533
00:16:10,236 --> 00:16:11,346
something interactive with the

534
00:16:11,426 --> 00:16:13,206
image in that case I use my

535
00:16:13,206 --> 00:16:14,336
ImageRequestHandler.

536
00:16:14,606 --> 00:16:16,226
It will hold on to the image for

537
00:16:16,226 --> 00:16:17,926
the time and I can do multiple

538
00:16:17,926 --> 00:16:19,656
passes on that image and get the

539
00:16:19,656 --> 00:16:20,866
best results out of that.

540
00:16:22,126 --> 00:16:23,426
Now the CVPixelBuffer

541
00:16:23,426 --> 00:16:24,926
technically will allow you that

542
00:16:24,926 --> 00:16:26,826
you could change the pixels

543
00:16:27,226 --> 00:16:28,216
[inaudible], but we see them as

544
00:16:28,216 --> 00:16:29,756
immutable so don't do that

545
00:16:29,756 --> 00:16:30,776
because we'll get some strange

546
00:16:30,776 --> 00:16:31,246
results.

547
00:16:31,546 --> 00:16:35,406
Next, if you want to track

548
00:16:35,606 --> 00:16:36,816
something we use the

549
00:16:36,816 --> 00:16:38,156
SequenceRequestHandler.

550
00:16:39,636 --> 00:16:40,506
It allows us to keep the

551
00:16:40,506 --> 00:16:42,946
tracking state and lifecycle of

552
00:16:42,946 --> 00:16:44,676
my image is not tied to those

553
00:16:44,736 --> 00:16:46,256
requests handler anymore, but

554
00:16:46,296 --> 00:16:47,576
just how long it needs it for

555
00:16:47,576 --> 00:16:47,976
the tracking.

556
00:16:52,176 --> 00:16:53,976
Performance, so these Vision

557
00:16:53,976 --> 00:16:55,796
tasks are computationally

558
00:16:55,796 --> 00:16:57,726
intensive very often and they do

559
00:16:57,726 --> 00:16:59,056
take time, so you have to think

560
00:16:59,056 --> 00:17:01,486
about that you want to actually

561
00:17:01,486 --> 00:17:03,376
run your task on a different

562
00:17:03,856 --> 00:17:05,465
queue not your main queue.

563
00:17:06,616 --> 00:17:08,296
And think about if you want to

564
00:17:08,296 --> 00:17:09,376
do it in the background, which

565
00:17:09,376 --> 00:17:11,086
is a bit slower or if you need

566
00:17:11,086 --> 00:17:12,356
it very quickly use a more

567
00:17:12,356 --> 00:17:13,915
interactive quality of service

568
00:17:14,366 --> 00:17:15,516
to get the performance.

569
00:17:16,596 --> 00:17:19,276
A good practice is to use the

570
00:17:19,276 --> 00:17:22,026
completion handler to get the

571
00:17:22,056 --> 00:17:23,606
results back, this is part of

572
00:17:23,606 --> 00:17:24,026
our API.

573
00:17:24,026 --> 00:17:26,165
But keep in mind that this

574
00:17:26,215 --> 00:17:27,866
completion handler gets called

575
00:17:27,915 --> 00:17:29,236
on that queue in which you

576
00:17:29,236 --> 00:17:30,686
actually set it off.

577
00:17:30,686 --> 00:17:32,576
So, if you need to update your

578
00:17:32,576 --> 00:17:33,936
UI you have to dispatch that

579
00:17:33,936 --> 00:17:34,746
back to the main queue.

580
00:17:35,426 --> 00:17:38,796
So as Brett already highlighted,

581
00:17:38,796 --> 00:17:40,036
we have a new face detection and

582
00:17:40,036 --> 00:17:41,206
you might say oh God, yet

583
00:17:41,206 --> 00:17:41,706
another one.

584
00:17:43,946 --> 00:17:45,156
But we have good reasons for

585
00:17:45,156 --> 00:17:45,446
this.

586
00:17:45,706 --> 00:17:47,546
Vision uses deep learning and

587
00:17:47,546 --> 00:17:48,976
this gives us really a lot

588
00:17:49,186 --> 00:17:50,516
better precision and recall,

589
00:17:50,926 --> 00:17:52,366
therefore, much better results.

590
00:17:52,916 --> 00:17:55,826
The downside of it, on older

591
00:17:55,826 --> 00:17:56,976
hardware it will run a bit

592
00:17:56,976 --> 00:17:57,356
slower.

593
00:17:57,436 --> 00:17:58,946
So, let's look a little bit at

594
00:17:58,946 --> 00:18:01,006
our overall landscape of face

595
00:18:01,006 --> 00:18:01,926
detectors that we have

596
00:18:01,926 --> 00:18:02,476
available.

597
00:18:03,236 --> 00:18:04,876
So, we have Vision which really

598
00:18:04,876 --> 00:18:06,346
gives us the best results and

599
00:18:06,346 --> 00:18:08,736
it's pretty fast and also pretty

600
00:18:08,736 --> 00:18:09,966
good in its power use as it is

601
00:18:09,966 --> 00:18:10,926
optimized for that.

602
00:18:11,246 --> 00:18:12,766
And we have it available on all

603
00:18:12,766 --> 00:18:14,376
platforms except the watchOS.

604
00:18:15,446 --> 00:18:16,886
And this is the same in terms of

605
00:18:16,886 --> 00:18:18,566
availability for Core Image and

606
00:18:18,566 --> 00:18:19,686
it's a bit faster, but the

607
00:18:19,686 --> 00:18:21,106
results are not quite as good.

608
00:18:21,736 --> 00:18:24,266
In the AV capture session which

609
00:18:24,266 --> 00:18:25,536
is only happening during the

610
00:18:25,536 --> 00:18:27,016
capture side we can actually use

611
00:18:27,016 --> 00:18:28,436
hardware so it's really fast in

612
00:18:28,436 --> 00:18:30,176
performance, but the results

613
00:18:30,176 --> 00:18:31,666
again are not as good as we get

614
00:18:31,666 --> 00:18:31,976
out of Vision.

615
00:18:32,466 --> 00:18:34,006
So, you have to choose depending

616
00:18:34,006 --> 00:18:35,286
on your application what you

617
00:18:35,286 --> 00:18:36,996
want to do choose the right

618
00:18:36,996 --> 00:18:38,146
technology for the face

619
00:18:38,146 --> 00:18:38,566
detection.

620
00:18:40,406 --> 00:18:41,446
Now I did mention that our

621
00:18:41,446 --> 00:18:43,056
quality is better, so let me try

622
00:18:43,056 --> 00:18:44,096
to prove that a little bit.

623
00:18:44,576 --> 00:18:46,796
So, I have here an image and I

624
00:18:46,796 --> 00:18:47,896
ran the face detection through

625
00:18:47,896 --> 00:18:48,506
Core Image.

626
00:18:48,976 --> 00:18:50,956
And we find two faces and we see

627
00:18:50,956 --> 00:18:53,056
roughly where the eyes and where

628
00:18:53,656 --> 00:18:55,126
the mouth are.

629
00:18:55,396 --> 00:18:56,986
Now in Vision we find all four

630
00:18:56,986 --> 00:18:58,496
faces even the occluded ones and

631
00:18:58,496 --> 00:18:59,986
we get a whole lot more details

632
00:19:00,036 --> 00:19:03,196
with the visual landmarks.

633
00:19:04,356 --> 00:19:05,486
Speaking of Core Image, I would

634
00:19:05,486 --> 00:19:06,426
like to highlight a little bit

635
00:19:06,426 --> 00:19:07,306
what's happening with the

636
00:19:07,386 --> 00:19:08,046
CIDetectors.

637
00:19:08,076 --> 00:19:11,456
So, whoever uses it already can

638
00:19:11,456 --> 00:19:12,536
keep on using them they are

639
00:19:12,536 --> 00:19:15,596
still in Core Image, but all new

640
00:19:15,596 --> 00:19:17,136
parts and all the improvements

641
00:19:17,136 --> 00:19:18,296
in terms of algorithms for

642
00:19:18,296 --> 00:19:20,216
computer moving forward will be

643
00:19:20,216 --> 00:19:22,046
in Vision that's the new home

644
00:19:22,046 --> 00:19:22,946
for Computer Vision.

645
00:19:23,546 --> 00:19:28,126
So, an awful lot of sides, how

646
00:19:28,126 --> 00:19:28,656
about a demo.

647
00:19:29,606 --> 00:19:30,826
So, what I'm going to show you

648
00:19:30,826 --> 00:19:33,756
is an application that runs an

649
00:19:33,756 --> 00:19:35,726
AV capture session on the device

650
00:19:35,996 --> 00:19:37,256
if the demo Gods are with us.

651
00:19:37,896 --> 00:19:39,936
And we will do a very simple

652
00:19:39,936 --> 00:19:41,296
rectangle detection request.

653
00:19:42,376 --> 00:19:43,916
So, what do I have to do to set

654
00:19:43,916 --> 00:19:44,296
this up?

655
00:19:46,156 --> 00:19:48,386
What you see here is I create my

656
00:19:48,386 --> 00:19:52,136
request, that's my simple

657
00:19:52,136 --> 00:19:54,516
rectangle detection request in

658
00:19:55,886 --> 00:19:56,816
this case.

659
00:19:56,996 --> 00:19:58,376
I'm actually in the wrong sample

660
00:19:58,376 --> 00:19:59,416
that's why I'm getting confused

661
00:19:59,416 --> 00:19:59,826
here, my apologies.

662
00:20:00,356 --> 00:20:02,516
Here we go.

663
00:20:02,546 --> 00:20:03,826
Okay we have our rectangle

664
00:20:03,826 --> 00:20:05,956
detection request and I'm

665
00:20:05,956 --> 00:20:07,406
setting some parameters just as

666
00:20:07,406 --> 00:20:08,856
an example here, I only want

667
00:20:08,856 --> 00:20:10,626
them this minimum size in our

668
00:20:10,626 --> 00:20:11,946
coordinates are normalized so I

669
00:20:11,946 --> 00:20:13,796
only want a 10% minimize size of

670
00:20:13,796 --> 00:20:16,666
the image and I just want 20

671
00:20:16,666 --> 00:20:17,206
rectangles.

672
00:20:17,436 --> 00:20:19,036
I could get more, but I want 20,

673
00:20:19,036 --> 00:20:20,156
I just picked a number.

674
00:20:21,386 --> 00:20:23,086
I set up my area of the request

675
00:20:23,116 --> 00:20:25,986
that I want to perform and the

676
00:20:26,236 --> 00:20:28,506
right here this is our

677
00:20:28,676 --> 00:20:30,616
completion handler and all that

678
00:20:30,616 --> 00:20:32,326
I'm going to do is I'm going to

679
00:20:32,326 --> 00:20:34,406
draw my rectangles, but as you

680
00:20:34,406 --> 00:20:35,816
notice I'm just patching it to

681
00:20:35,816 --> 00:20:37,826
the main queue to update our UI.

682
00:20:39,176 --> 00:20:40,216
Where do our images come from?

683
00:20:40,216 --> 00:20:41,736
So, we look at the capture

684
00:20:41,736 --> 00:20:45,526
output here and as I promised,

685
00:20:45,666 --> 00:20:47,006
in the capture output we get our

686
00:20:47,006 --> 00:20:48,566
pixelBuffer from the

687
00:20:48,566 --> 00:20:49,606
CMSampleBuffer.

688
00:20:50,376 --> 00:20:54,276
Right here I'm getting the

689
00:20:54,276 --> 00:20:55,166
cameraIntrinsics.

690
00:20:55,166 --> 00:20:56,496
Now this is something that is

691
00:20:56,596 --> 00:20:57,606
important in some of these

692
00:20:57,656 --> 00:20:58,816
Computer Vision paths where we

693
00:20:58,816 --> 00:21:00,126
actually know what the camera is

694
00:21:00,126 --> 00:21:00,866
kind of looking at.

695
00:21:02,406 --> 00:21:04,206
As I mentioned, we don't forget

696
00:21:04,206 --> 00:21:06,446
the acts of orientation and I

697
00:21:06,446 --> 00:21:07,876
create an image request handler

698
00:21:08,026 --> 00:21:09,696
and perform our tasks.

699
00:21:09,696 --> 00:21:10,636
So, how does this look when we

700
00:21:10,636 --> 00:21:11,886
actually run it?

701
00:21:12,156 --> 00:21:13,006
All right, so what we're going

702
00:21:13,006 --> 00:21:14,296
to see here is that now we

703
00:21:14,586 --> 00:21:16,516
tracked this rectangle and

704
00:21:16,516 --> 00:21:17,906
that's as simple as it is, we

705
00:21:17,906 --> 00:21:19,386
can find other rectangles.

706
00:21:20,896 --> 00:21:22,036
If the cable is long enough we

707
00:21:22,036 --> 00:21:23,076
can actually look oh, there we

708
00:21:23,076 --> 00:21:24,406
find a computer with various

709
00:21:24,406 --> 00:21:24,906
rectangles.

710
00:21:25,516 --> 00:21:27,976
Now I chose the yellow kind of

711
00:21:27,976 --> 00:21:29,666
on purpose because it's the same

712
00:21:29,666 --> 00:21:31,456
color as you saw in the demo

713
00:21:31,726 --> 00:21:33,246
during the keynote for the new

714
00:21:33,246 --> 00:21:34,516
document camera on notes.

715
00:21:34,576 --> 00:21:36,536
And I borrowed their color

716
00:21:36,536 --> 00:21:37,706
because they borrowed our code

717
00:21:37,706 --> 00:21:38,686
to do actually the rectangle

718
00:21:38,686 --> 00:21:38,976
detection.

719
00:21:39,516 --> 00:21:44,636
[ Applause ]

720
00:21:45,136 --> 00:21:45,876
Thank you.

721
00:21:46,016 --> 00:21:48,000
[ Applause ]

722
00:21:51,046 --> 00:21:52,516
Now that was simple, let's do a

723
00:21:52,516 --> 00:21:52,916
bit more.

724
00:21:56,396 --> 00:21:58,746
So, how about we throw some

725
00:21:58,746 --> 00:22:00,096
machine learning at this as well

726
00:22:00,096 --> 00:22:01,286
just for the fun of it.

727
00:22:01,956 --> 00:22:03,776
So, what I have to do is I have

728
00:22:03,776 --> 00:22:06,136
a little model that I just

729
00:22:06,196 --> 00:22:07,566
dragged into my project here.

730
00:22:14,156 --> 00:22:16,456
And that is a classifier that

731
00:22:16,456 --> 00:22:17,576
will tell us a bit something

732
00:22:17,576 --> 00:22:18,276
about the image.

733
00:22:19,456 --> 00:22:21,726
And we see when we look at this

734
00:22:21,726 --> 00:22:25,536
part here that we need to feed

735
00:22:25,536 --> 00:22:27,116
it an image of a very strange

736
00:22:27,116 --> 00:22:30,556
size and get out of it some

737
00:22:30,556 --> 00:22:31,416
classification.

738
00:22:32,496 --> 00:22:33,596
Now you don't need to worry

739
00:22:33,596 --> 00:22:35,886
about that size because Vision

740
00:22:35,886 --> 00:22:37,006
will do the work for you.

741
00:22:37,006 --> 00:22:43,026
So, what do I need to do?

742
00:22:43,626 --> 00:22:46,076
I first need to create a Vision

743
00:22:46,076 --> 00:22:48,226
model and my request with that.

744
00:22:48,286 --> 00:22:50,326
And that is the part that we

745
00:22:50,326 --> 00:22:52,736
have here, so I'm simply loading

746
00:22:52,736 --> 00:22:56,166
the inception model and I create

747
00:22:56,166 --> 00:22:57,456
my classification request.

748
00:22:58,726 --> 00:22:59,476
Now it tells me there's

749
00:22:59,476 --> 00:23:00,696
something missing and I will get

750
00:23:00,696 --> 00:23:01,706
to that in just a moment.

751
00:23:01,996 --> 00:23:03,256
The last thing I want to

752
00:23:03,256 --> 00:23:05,006
highlight here is it says that

753
00:23:05,366 --> 00:23:06,956
it was okay square image, but

754
00:23:06,956 --> 00:23:09,446
our cameras don't see squares so

755
00:23:09,446 --> 00:23:10,786
I need to tell it actually how

756
00:23:10,786 --> 00:23:12,586
to handle just, you know, the

757
00:23:12,586 --> 00:23:14,136
aspect ratio that I want to use.

758
00:23:14,136 --> 00:23:15,636
And I say okay I want to just

759
00:23:15,636 --> 00:23:15,976
send a crop.

760
00:23:20,276 --> 00:23:22,636
So, I need a completion handler

761
00:23:22,636 --> 00:23:26,246
for my task and I have that

762
00:23:26,246 --> 00:23:28,336
already pre canned here as well.

763
00:23:30,496 --> 00:23:31,806
So, in this completion handler I

764
00:23:31,806 --> 00:23:33,296
simply look at my observation

765
00:23:33,296 --> 00:23:34,656
and I will get so this

766
00:23:34,696 --> 00:23:36,216
classifier can see a thousand

767
00:23:36,216 --> 00:23:37,876
different things and I don't

768
00:23:37,876 --> 00:23:39,206
want to show all of them I only

769
00:23:39,376 --> 00:23:40,816
show the ones that I care about.

770
00:23:40,816 --> 00:23:42,576
So, what I'm doing is a little

771
00:23:42,576 --> 00:23:43,516
bit of filtering, I only take

772
00:23:43,516 --> 00:23:46,216
the top four and I only look at

773
00:23:46,216 --> 00:23:47,706
the ones that have a confidence

774
00:23:47,706 --> 00:23:49,676
of at least 30%, it just works

775
00:23:49,676 --> 00:23:50,946
well for my demo here, but you

776
00:23:50,946 --> 00:23:52,126
know you will figure out what

777
00:23:52,126 --> 00:23:53,356
kind of works well for your

778
00:23:53,356 --> 00:23:53,696
model.

779
00:23:54,036 --> 00:23:57,286
And all I have to do next is add

780
00:23:58,016 --> 00:24:01,136
my classification request into

781
00:24:01,136 --> 00:24:02,516
my area of request and now I

782
00:24:02,516 --> 00:24:05,346
will actually run two requests.

783
00:24:06,026 --> 00:24:07,416
So, I have this already loaded

784
00:24:07,416 --> 00:24:09,886
on my device, let's see how this

785
00:24:09,886 --> 00:24:11,196
actually looks.

786
00:24:12,156 --> 00:24:13,526
Of course, you will see it when

787
00:24:13,526 --> 00:24:14,826
I switch to the correct machine,

788
00:24:15,386 --> 00:24:15,686
there we go.

789
00:24:22,046 --> 00:24:25,056
Okay, so we have a coffee mug

790
00:24:25,456 --> 00:24:27,016
which is empty, somebody better

791
00:24:27,016 --> 00:24:27,966
fill that for me.

792
00:24:27,966 --> 00:24:33,066
We have a ballpoint pen, we have

793
00:24:33,066 --> 00:24:37,616
a padlock and look an iPod.

794
00:24:38,396 --> 00:24:44,076
Who has stolen those empty cards

795
00:24:44,076 --> 00:24:45,436
away and didn't realize it was

796
00:24:45,436 --> 00:24:45,976
an iPod?

797
00:24:46,516 --> 00:24:49,500
[ Applause ]

798
00:24:53,056 --> 00:24:54,676
All right, let's go back to the

799
00:24:54,676 --> 00:24:58,096
slides before we get to the next

800
00:24:58,096 --> 00:24:59,406
show-and-tell.

801
00:25:00,456 --> 00:25:01,846
For my next demo, I want to do

802
00:25:01,846 --> 00:25:02,976
something a little bit more

803
00:25:02,976 --> 00:25:06,516
elaborate and with that I chose

804
00:25:06,516 --> 00:25:07,246
something that's called

805
00:25:07,246 --> 00:25:08,006
MNISTVision.

806
00:25:09,486 --> 00:25:10,876
People in the machine learning

807
00:25:10,876 --> 00:25:12,296
community have already looked at

808
00:25:12,296 --> 00:25:13,376
that a little bit more.

809
00:25:13,566 --> 00:25:15,766
MNIST is a dataset where a bunch

810
00:25:15,766 --> 00:25:17,076
of government employees and high

811
00:25:17,076 --> 00:25:18,536
school students wrote numbers

812
00:25:18,596 --> 00:25:21,086
down and this was marked up and

813
00:25:21,146 --> 00:25:22,106
people were trained in our

814
00:25:22,106 --> 00:25:23,166
classifier on that.

815
00:25:23,796 --> 00:25:25,156
Note this is basically like

816
00:25:25,156 --> 00:25:26,326
white numbers on black

817
00:25:26,396 --> 00:25:28,336
background, so I guess they've

818
00:25:28,336 --> 00:25:30,626
written it with chalk on an old

819
00:25:30,666 --> 00:25:31,136
blackboard.

820
00:25:32,516 --> 00:25:34,346
So, in this sample code I'm

821
00:25:34,346 --> 00:25:35,846
going to show you I want to show

822
00:25:35,846 --> 00:25:37,356
a few concepts that are kind of

823
00:25:37,356 --> 00:25:38,646
important like making something

824
00:25:38,646 --> 00:25:40,606
a bit more elaborate with

825
00:25:41,276 --> 00:25:41,456
Vision.

826
00:25:41,606 --> 00:25:43,646
First, we'll spin off model

827
00:25:43,646 --> 00:25:45,006
requests based on top of each

828
00:25:45,006 --> 00:25:47,516
other then we use Core Image in

829
00:25:47,516 --> 00:25:49,566
between to do some image process

830
00:25:49,566 --> 00:25:51,666
and last but not least, we use

831
00:25:51,666 --> 00:25:53,246
Core ML again for the machine

832
00:25:53,246 --> 00:25:53,716
learning part.

833
00:25:56,666 --> 00:25:57,686
So, how is this going to work?

834
00:25:58,926 --> 00:26:00,596
We have here an image on which

835
00:26:00,596 --> 00:26:01,506
we find a sticky note.

836
00:26:02,106 --> 00:26:04,846
Well we find it by using the

837
00:26:04,846 --> 00:26:06,796
rectangle detector, there's our

838
00:26:06,796 --> 00:26:07,326
sticky note.

839
00:26:07,896 --> 00:26:09,226
Now that is prospectively

840
00:26:09,226 --> 00:26:10,866
distorted and it's clearly not

841
00:26:10,926 --> 00:26:12,306
white text on black background.

842
00:26:13,456 --> 00:26:14,846
So, we use Core Image in the

843
00:26:14,846 --> 00:26:16,686
next step and we'll actually do

844
00:26:16,686 --> 00:26:18,046
the perspective correction of it

845
00:26:18,686 --> 00:26:20,666
and invert the color and enhance

846
00:26:20,666 --> 00:26:22,086
also the contrast so that we get

847
00:26:22,086 --> 00:26:23,356
rid this black-and-white image.

848
00:26:24,896 --> 00:26:26,076
And last but not least, I need

849
00:26:26,076 --> 00:26:28,826
to run my MNIST classifier on it

850
00:26:29,016 --> 00:26:30,476
and it should tell me that this

851
00:26:30,476 --> 00:26:32,256
is the number four and this has

852
00:26:32,256 --> 00:26:35,096
80% confidence that this is the

853
00:26:35,096 --> 00:26:35,636
number four.

854
00:26:35,636 --> 00:26:38,666
Again, let's see how this looks

855
00:26:38,666 --> 00:26:39,066
in the app.

856
00:26:41,496 --> 00:26:43,426
So again, I start off as a

857
00:26:43,426 --> 00:26:45,236
rectangle detector request, it's

858
00:26:45,236 --> 00:26:46,316
my favorite I know.

859
00:26:47,886 --> 00:26:48,946
But it's more interesting what

860
00:26:48,946 --> 00:26:50,246
I'm going to do in the

861
00:26:50,246 --> 00:26:51,206
completion handler.

862
00:26:52,426 --> 00:26:53,746
So, I do some validation just to

863
00:26:53,746 --> 00:26:54,876
make sure that the rectangles

864
00:26:54,876 --> 00:26:55,746
I'm getting out of it are

865
00:26:55,746 --> 00:26:56,456
actually okay.

866
00:26:56,456 --> 00:26:58,656
But the interesting part happens

867
00:26:58,736 --> 00:26:58,926
here.

868
00:26:59,596 --> 00:27:01,256
I get the coordinates of the

869
00:27:01,256 --> 00:27:04,126
corners and feed them into CI to

870
00:27:04,126 --> 00:27:05,786
use the CIPerspectiveCorrection.

871
00:27:06,476 --> 00:27:07,636
That allows me to take this

872
00:27:07,636 --> 00:27:09,226
prospectively distorted image

873
00:27:09,226 --> 00:27:10,916
and actually bring it upright as

874
00:27:10,916 --> 00:27:11,966
if I would have the camera

875
00:27:11,966 --> 00:27:12,506
straight on.

876
00:27:14,076 --> 00:27:15,936
I use the CIColorControls to

877
00:27:16,186 --> 00:27:17,456
really bring out the contrast of

878
00:27:17,506 --> 00:27:19,066
the image to make it kind of

879
00:27:19,066 --> 00:27:19,546
binarized.

880
00:27:20,896 --> 00:27:22,246
And as I said, I have to color

881
00:27:22,246 --> 00:27:23,306
invert it.

882
00:27:24,976 --> 00:27:26,776
Now the resulting image of that

883
00:27:26,776 --> 00:27:28,116
I feed into a new request in

884
00:27:28,116 --> 00:27:28,936
there because we have a new

885
00:27:28,936 --> 00:27:31,146
image on which I'll run the

886
00:27:31,146 --> 00:27:32,086
classification.

887
00:27:32,296 --> 00:27:34,066
So, how does the classification

888
00:27:34,066 --> 00:27:34,416
look like?

889
00:27:35,906 --> 00:27:37,446
The classification for that I

890
00:27:37,446 --> 00:27:39,446
use my endless model which I

891
00:27:39,446 --> 00:27:41,776
actually have ready and this is

892
00:27:41,776 --> 00:27:43,046
a small model that I've really

893
00:27:43,046 --> 00:27:44,866
trained actually on this laptop

894
00:27:44,866 --> 00:27:45,996
very easily give us a few lines

895
00:27:45,996 --> 00:27:47,576
of code script and then thanks

896
00:27:47,576 --> 00:27:49,386
to Core ML I can just drag that

897
00:27:49,386 --> 00:27:50,706
in and use this very easily.

898
00:27:51,106 --> 00:27:52,316
So, I have my model here.

899
00:27:53,766 --> 00:27:55,046
Now again, this one part I would

900
00:27:55,046 --> 00:27:57,636
like to highlight this, so that

901
00:27:57,636 --> 00:27:59,436
takes in this case a very small

902
00:27:59,436 --> 00:28:00,166
grayscale image.

903
00:28:00,166 --> 00:28:02,636
So, an image 28 by 28 pixels it

904
00:28:02,636 --> 00:28:03,636
should be able to read these

905
00:28:03,636 --> 00:28:03,976
numbers.

906
00:28:12,056 --> 00:28:12,526
So that's where my

907
00:28:12,526 --> 00:28:14,716
classification is coming from

908
00:28:15,326 --> 00:28:16,906
and now I need to feed in the

909
00:28:16,906 --> 00:28:17,216
image.

910
00:28:17,216 --> 00:28:19,056
So, this sample code has been

911
00:28:19,056 --> 00:28:20,046
made available also for the

912
00:28:20,046 --> 00:28:21,506
session to make it easy also to

913
00:28:21,506 --> 00:28:22,716
run a simulator not running it

914
00:28:22,716 --> 00:28:24,266
live off of the camera I'm just

915
00:28:24,266 --> 00:28:25,196
going to use actually the

916
00:28:25,196 --> 00:28:29,046
UIImagePicker and feed it into

917
00:28:29,086 --> 00:28:32,306
my VMImageRequestHandler and let

918
00:28:32,306 --> 00:28:33,526
it just perform the rectangle

919
00:28:33,526 --> 00:28:33,986
request.

920
00:28:33,986 --> 00:28:37,136
Now notice I buried the request

921
00:28:37,136 --> 00:28:38,906
for the classification into the

922
00:28:38,906 --> 00:28:40,396
completion handler of my

923
00:28:40,396 --> 00:28:42,146
rectangle detection and that

924
00:28:42,146 --> 00:28:43,496
allows us to basically cascade

925
00:28:43,496 --> 00:28:44,966
multiple requests on top of each

926
00:28:44,966 --> 00:28:45,146
other.

927
00:28:46,096 --> 00:28:47,546
So, let's try the demo for this.

928
00:28:50,896 --> 00:28:55,016
Okay, so I have my app here and

929
00:28:55,196 --> 00:28:56,656
well [inaudible] giveaway.

930
00:28:59,076 --> 00:29:00,696
Okay, so what I see here is

931
00:29:00,696 --> 00:29:01,976
again I have my image on the

932
00:29:01,976 --> 00:29:03,436
top, this was actually the photo

933
00:29:03,436 --> 00:29:04,346
that I took earlier.

934
00:29:04,926 --> 00:29:07,476
We see its correctly classifying

935
00:29:07,476 --> 00:29:10,366
as a number one, it was a really

936
00:29:10,366 --> 00:29:11,556
high confidence in this case.

937
00:29:11,556 --> 00:29:12,646
And what you see on the bottom

938
00:29:12,646 --> 00:29:14,816
is just basically just to

939
00:29:14,816 --> 00:29:16,126
visualize that I took this

940
00:29:16,126 --> 00:29:17,296
intermediate image that we

941
00:29:17,296 --> 00:29:19,676
created in CI and show this as

942
00:29:19,676 --> 00:29:19,946
well.

943
00:29:19,946 --> 00:29:21,706
Let's choose another number.

944
00:29:23,066 --> 00:29:24,316
Yes, this is the number three.

945
00:29:26,466 --> 00:29:27,756
Can we guess what this number

946
00:29:27,846 --> 00:29:28,826
is, it's the number four?

947
00:29:29,626 --> 00:29:30,706
It works correctly.

948
00:29:32,576 --> 00:29:33,806
All right, thank you.

949
00:29:34,516 --> 00:29:37,556
[ Applause ]

950
00:29:38,056 --> 00:29:39,176
Let me go back to our slides.

951
00:29:41,486 --> 00:29:42,936
So that is our Vision framework.

952
00:29:43,986 --> 00:29:45,316
Let's capitalize a little bit on

953
00:29:45,316 --> 00:29:46,466
what we really have seen here.

954
00:29:47,486 --> 00:29:49,426
So, Vision is a high-level

955
00:29:49,426 --> 00:29:50,696
framework for Computer Vision

956
00:29:50,756 --> 00:29:51,816
and it should really make it

957
00:29:51,816 --> 00:29:53,756
easy for you to use this in your

958
00:29:53,756 --> 00:29:55,356
applications even if you're not

959
00:29:55,356 --> 00:29:56,576
a Computer Vision expert.

960
00:29:57,276 --> 00:29:58,756
We have various detectors and

961
00:29:58,756 --> 00:30:00,166
there's a whole variety of that

962
00:30:00,166 --> 00:30:01,946
and they all run through one

963
00:30:01,946 --> 00:30:03,296
consistent interface which would

964
00:30:03,336 --> 00:30:04,896
make it very easy to learn that

965
00:30:04,896 --> 00:30:05,446
set of APIs.

966
00:30:06,826 --> 00:30:08,596
And last but not least, the

967
00:30:08,596 --> 00:30:09,886
integration with Core ML.

968
00:30:10,376 --> 00:30:11,736
By bringing your own custom

969
00:30:11,736 --> 00:30:13,696
models you can do a lot in your

970
00:30:13,696 --> 00:30:15,586
application, you can find

971
00:30:15,586 --> 00:30:17,426
hotdogs and see if they are

972
00:30:17,426 --> 00:30:18,076
really hotdogs.

973
00:30:19,416 --> 00:30:20,336
I had to make that joke.

974
00:30:23,576 --> 00:30:24,606
So, if you want to learn more

975
00:30:24,606 --> 00:30:26,816
about our session, please go to

976
00:30:26,816 --> 00:30:28,626
our website and I would

977
00:30:28,626 --> 00:30:29,746
definitely highlight there are

978
00:30:29,796 --> 00:30:31,576
some related sessions that you

979
00:30:31,576 --> 00:30:33,266
should have watched perhaps in

980
00:30:33,266 --> 00:30:33,706
the past.

981
00:30:33,706 --> 00:30:34,786
I'll read you the Core ML one,

982
00:30:34,786 --> 00:30:35,876
but you can find it on our

983
00:30:35,876 --> 00:30:36,106
website.

984
00:30:36,106 --> 00:30:39,616
Please come for our get-together

985
00:30:39,616 --> 00:30:42,146
that we have at 6:30 today, chat

986
00:30:42,146 --> 00:30:43,416
about what we can do.

987
00:30:43,896 --> 00:30:45,536
And for the little bit more

988
00:30:45,536 --> 00:30:46,776
advanced part of Core ML there's

989
00:30:46,826 --> 00:30:48,886
a session on Thursday, as well

990
00:30:48,886 --> 00:30:50,256
as we have a session with Core

991
00:30:50,256 --> 00:30:51,486
Image where they will also do

992
00:30:51,486 --> 00:30:53,556
some very fancy stuff with Core

993
00:30:53,556 --> 00:30:54,246
Image and Vision.

994
00:30:55,826 --> 00:30:57,126
And with that I'd like to thank

995
00:30:57,126 --> 00:30:58,646
you for coming today and enjoy

996
00:30:58,646 --> 00:30:59,276
the rest of WWDC.

997
00:30:59,276 --> 00:30:59,456
Thank you.

998
00:31:00,516 --> 00:31:06,770
[ Applause ]

