1
00:00:18,752 --> 00:00:24,124
（iOS ARKit增强现实简介）

2
00:00:24,258 --> 00:00:26,894
欢迎参加我们的ARKit简介演讲

3
00:00:27,060 --> 00:00:29,630
我是Mike
我是ARKit团队中的一名工程师

4
00:00:29,696 --> 00:00:33,767
今天我很高兴要跟大家讲讲
概念以及代码

5
00:00:33,834 --> 00:00:38,105
你可以用来创建
你自己的iOS增强现实体验

6
00:00:40,407 --> 00:00:41,241
谢谢

7
00:00:41,308 --> 00:00:44,511
我知道你们中有很多人
很想了解增强现实

8
00:00:44,578 --> 00:00:47,147
让我来告诉你使用ARKit
是一件多么简单的事

9
00:00:48,415 --> 00:00:51,585
但首先什么是增强现实？

10
00:00:52,019 --> 00:00:53,987
增强现实是创建错觉

11
00:00:54,054 --> 00:00:57,024
虚拟对象被放在物理世界中

12
00:00:57,257 --> 00:00:59,193
它使用你iPhone或iPad

13
00:00:59,259 --> 00:01:02,963
作为进入虚拟世界的镜头
根据你的摄像头所能看到的内容

14
00:01:03,697 --> 00:01:05,265
让我们来看一些示例

15
00:01:05,832 --> 00:01:08,869
我们提前给了一群开发人员
使用ARKit的权限

16
00:01:09,203 --> 00:01:10,370
这是他们所创建出来的

17
00:01:10,437 --> 00:01:13,941
你可能在不久的将来看到这些东西
让我们来先睹为快吧

18
00:01:17,544 --> 00:01:20,848
这里面是一个公司
正在沉浸在讲故事的氛围中

19
00:01:20,914 --> 00:01:23,684
他们正在使用AR讲金发姑娘的故事

20
00:01:26,186 --> 00:01:29,022
把一间卧室
转换成了一本虚拟的故事书

21
00:01:29,089 --> 00:01:32,125
他们允许你通过朗诵文本
推动故事发展

22
00:01:32,192 --> 00:01:36,296
但更重要的是 他们允许你
从任意角度来探索场景

23
00:01:39,633 --> 00:01:43,937
这种等级的交互性真的会帮助你
把虚拟场景变成现实

24
00:01:48,208 --> 00:01:52,546
接下来宜家家居通过ARKit
重新设计你的客厅

25
00:01:58,519 --> 00:02:02,122
通过把虚拟内容放在物理对象中实现

26
00:02:02,189 --> 00:02:04,892
你打开了一个
通往你用户的无限可能性

27
00:02:08,127 --> 00:02:10,130
最后是游戏

28
00:02:10,264 --> 00:02:14,034
Pokemon Go
一款你可能已经听过的应用

29
00:02:14,434 --> 00:02:17,905
通过ARKit将捕捉Pokemon
带入了一个新层级

30
00:02:18,839 --> 00:02:22,609
通过把你的虚拟物品
锚定在现实世界中

31
00:02:22,676 --> 00:02:25,145
你真的可以获得沉浸感更强的体验

32
00:02:25,212 --> 00:02:26,480
比之前的都要强

33
00:02:27,714 --> 00:02:29,950
但它并没有止步于此 有许多方式

34
00:02:30,017 --> 00:02:33,687
使用增强现实来提升你的用户体验

35
00:02:34,221 --> 00:02:35,689
让我们看看要如何实现吧

36
00:02:39,026 --> 00:02:43,197
关于创建增强现实
需要储备大量的专业知识

37
00:02:43,463 --> 00:02:47,267
从计算机视觉到传感器融合
再到与硬件通讯

38
00:02:47,334 --> 00:02:50,070
都是为了获取相机标定和相机内参

39
00:02:50,437 --> 00:02:52,339
我们想把这些变得简单一些

40
00:02:52,840 --> 00:02:55,709
那么今天我们要介绍ARKit

41
00:03:02,583 --> 00:03:04,651
ARKit是一款移动设备AR平台

42
00:03:04,718 --> 00:03:07,588
用于在iOS上开发增强现实应用

43
00:03:08,055 --> 00:03:10,824
它是一个高等级的API
有一个简单的界面

44
00:03:10,891 --> 00:03:12,526
提供一系列强大的功能

45
00:03:12,893 --> 00:03:14,695
但更重要的是它可以

46
00:03:14,761 --> 00:03:18,165
支持现有的数以亿计的iOS设备

47
00:03:18,498 --> 00:03:20,701
要获得ARKit的全部功能

48
00:03:20,767 --> 00:03:22,803
你需要获得一个A9及以上的芯片

49
00:03:22,970 --> 00:03:27,007
这包括了绝大部分iOS 11设备
其中包括iPhone 6S

50
00:03:28,108 --> 00:03:31,645
现在 让我们谈谈功能
那么 ARKit提供什么呢？

51
00:03:32,145 --> 00:03:35,048
ARKit可以分解为三个不同的层

52
00:03:35,115 --> 00:03:37,284
第一个层是追踪

53
00:03:38,752 --> 00:03:41,221
追踪是ARKit的核心功能

54
00:03:41,488 --> 00:03:44,424
它可以实时追踪你的设备

55
00:03:44,758 --> 00:03:47,160
通过追踪 你可以

56
00:03:47,227 --> 00:03:50,731
获得设备在物理环境中的相对位置

57
00:03:51,331 --> 00:03:55,068
我们通过相机图像
使用虚拟惯性里程计

58
00:03:55,135 --> 00:03:58,906
并通过来自你设备的动作数据
获取精确视图

59
00:03:58,972 --> 00:04:02,176
关于你的设备在哪个位置
以及它的方向

60
00:04:02,910 --> 00:04:06,180
但同时更重要的是不需要外部设置

61
00:04:06,246 --> 00:04:08,415
不需要关于你所处环境的
已存在的知识

62
00:04:08,482 --> 00:04:12,352
也不需要你设备上
目前没有的额外传感器

63
00:04:13,353 --> 00:04:16,857
接下来根据追踪进行创建
我们提供场景理解

64
00:04:19,392 --> 00:04:23,797
场景理解是决定

65
00:04:23,864 --> 00:04:25,899
你设备周围的环境特性或属性的能力

66
00:04:26,166 --> 00:04:28,235
它提供类似平面探测这样的功能

67
00:04:28,468 --> 00:04:30,938
平面探测是决定物理环境中的表面

68
00:04:31,004 --> 00:04:32,873
或平面的能力

69
00:04:33,207 --> 00:04:35,976
比如说一楼或许是一张桌子

70
00:04:37,044 --> 00:04:40,647
要摆放你的虚拟对象
我们提供碰撞测试功能

71
00:04:40,714 --> 00:04:44,084
S是通过现实世界拓扑
获取一个交叉点

72
00:04:44,151 --> 00:04:47,087
以便你可以在现实世界中
放置你的虚拟对象

73
00:04:47,654 --> 00:04:51,358
最后场景理解还提供光估计

74
00:04:51,625 --> 00:04:57,297
那么光估计用于渲染
你的虚拟几何体或实现正确打光

75
00:04:57,364 --> 00:04:59,233
以匹配物理世界的实际环境

76
00:04:59,900 --> 00:05:04,037
通过这些功能
我们可以无缝地整合虚拟内容

77
00:05:04,104 --> 00:05:06,039
到你的物理环境中

78
00:05:06,573 --> 00:05:09,409
那么ARKit的最后一个层是渲染

79
00:05:11,578 --> 00:05:14,548
对于渲染
你可以轻松地整合到任何渲染器中

80
00:05:14,615 --> 00:05:18,151
我们提供源源不断的相机图像、
追踪信息

81
00:05:18,218 --> 00:05:21,889
以及场景理解
你可以放到任意渲染器中

82
00:05:23,156 --> 00:05:27,528
如果你们用的是SceneKit或SpriteKit
我们提供自定义AR视图

83
00:05:27,594 --> 00:05:29,696
可以为你们实施绝大部分渲染

84
00:05:29,763 --> 00:05:31,365
所以起步是很简单的

85
00:05:32,432 --> 00:05:33,934
如果你们用的是自定义渲染器

86
00:05:34,001 --> 00:05:36,069
我们提供Xcode金属模板

87
00:05:36,637 --> 00:05:40,407
可以帮助你把ARKit
整合到你的自定义渲染器中

88
00:05:41,575 --> 00:05:42,676
还有一点

89
00:05:43,310 --> 00:05:47,347
统一性和虚幻
支持了ARKit中的全部功能

90
00:05:53,921 --> 00:05:54,988
那么你们大家准备好了吗？

91
00:05:56,056 --> 00:06:00,194
让我们开始吧
我要如何在我的应用中使用ARKit？

92
00:06:01,328 --> 00:06:04,731
ARKit是一个框架 可以处理全部进程

93
00:06:04,798 --> 00:06:07,067
用于创建一种增强现实体验

94
00:06:07,634 --> 00:06:12,306
通过我所选择的渲染器
我可以很方便地用ARKit来实现进程

95
00:06:12,539 --> 00:06:16,076
它会提供我要渲染
增强现实场景的一切功能

96
00:06:17,044 --> 00:06:20,581
除了进程 ARKit还可以捕获

97
00:06:20,647 --> 00:06:23,283
以便实现增强现实

98
00:06:23,584 --> 00:06:28,388
那么在底层通过
AVFoundation和Core Motion

99
00:06:28,455 --> 00:06:32,226
从你的设备中捕捉图像并获取运动数据
以便进行追踪

100
00:06:32,292 --> 00:06:34,595
并为你的渲染器提供相机图像

101
00:06:35,262 --> 00:06:36,930
那么现在 要如何使用ARKit呢？

102
00:06:37,631 --> 00:06:39,833
ARKit是一个基于会话的API

103
00:06:39,900 --> 00:06:41,535
你需要了解的第一件事

104
00:06:41,602 --> 00:06:43,337
是简单地创建一个ARSession

105
00:06:44,104 --> 00:06:47,574
ARSession是一个对象 控制着

106
00:06:47,641 --> 00:06:50,010
用于创建增强现实应用的全部进程

107
00:06:50,611 --> 00:06:54,014
但首先我需要决定实现哪种追踪

108
00:06:54,081 --> 00:06:55,516
在我的增强现实应用中

109
00:06:55,582 --> 00:06:59,386
那么要确定这个问题
我们要创建一个AR会话配置

110
00:07:01,188 --> 00:07:03,257
AR会话配置及其子类

111
00:07:03,323 --> 00:07:06,026
决定你想在你的会话上实现哪种追踪

112
00:07:06,827 --> 00:07:08,795
通过启用和禁用属性

113
00:07:08,862 --> 00:07:10,797
你可以获得不同的场景理解

114
00:07:10,864 --> 00:07:13,100
并用你的ARSession来实现不同的进程

115
00:07:14,034 --> 00:07:18,405
要运行我的会话
我只需要在ARSession上调用Run方法

116
00:07:18,472 --> 00:07:20,607
就能提供我想要运行的配置

117
00:07:22,409 --> 00:07:24,745
然后就立即开启进程

118
00:07:25,212 --> 00:07:27,181
我们还要在底层设置捕捉

119
00:07:27,247 --> 00:07:30,350
那么在底层
你将看到有一个AV捕捉会话

120
00:07:30,417 --> 00:07:33,253
和一个CM运动管理器
已经为你创建好了

121
00:07:33,987 --> 00:07:35,556
我们通过这两个来获取图像数据

122
00:07:35,622 --> 00:07:38,158
以及运动数据用于追踪

123
00:07:38,225 --> 00:07:41,795
一旦进程完成
ARSession将输出ARFrame

124
00:07:42,496 --> 00:07:45,032
那么ARFrame是一种适时快照

125
00:07:45,098 --> 00:07:46,834
包括关于你会话的全部状态、

126
00:07:46,900 --> 00:07:50,103
要渲染你的增强现实场景所需要的一切

127
00:07:51,004 --> 00:07:54,708
要获取ARFrame
你只需要从ARSession中

128
00:07:54,775 --> 00:07:57,110
调用或拉动当前的帧属性

129
00:07:57,511 --> 00:07:59,079
或者你可以将你自己设为委托

130
00:07:59,146 --> 00:08:02,149
以接收更新
当有新的ARFrame可用时

131
00:08:03,350 --> 00:08:06,186
那么让我们具体看一下
ARSessionConfiguration

132
00:08:09,790 --> 00:08:11,158
ARSession配置决定了

133
00:08:11,225 --> 00:08:13,427
你想在你的会话上运行哪种追踪

134
00:08:13,961 --> 00:08:17,064
所以它提供不同的配置类

135
00:08:17,564 --> 00:08:19,900
基类是ARSessionConfiguration

136
00:08:19,967 --> 00:08:22,035
提供自由追踪的三个角度

137
00:08:22,102 --> 00:08:24,238
即你设备的定向

138
00:08:25,272 --> 00:08:26,473
它的子类

139
00:08:26,540 --> 00:08:28,475
ARWorldTrackingSession
Configuration

140
00:08:28,542 --> 00:08:30,744
提供自由追踪的六个角度

141
00:08:30,811 --> 00:08:33,679
那么这使用了我们的核心功能
世界追踪

142
00:08:33,746 --> 00:08:36,082
为了获得不仅仅是你设备的方向

143
00:08:36,149 --> 00:08:38,885
也为了获得你设备的相对位置

144
00:08:39,385 --> 00:08:41,855
然后我们还要获取关于场景的信息

145
00:08:41,922 --> 00:08:45,726
那么我们提供场景理解 如特征点以及

146
00:08:45,792 --> 00:08:48,495
你现实世界中的物理位置

147
00:08:49,363 --> 00:08:51,231
要启用和禁用功能

148
00:08:51,298 --> 00:08:54,434
你只需要在你的会话配置类中
设置属性即可

149
00:08:58,438 --> 00:09:01,508
会话配置还提供可用性

150
00:09:02,242 --> 00:09:05,312
所以如果你想查看你的设备
是否支持世界追踪

151
00:09:05,379 --> 00:09:07,548
你只需要调用类属性

152
00:09:07,614 --> 00:09:10,551
isSupported 在ARWorldTracking
SessionConfiguration上

153
00:09:11,185 --> 00:09:14,121
然后你就可以
使用你的世界追踪会话配置

154
00:09:14,188 --> 00:09:15,923
或退回到基类

155
00:09:15,989 --> 00:09:18,592
退回基类将只能提供
自由追踪的三个角度

156
00:09:18,659 --> 00:09:21,094
这里有一个重要的点 因为基类

157
00:09:21,161 --> 00:09:22,896
没有任何场景理解功能

158
00:09:22,963 --> 00:09:26,133
如碰撞测试
那么在这台设备上也不可用

159
00:09:26,233 --> 00:09:29,469
那么我们还提供UI所需的设备功能

160
00:09:29,536 --> 00:09:31,839
你可以在你的应用中进行设置
以便你的应用仅能

161
00:09:31,905 --> 00:09:34,641
在App Store中
支持世界追踪的设备上出现

162
00:09:35,976 --> 00:09:37,611
接下来让我们看看ARSession

163
00:09:39,813 --> 00:09:43,317
ARSession是管理

164
00:09:43,383 --> 00:09:45,152
用于增强现实应用中全部进程的类

165
00:09:46,820 --> 00:09:49,156
除了通过配置调用Run

166
00:09:49,223 --> 00:09:50,591
你还可以调用Pause

167
00:09:51,425 --> 00:09:55,295
Pause允许你暂时停止
发生在你会话上的全部进程

168
00:09:55,362 --> 00:09:59,199
所以如果你的视图不再可见
你可能想停止进程

169
00:09:59,266 --> 00:10:03,203
以停止使用CPU
在暂停时将不会发生任何追踪

170
00:10:03,670 --> 00:10:06,473
要在暂停后恢复追踪

171
00:10:06,540 --> 00:10:08,075
你只需要再次调用Run即可

172
00:10:08,141 --> 00:10:10,344
通过你会话上所存储的配置

173
00:10:11,378 --> 00:10:13,714
最后你可以多次调用Run

174
00:10:13,780 --> 00:10:16,350
以在不同的配置中转换

175
00:10:16,617 --> 00:10:20,521
那么比如说我想启用平面探测
我可以把我的配置修改为

176
00:10:20,587 --> 00:10:23,657
启用平面探测
并在我的会话中再次调用Run

177
00:10:23,724 --> 00:10:26,727
我的会话将自动进行无缝转换

178
00:10:26,793 --> 00:10:30,664
在不同的配置之间
而不会失掉任何相机图像

179
00:10:33,166 --> 00:10:36,436
那么通过Run命令
我们还提供追踪的重设

180
00:10:36,503 --> 00:10:39,973
那么这是你在Run命令中
所能提供的Run选项

181
00:10:40,040 --> 00:10:41,441
以便重设追踪

182
00:10:41,508 --> 00:10:43,877
它会重新初始化
正在进行中的全部追踪

183
00:10:43,944 --> 00:10:46,980
你的相机位置将再次从000出发

184
00:10:47,047 --> 00:10:49,616
所以这对于你的应用很有用
如果你想把它重设

185
00:10:49,683 --> 00:10:50,884
到某个起点的话

186
00:10:53,287 --> 00:10:55,722
我如何利用ARSession进程？

187
00:10:56,290 --> 00:10:59,526
将你自己设为委托
可获得可用的会话更新

188
00:11:00,794 --> 00:11:03,463
那么为了获得所处理的最后一帧

189
00:11:03,530 --> 00:11:05,699
我需要实施didUpdate Frame会话

190
00:11:06,600 --> 00:11:08,569
这将给我提供最新的一帧

191
00:11:08,635 --> 00:11:12,573
对于报错处理 你还可以实施
诸如DidFailWithError这样的会话

192
00:11:12,639 --> 00:11:14,408
那么这是严重错误时的情况

193
00:11:14,474 --> 00:11:17,544
也许你正在一台
不支持世界追踪的设备上运行

194
00:11:17,611 --> 00:11:20,414
你将得到一个这样的报错
并且你的会话将被暂停

195
00:11:21,281 --> 00:11:24,384
ARSession进程的另一个作用是

196
00:11:24,451 --> 00:11:26,186
拉动当前帧属性

197
00:11:27,921 --> 00:11:30,190
那么现在ARFrame包含什么？

198
00:11:30,824 --> 00:11:35,395
每个ARFrame包含你用于
渲染增强现实场景的一切东西

199
00:11:36,296 --> 00:11:38,532
它提供的第一个东西是相机图像

200
00:11:38,866 --> 00:11:42,469
那么这是你用来渲染场景背景的东西

201
00:11:43,604 --> 00:11:45,973
接下来它提供追踪信息

202
00:11:46,039 --> 00:11:50,544
或我设备的方向以及位置
甚至是追踪状态

203
00:11:51,245 --> 00:11:53,614
最后它提供场景理解

204
00:11:54,214 --> 00:11:57,317
那么关于场景的信息 比如特征点、

205
00:11:57,384 --> 00:12:01,655
在空间中的物理位置
以及光估计或光预估

206
00:12:02,823 --> 00:12:08,262
那么在空间中的物理位置
ARKit的呈现方式是

207
00:12:08,795 --> 00:12:11,732
通过ARFrame或ARAnchor 抱歉

208
00:12:12,165 --> 00:12:15,536
ARAnchor是一个相对位置
或现实世界的位置

209
00:12:15,602 --> 00:12:17,938
和在空间中的方向

210
00:12:20,207 --> 00:12:23,110
你可以从你的场景中
添加或移除ARAnchor

211
00:12:23,177 --> 00:12:25,279
它们用于基本呈现

212
00:12:25,345 --> 00:12:29,516
锚定到你物理环境中的虚拟内容

213
00:12:29,816 --> 00:12:31,318
如果你想添加一个自定义锚点

214
00:12:31,385 --> 00:12:33,020
你可以通过
将其添加到你的会话中实现

215
00:12:33,086 --> 00:12:35,489
它将一直留在你的会话中
直到会话结束

216
00:12:36,023 --> 00:12:39,293
但还有额外的一点 如果你正在运行
诸如平面探测之类的功能

217
00:12:39,359 --> 00:12:42,029
ARAnchor将被自动添加到你的会话中

218
00:12:42,396 --> 00:12:43,997
那么为了响应这个

219
00:12:44,331 --> 00:12:47,968
你可以在当前的ARFrame中
为他们提供一个完整列表

220
00:12:48,168 --> 00:12:51,271
将包含你的会话
当前正在追踪的全部锚点

221
00:12:51,338 --> 00:12:56,310
或者你可以响应委托方法
比如添加、更新和移除

222
00:12:56,376 --> 00:12:58,412
如果锚点被成功添加

223
00:12:58,478 --> 00:13:00,747
更新或从会话中移除 将会通知你

224
00:13:02,616 --> 00:13:04,585
那就推断出四个主要类

225
00:13:04,651 --> 00:13:07,387
是你用来创建增强现实体验的

226
00:13:08,956 --> 00:13:11,225
现在让我们特别谈谈追踪

227
00:13:13,627 --> 00:13:18,365
那么追踪是决定
在空间中的实时物理位置的功能

228
00:13:19,800 --> 00:13:20,667
这并不简单

229
00:13:20,868 --> 00:13:23,437
但是它至关重要 它可以让增强现实

230
00:13:23,770 --> 00:13:25,806
发现你设备的位置

231
00:13:25,873 --> 00:13:29,209
那么并不是任意位置
而是你设备的位置和方向

232
00:13:29,276 --> 00:13:30,978
以便准确地渲染内容

233
00:13:31,044 --> 00:13:32,479
那么让我们来看个例子

234
00:13:33,247 --> 00:13:35,015
在这里我放了一把虚拟的椅子

235
00:13:35,449 --> 00:13:37,818
和一张桌子在物理环境中

236
00:13:38,185 --> 00:13:43,423
你会注意到如果我左右平移它
或重定向到我的设备

237
00:13:43,490 --> 00:13:45,192
它们会在空间中保持固定

238
00:13:45,325 --> 00:13:49,463
但更重要的是 当我在场景中溜达时
它们仍然会保持固定

239
00:13:50,063 --> 00:13:51,665
那么这是因为我们用了

240
00:13:51,732 --> 00:13:54,768
我们不断地更新投影转换

241
00:13:54,835 --> 00:13:58,438
或不断地更新我们用于
渲染这个虚拟内容的投影矩阵

242
00:13:58,505 --> 00:14:01,241
以便从任意视角看起来它的位置
都是正确的

243
00:14:03,210 --> 00:14:04,511
现在我们要如何实现呢？

244
00:14:05,712 --> 00:14:07,714
ARKit提供世界追踪

245
00:14:08,015 --> 00:14:10,817
我们的这个技术使用了视觉惯性里程计

246
00:14:10,884 --> 00:14:13,420
它是你的相机图像
它是你设备的移动

247
00:14:13,921 --> 00:14:15,722
并且它给你提供你设备的旋转

248
00:14:15,789 --> 00:14:19,359
和位置或相对位置

249
00:14:21,094 --> 00:14:24,264
但更重要的是 它提供现实世界的比例

250
00:14:24,631 --> 00:14:27,568
你的全部虚拟内容实际上要

251
00:14:27,634 --> 00:14:29,870
在物理场景中按比例进行渲染

252
00:14:31,438 --> 00:14:33,841
它还意味着你设备的移动

253
00:14:33,907 --> 00:14:37,411
与以米为测量单位测定的
所移动的物理距离相互关联

254
00:14:40,781 --> 00:14:42,749
追踪提供的全部位置

255
00:14:42,816 --> 00:14:45,485
都是相对于
你会话的起始位置而言的

256
00:14:47,221 --> 00:14:49,523
那么对于世界追踪还有另一个功能

257
00:14:50,123 --> 00:14:51,925
我们提供3D特征点

258
00:14:52,492 --> 00:14:55,529
那么这里有一个
关于世界追踪的呈现

259
00:14:55,596 --> 00:14:58,866
它通过探测特征进行的
也就是信息的唯一片段

260
00:14:58,932 --> 00:14:59,833
在相机图像中

261
00:15:00,601 --> 00:15:04,304
你可以看到
轴线代表我设备的位置和方向

262
00:15:04,371 --> 00:15:06,473
它随着我在现实世界中的
移动而创建路径

263
00:15:06,540 --> 00:15:08,275
但你还可以看到这上边的这些点

264
00:15:08,342 --> 00:15:11,845
这些代表3D特征点
是我在我的场景中探测到的

265
00:15:11,912 --> 00:15:15,682
我可以通过在场景中移动
让它们组成三角形

266
00:15:15,749 --> 00:15:18,585
然后通过利用这些点 匹配这些特征

267
00:15:18,652 --> 00:15:22,856
你会看到当我匹配一个我之前见到过的
现有特征时我会画一条线

268
00:15:23,290 --> 00:15:26,026
通过全部这些信息和我们的运动数据

269
00:15:26,093 --> 00:15:31,665
我们就可以精确地提供
设备方向和位置

270
00:15:32,766 --> 00:15:37,004
那么看起来可能很难 让我们看看代码
看我们是如何运行世界追踪的

271
00:15:38,438 --> 00:15:41,074
你需要做的第一件事
就是创建一个ARSession

272
00:15:41,141 --> 00:15:43,677
因为它会管理

273
00:15:43,744 --> 00:15:45,679
即将在世界追踪过程中
所发生的全部进程

274
00:15:45,979 --> 00:15:49,116
接下来你要把自己设为会话的委托

275
00:15:49,183 --> 00:15:52,085
以便当出现新帧时可以接收更新

276
00:15:53,120 --> 00:15:54,988
通过创建世界追踪会话配置

277
00:15:55,055 --> 00:15:56,523
你其实是在说“我想使用世界追踪

278
00:15:56,590 --> 00:15:58,825
我想让我的会话运行这个进程 ”

279
00:15:59,293 --> 00:16:02,563
然后通过调用Run 将立即启动进程

280
00:16:02,629 --> 00:16:03,864
开始捕捉

281
00:16:04,298 --> 00:16:09,036
那么在底层 我们的会话
创建一个AVCaptureSession 抱歉

282
00:16:09,102 --> 00:16:14,074
还有CMMotionManager
以便获取图像和运动数据

283
00:16:14,441 --> 00:16:16,677
我们通过图像探测场景中的特征

284
00:16:17,144 --> 00:16:18,946
我们高频率地使用运动数据

285
00:16:19,012 --> 00:16:22,149
以随着时间对它进行整合
获取你设备的移动信息

286
00:16:23,083 --> 00:16:26,153
将这些结合起来使用
我们就能使用传感器融合

287
00:16:26,220 --> 00:16:30,624
以提供精确的姿态
那么这些会在ARFrame中返回

288
00:16:32,192 --> 00:16:35,829
每个ARFrame
都会包含一个ARCamera

289
00:16:38,165 --> 00:16:41,034
那么ARCamera
是代表虚拟相机的对象

290
00:16:41,101 --> 00:16:42,769
或者你可以把它当虚拟相机使用

291
00:16:42,836 --> 00:16:45,606
它会呈现你设备的方向以及位置

292
00:16:45,873 --> 00:16:47,207
所以它提供一种转换

293
00:16:47,774 --> 00:16:50,944
转换是一个矩阵或SIMD浮点4x4

294
00:16:51,011 --> 00:16:55,015
提供你物理设备的方向
或旋转以及转化

295
00:16:55,082 --> 00:16:58,085
从会话的起点

296
00:16:59,019 --> 00:17:01,421
除此之外 我们还提供一个追踪状态

297
00:17:01,488 --> 00:17:03,924
让你了解如何使用转换

298
00:17:04,790 --> 00:17:07,426
最后 我们提供相机内参

299
00:17:07,928 --> 00:17:11,431
那么相机内参非常重要
我们在每一帧中都能获得相机内参

300
00:17:11,498 --> 00:17:14,535
因为它匹配你设备上的物理相机

301
00:17:14,734 --> 00:17:18,005
这种信息 比如焦距和主点

302
00:17:18,070 --> 00:17:19,473
用于寻找一个投影矩阵

303
00:17:20,073 --> 00:17:23,609
投影矩阵也是ARCamera上的一个
便利的方法

304
00:17:23,676 --> 00:17:26,646
那么你可以用这个
来方便地渲染你的虚拟几何体

305
00:17:28,615 --> 00:17:31,385
就是这样 这就是ARKit提供的追踪

306
00:17:31,451 --> 00:17:33,820
让我们继续 看一个使用世界追踪

307
00:17:33,887 --> 00:17:35,956
创建你的第一个ARKit应用的演示

308
00:17:42,629 --> 00:17:45,666
那么当你打开新版Xcode 9时
你注意到的第一件事

309
00:17:45,732 --> 00:17:49,603
就是有一个新模板
可用于创建增强现实应用

310
00:17:49,670 --> 00:17:51,171
那么让我们选择它

311
00:17:51,705 --> 00:17:54,341
我要创建一个增强现实应用
点击下一步

312
00:17:54,675 --> 00:17:57,878
在给对象命名之后 比如MyARApp

313
00:17:58,378 --> 00:17:59,980
我可以选择语言

314
00:18:00,314 --> 00:18:03,417
在这里我可以选择Swift和ObjectiveC

315
00:18:04,084 --> 00:18:05,719
以及内容技术

316
00:18:05,786 --> 00:18:08,388
那么内容技术用来

317
00:18:08,455 --> 00:18:10,657
渲染你的增强现实场景

318
00:18:10,858 --> 00:18:14,595
这里的选项有SceneKit、
SpriteKit以及Metal

319
00:18:15,128 --> 00:18:17,197
比如说我要使用SceneKit

320
00:18:19,233 --> 00:18:21,134
点击下一步
并创建我的工作空间之后

321
00:18:21,201 --> 00:18:22,569
看起来是这样的

322
00:18:23,203 --> 00:18:25,639
在这里我有一个
创建好的视图控制器

323
00:18:25,706 --> 00:18:27,741
你会看到它有一个ARSCNView

324
00:18:28,008 --> 00:18:32,980
那么这个ARSCNView是一个
自定义AR子类 替我实施全部渲染

325
00:18:33,046 --> 00:18:34,515
或替我实施绝大部分渲染

326
00:18:35,082 --> 00:18:37,351
那么它会处理虚拟相机的更新

327
00:18:37,417 --> 00:18:40,053
基于返回给它的ARFrame

328
00:18:40,287 --> 00:18:44,525
作为ARSCNView或sceneView的一个
属性 它有一个会话

329
00:18:45,292 --> 00:18:48,529
那么你可以看到我的sceneView
我设置了一个场景

330
00:18:48,595 --> 00:18:51,198
它将会成为一艘船 沿着Z轴

331
00:18:51,265 --> 00:18:53,834
在世界原点稍前一点儿的地方进行转换

332
00:18:54,101 --> 00:18:57,771
然后最重要的部分是我正在访问会话…

333
00:18:59,306 --> 00:19:01,241
我正在访问会话并调用Run

334
00:19:01,308 --> 00:19:03,410
通过一个世界追踪会话配置

335
00:19:03,810 --> 00:19:05,279
那么 这将运行世界追踪

336
00:19:05,345 --> 00:19:08,949
并且视图将自动处理
我的虚拟相机的更新

337
00:19:09,683 --> 00:19:11,318
那么让我们来试试看

338
00:19:11,385 --> 00:19:15,522
也许我应该把标准船型
改为使用arship

339
00:19:17,257 --> 00:19:19,626
那么让我们在设备上运行一下

340
00:19:25,332 --> 00:19:27,501
在安装后 你注意到的第一件事

341
00:19:27,568 --> 00:19:29,436
就是它会请求相机许可

342
00:19:29,503 --> 00:19:31,138
这是使用追踪

343
00:19:31,205 --> 00:19:33,340
以及渲染你场景的背景的请求

344
00:19:33,774 --> 00:19:35,676
接下来你将看到
我得到了一个相机种子

345
00:19:35,742 --> 00:19:37,744
在我正前方有一艘太空船

346
00:19:37,878 --> 00:19:42,216
你会看到当我改变设备的方向时
它在空间中仍保持固定

347
00:19:42,649 --> 00:19:45,485
但更重要的是
当我朝着太空船走过来走过去时

348
00:19:46,353 --> 00:19:49,323
你会看到它实际上是
被锚定在了物理世界中

349
00:19:50,390 --> 00:19:52,392
那么这是用了我设备的方向

350
00:19:52,459 --> 00:19:55,762
以及相对位置来更新虚拟相机

351
00:19:57,264 --> 00:19:58,432
和对太空船的视角

352
00:19:59,566 --> 00:20:01,168
那么让我们…谢谢

353
00:20:07,407 --> 00:20:09,042
如果这对于你
还不是那么有意思

354
00:20:09,109 --> 00:20:12,346
也许我们可以添加一些东西到场景中
每当我们轻触屏幕时

355
00:20:12,479 --> 00:20:15,315
让我们来试试
让我们试着在这个例子中添加一些东西

356
00:20:17,451 --> 00:20:21,188
正如我所说的 每次我轻触屏幕时
都要在场景中添加几何体

357
00:20:21,688 --> 00:20:23,390
我要做的第一件事

358
00:20:24,024 --> 00:20:25,959
就是添加一个轻触手势识别器

359
00:20:27,494 --> 00:20:32,199
那么将它添加到我的场景视图中后
每次我调用处理轻触方法时

360
00:20:32,266 --> 00:20:35,202
或每次我轻触屏幕时
都将调用处理轻触方法

361
00:20:36,069 --> 00:20:37,437
那么让我们实施一下

362
00:20:39,540 --> 00:20:42,943
那么如果我想创建一些几何体
比如说 我想创建一架飞机

363
00:20:43,010 --> 00:20:44,478
或飞机图像

364
00:20:46,880 --> 00:20:50,350
那么我要做的第一件事就是
用宽和高创建一个SCNPlane

365
00:20:50,417 --> 00:20:53,720
然后是最棘手的部分 我要设置内容

366
00:20:53,787 --> 00:20:57,524
或材料作为视图的快照

367
00:20:59,026 --> 00:21:00,827
那么你认为这会是什么呢？

368
00:21:00,928 --> 00:21:03,564
嗯 这其实是拍快照或视图的渲染

369
00:21:03,630 --> 00:21:06,266
包括背景相机图像

370
00:21:06,333 --> 00:21:09,536
以及我们放在背景前方的虚拟几何体

371
00:21:10,137 --> 00:21:12,139
我要把光照模型设为常量

372
00:21:12,206 --> 00:21:14,408
从而使ARKit提供的光估计

373
00:21:14,474 --> 00:21:16,343
不会应用于这个相机图像中

374
00:21:16,410 --> 00:21:18,846
因为它已经已经要跟环境匹配了

375
00:21:20,147 --> 00:21:22,015
接下来我要把这个添加到场景中

376
00:21:22,082 --> 00:21:24,718
我需要创建一个飞机节点

377
00:21:28,455 --> 00:21:31,592
在创建一个
封装此几何体的SCNode之后

378
00:21:31,658 --> 00:21:32,793
我把它添加到场景中

379
00:21:33,293 --> 00:21:35,462
已经在这里了 每次我轻触屏幕时

380
00:21:35,529 --> 00:21:37,197
它会在我的场景中
添加飞机图像

381
00:21:37,264 --> 00:21:40,067
但问题是它将总是会在000

382
00:21:40,501 --> 00:21:42,236
该如何让这个变得更有意思呢？

383
00:21:42,569 --> 00:21:44,571
嗯 我们已经给自己提供了

384
00:21:44,638 --> 00:21:47,007
一个当前帧 包含一个AR相机

385
00:21:47,774 --> 00:21:50,878
我很可能使用相机的转换

386
00:21:50,944 --> 00:21:53,146
来更新飞机节点的转换

387
00:21:53,213 --> 00:21:57,684
从而使飞机节点处于我的相机
当前在空间中所处的位置

388
00:21:58,452 --> 00:22:00,854
那么我首先要获取当前帧

389
00:22:00,921 --> 00:22:02,222
从我的SceneView会话中

390
00:22:04,124 --> 00:22:06,660
接下来我要更新飞机节点的转换

391
00:22:08,595 --> 00:22:10,597
以使用我的相机的转换

392
00:22:15,235 --> 00:22:16,703
在这里你将注意到我要做的第一件事是

393
00:22:16,770 --> 00:22:18,805
创建转化矩阵

394
00:22:19,006 --> 00:22:20,474
因为我不想把飞机图像

395
00:22:20,541 --> 00:22:22,843
放在相机所处的位置
那样会阻碍我的视线

396
00:22:22,910 --> 00:22:24,545
我想把它放在相机前方

397
00:22:24,745 --> 00:22:28,415
那么为此 我要将负Z轴用作转化

398
00:22:29,283 --> 00:22:32,986
你还可以看到 为了获得同样的比例
一切都是以米为测量单位的

399
00:22:33,053 --> 00:22:37,191
那么我要使用.1代表相机前10厘米

400
00:22:37,891 --> 00:22:40,827
通过多个位置和多个相机转换

401
00:22:40,894 --> 00:22:44,698
并将其应用到我的飞机节点中
将会形成一个飞机图像

402
00:22:44,765 --> 00:22:47,034
位于相机前10厘米处

403
00:22:47,835 --> 00:22:50,003
让我们尝试一下 看是什么样子

404
00:22:58,745 --> 00:23:02,516
那么你再一次看到做了这里
正在运行相机场景

405
00:23:02,583 --> 00:23:04,484
飞船也浮在空中

406
00:23:06,453 --> 00:23:10,090
现在 如果我轻触屏幕
也许这里 这里和这里

407
00:23:10,157 --> 00:23:13,026
你会看到它会在空间中我轻触的地方

408
00:23:13,093 --> 00:23:14,161
留下一个快照或图像

409
00:23:22,302 --> 00:23:25,639
这只是显示了你通过ARKit
所获得的其中一个可能性

410
00:23:25,873 --> 00:23:28,475
它真的实现了一个很酷的体验

411
00:23:30,644 --> 00:23:32,346
谢谢 这就是ARKit的使用

412
00:23:32,412 --> 00:23:37,718
（演示
创建你的第一个ARKit应用）

413
00:23:41,922 --> 00:23:44,491
那么现在你已经看到了
使用ARKit追踪的演示

414
00:23:44,558 --> 00:23:47,728
让我们谈谈从追踪结果中
获得最好的品质

415
00:23:49,029 --> 00:23:52,733
要注意的第一件事是追踪
依赖于不间断的传感器数据

416
00:23:52,799 --> 00:23:55,269
意思就是如果不再提供相机图像

417
00:23:55,335 --> 00:23:57,704
给你的会话 追踪将会停止

418
00:23:57,771 --> 00:23:59,006
我们就不能追踪了

419
00:24:00,574 --> 00:24:03,377
接下来追踪在结构良好的环境中
用起来效果最好

420
00:24:04,044 --> 00:24:07,014
意思是我们需要足够的视觉复杂性

421
00:24:07,080 --> 00:24:08,916
从你的相机图像中寻找特性

422
00:24:09,249 --> 00:24:11,752
那么如果我面对着一面白墙
或房间内的灯光太暗

423
00:24:11,818 --> 00:24:15,956
我就不能找到特征 并且追踪会受限制

424
00:24:17,624 --> 00:24:20,027
接下来追踪在静态场景中用起来也很好

425
00:24:20,360 --> 00:24:24,164
如果我相机捕捉到的大部分东西
都在移动 可是数据将不会响应

426
00:24:24,231 --> 00:24:29,136
移动数据 可能导致漂移
这也是一个受限的追踪状态

427
00:24:29,903 --> 00:24:34,508
要处理这些情况
ARCamera提供了一个追踪状态属性

428
00:24:36,043 --> 00:24:41,548
追踪状态有三个值：
不可用、正常和受限

429
00:24:42,149 --> 00:24:45,719
当你首次启动会话时
它开始显示为不可用

430
00:24:45,786 --> 00:24:47,421
意思就是你相机的转换

431
00:24:47,487 --> 00:24:50,457
还没有被填充 单位矩阵也没有准备好

432
00:24:51,792 --> 00:24:54,695
很快 一旦我们找到第一个追踪姿态

433
00:24:54,761 --> 00:24:57,698
状态将从不可用改为正常

434
00:24:58,198 --> 00:25:00,968
这表示你现在可以使用相机的转换了

435
00:25:02,870 --> 00:25:05,973
如果在这之后的某一个时间点
追踪受到限制

436
00:25:06,039 --> 00:25:09,009
追踪状态将会从正常改为受限

437
00:25:09,409 --> 00:25:10,911
同时会提供一个原因

438
00:25:11,078 --> 00:25:13,514
那么本例中的原因是
我正面对着一面白墙

439
00:25:13,580 --> 00:25:16,250
或者没有充足的光照 是特征不足

440
00:25:17,551 --> 00:25:19,987
当发生此种情况时
通知你的用户是很有帮助的

441
00:25:20,053 --> 00:25:21,989
那么 要实现这个功能 我们提供了

442
00:25:22,422 --> 00:25:24,558
一个你可以实施的会话委托方法：

443
00:25:24,725 --> 00:25:26,426
cameraDidChangeTrackingState

444
00:25:26,960 --> 00:25:29,329
当发生这种情况时
你可以获取追踪状态

445
00:25:29,396 --> 00:25:31,565
看是否受限 以及受限原因

446
00:25:32,499 --> 00:25:34,001
根据这个信息来通知你的用户

447
00:25:34,067 --> 00:25:37,838
因为他们是唯一能修改追踪情形的人

448
00:25:37,905 --> 00:25:41,008
通过开灯或不面对着白墙

449
00:25:43,143 --> 00:25:47,114
另一部分是如果传感器数据不可用

450
00:25:47,881 --> 00:25:50,717
那么对于这种情况
我们通过会话间断来处理

451
00:25:51,451 --> 00:25:54,121
那么如果你的相机输入不可用 由于…

452
00:25:54,188 --> 00:25:56,356
主要原因是你的应用处于后台

453
00:25:56,423 --> 00:25:59,226
或也许你正在iPad上实施多项任务

454
00:25:59,293 --> 00:26:01,795
相机图像也不会提供给你的会话

455
00:26:02,196 --> 00:26:04,831
在这种情况下 追踪变得不可用

456
00:26:04,898 --> 00:26:07,601
或停止 并且你的会话将被中断

457
00:26:07,668 --> 00:26:10,270
要处理这个问题 我们提供了委托方法

458
00:26:10,771 --> 00:26:11,839
让它变得非常简单

459
00:26:12,673 --> 00:26:16,476
在这里你可以呈现一个覆盖
或也许模糊屏幕

460
00:26:16,543 --> 00:26:19,880
向用户表明你的体验当前已被暂停

461
00:26:19,947 --> 00:26:21,448
追踪已停止

462
00:26:22,015 --> 00:26:25,219
在中断过程中 有一个重点需要注意

463
00:26:25,285 --> 00:26:27,754
因为没有任何追踪

464
00:26:27,821 --> 00:26:30,490
你设备的相对位置也不可用

465
00:26:30,691 --> 00:26:34,194
所以如果你在场景中
有锚点或物理位置

466
00:26:34,261 --> 00:26:37,798
它们可能不再对齐
如果在这个中断过程中有移动的话

467
00:26:38,699 --> 00:26:41,602
为此你可能想根据情况
重新启动你的体验

468
00:26:41,668 --> 00:26:43,470
当你从中断中返回来时

469
00:26:44,872 --> 00:26:47,508
那么这就是追踪

470
00:26:48,342 --> 00:26:51,778
让我们把舞台交给Stefan
让他讲讲场景理解 谢谢大家

471
00:26:57,818 --> 00:26:58,719
谢谢Mike

472
00:26:59,720 --> 00:27:00,988
大家下午好

473
00:27:01,321 --> 00:27:04,658
我叫Stefan Misslinger
我是ARKit团队中的一名工程师

474
00:27:04,725 --> 00:27:07,361
接下来我要讲讲场景理解

475
00:27:07,728 --> 00:27:11,365
那么场景理解的目标是
寻找更多与环境相关的信息

476
00:27:11,431 --> 00:27:14,568
以便在这个环境中放置虚拟对象

477
00:27:15,102 --> 00:27:19,072
这种信息包括比如环境3D拓扑

478
00:27:19,139 --> 00:27:24,778
以及光照条件 以便逼真地摆放对象

479
00:27:25,979 --> 00:27:28,482
让我们看一下这个桌子作为示例

480
00:27:29,183 --> 00:27:32,586
如果你想在这个桌子上放置一个对象
一个虚拟对象

481
00:27:32,653 --> 00:27:34,121
我们需要了解的第一件事

482
00:27:34,188 --> 00:27:37,191
就是那儿有一个表面
可以让我们放置东西

483
00:27:37,524 --> 00:27:40,227
这是通过平面探测实现的

484
00:27:41,628 --> 00:27:44,531
第二 我们需要计算3D坐标

485
00:27:44,598 --> 00:27:47,134
我们要在这个坐标上
放置我们的虚拟对象

486
00:27:47,734 --> 00:27:50,437
要找到坐标 我们得使用碰撞测试

487
00:27:51,004 --> 00:27:54,474
这包括从我们的设备上发送一条射线
并将其与现实世界交叉

488
00:27:54,541 --> 00:27:56,977
以便找到这个坐标

489
00:27:58,645 --> 00:28:03,650
第三 要把这个对象
逼真地摆放到现实世界

490
00:28:03,717 --> 00:28:07,955
我们需要光预计
以匹配环境中的光照情况

491
00:28:08,889 --> 00:28:11,391
让我们分别来看一下这三点

492
00:28:11,458 --> 00:28:12,926
让我们从平面探测开始

493
00:28:14,394 --> 00:28:16,997
那么平面探测为你提供水平面

494
00:28:17,064 --> 00:28:18,832
就重力而言

495
00:28:19,533 --> 00:28:21,702
这种位面包括地平面

496
00:28:21,768 --> 00:28:25,272
以及任意平行平面 如桌子

497
00:28:26,607 --> 00:28:30,777
ARKit通过在多帧之上
聚合信息来实现

498
00:28:30,844 --> 00:28:32,112
所以它在后台运行

499
00:28:32,679 --> 00:28:35,449
当用户在场景中移动设备时

500
00:28:35,516 --> 00:28:37,518
它就了解了关于这个位面的更多信息

501
00:28:39,086 --> 00:28:43,991
这也允许我们取回
关于这个位面的一致性数据

502
00:28:44,057 --> 00:28:49,329
意思是我们正在把一个矩形
试着放到这个位面被检测的部分

503
00:28:49,396 --> 00:28:51,331
并让它与主范围对齐

504
00:28:51,598 --> 00:28:53,066
这就给你提供了

505
00:28:53,133 --> 00:28:56,737
某个物理位面的主要定位

506
00:28:58,405 --> 00:29:01,575
此外如果在同一个物理水平面上
检测多个虚拟位面

507
00:29:01,642 --> 00:29:05,445
ARKit将合并在一起处理

508
00:29:06,580 --> 00:29:12,152
然后合并后的位面
将增大到两个位面的范围

509
00:29:12,219 --> 00:29:14,788
因为较新的位面将从会话中被移除

510
00:29:16,190 --> 00:29:18,258
让我们看一下在代码中是如何使用的

511
00:29:20,194 --> 00:29:24,665
你要做的第一件事就是创建一个
ARWorldTracking会话配置

512
00:29:25,666 --> 00:29:27,935
平面探测是一个属性 你可以

513
00:29:28,001 --> 00:29:30,304
在ARWorldTracking会话配置中设置

514
00:29:30,571 --> 00:29:32,272
那么要启动平面探测

515
00:29:32,339 --> 00:29:35,709
你只需要将平面探测属性
设为水平即可

516
00:29:36,810 --> 00:29:39,146
之后把配置传回

517
00:29:39,213 --> 00:29:42,015
ARSession 通过调用Run方法

518
00:29:42,082 --> 00:29:45,118
它将会在你的环境中开始探测平面

519
00:29:47,421 --> 00:29:50,224
如果你要关闭平面探测

520
00:29:50,290 --> 00:29:53,927
我们只需要将平面探测属性设为无

521
00:29:54,161 --> 00:29:57,297
然后在ARSession上
再次调用Run方法

522
00:29:58,065 --> 00:30:01,335
之前探测到的平面将保留在会话中

523
00:30:02,102 --> 00:30:06,473
意思是它们仍会
出现在我们的ARFrame锚点中

524
00:30:08,242 --> 00:30:11,445
那么无论何时探测到一个新平面

525
00:30:11,512 --> 00:30:14,248
它们都将
作为ARPlaneAnchor显示给你

526
00:30:15,015 --> 00:30:18,118
ARPlaneAnchor是ARAnchor的子类

527
00:30:18,185 --> 00:30:21,488
意思是它呈现现实世界位置和方向

528
00:30:23,457 --> 00:30:25,893
无论何时检测到一个新锚点

529
00:30:25,959 --> 00:30:29,429
你都将收到一个
委托调用会话didAdd锚点

530
00:30:30,030 --> 00:30:33,100
你可以用那个来 比如形象化你的平面

531
00:30:34,034 --> 00:30:38,739
平面的范围将作为范围呈现给你

532
00:30:38,972 --> 00:30:42,609
是相对于中心属性而言的

533
00:30:44,778 --> 00:30:47,381
那么随着用户在场景中移动设备

534
00:30:47,447 --> 00:30:50,584
我们将更多地了解这个平面
并且更新它的范围

535
00:30:52,052 --> 00:30:56,623
当更新范围时
你会收到一个委托会话didUpdate帧

536
00:30:56,690 --> 00:30:58,158
didUpdate锚点

537
00:30:58,759 --> 00:31:01,562
你可以用于更新你的视觉展现

538
00:31:02,529 --> 00:31:06,233
请注意中心属性实际是如何移动的
因为平面

539
00:31:06,300 --> 00:31:08,502
更多的是在一个方向上增长

540
00:31:11,305 --> 00:31:14,441
无论何时锚点被从会话中移除时

541
00:31:14,508 --> 00:31:17,477
你都将收到一个委托调用会话
didRemove锚点

542
00:31:18,579 --> 00:31:22,115
如果把多个平面合并在一起

543
00:31:22,182 --> 00:31:24,117
并移除其中一个作为结果时
会出现这种情况

544
00:31:24,618 --> 00:31:29,223
在那种情况下 你会收到一个
委托调用会话didRemove锚点

545
00:31:29,289 --> 00:31:31,725
你可以根据情况更新你的视觉展现

546
00:31:33,961 --> 00:31:37,397
那么现在我们有了一个
关于环境中哪里有平面的概念了

547
00:31:37,464 --> 00:31:40,300
让我们看看如何
在平面上实际地放置东西

548
00:31:40,934 --> 00:31:43,103
为此我们提供碰撞测试

549
00:31:44,638 --> 00:31:47,040
那么碰撞测试包括向现实世界发送

550
00:31:47,107 --> 00:31:49,843
或交叉来自你设备的一条射线

551
00:31:49,910 --> 00:31:53,180
并寻找交叉点

552
00:31:55,616 --> 00:31:58,385
ARKit会使用全部可用的场景信息

553
00:31:58,452 --> 00:32:02,456
包括所探测到的平面以及

554
00:32:02,523 --> 00:32:06,193
ARWorldTracking 所使用的3D特征点
以算出它的位置

555
00:32:08,862 --> 00:32:11,832
然后ARKit交叉我们的射线

556
00:32:11,899 --> 00:32:16,336
通过全部可用信息

557
00:32:16,403 --> 00:32:21,975
并将全部交叉点返回为一个
按距离排序的数组

558
00:32:22,242 --> 00:32:25,946
这个数组中的第一条
将是距离摄像机最近的交叉点

559
00:32:28,348 --> 00:32:32,619
并且有不同的方式可以实施这个交叉点

560
00:32:32,986 --> 00:32:37,057
你可以通过提供碰撞测试类型
来定义这个方式

561
00:32:38,559 --> 00:32:41,328
有四种方式可以实现
让我们来看看

562
00:32:43,530 --> 00:32:46,633
如果你正在运行平面探测
并且ARKit已经

563
00:32:46,700 --> 00:32:50,270
在我们的环境中探测到了一个平面
我们就可以利用它

564
00:32:51,805 --> 00:32:56,143
在这里你可以选择
使用平面的范围或忽略它

565
00:32:56,944 --> 00:33:01,215
如果你希望你的用户可以

566
00:33:01,281 --> 00:33:04,551
在平面上移动对象

567
00:33:04,618 --> 00:33:09,289
你可以考虑使用范围 意思是
如果射线在它的范围内交叉了

568
00:33:09,356 --> 00:33:12,526
它就会提供给你一个交叉点

569
00:33:12,926 --> 00:33:16,830
如果涉嫌碰到了它的外部
它将不会返回给你一个交叉点

570
00:33:19,099 --> 00:33:22,669
情况有 比如移动家具

571
00:33:22,736 --> 00:33:26,306
或当你只探测到了水平面的一小部分时

572
00:33:26,373 --> 00:33:29,543
我们可以选择忽略
这个范围并将现有平面

573
00:33:29,610 --> 00:33:30,911
作为一个无限平面

574
00:33:31,912 --> 00:33:34,481
在那种情况下
你总是会收到一个交叉点

575
00:33:35,916 --> 00:33:38,619
你可以只使用现实世界中的一小片

576
00:33:38,685 --> 00:33:43,490
但让你的用户沿着这个平面移动对象

577
00:33:45,592 --> 00:33:50,531
如果你没有运行平面探测
或我们还没有探测到任何平面

578
00:33:50,597 --> 00:33:53,767
我们也可以根据当前可用的3D特征点

579
00:33:53,834 --> 00:33:54,968
预估一个平面

580
00:33:56,270 --> 00:34:00,340
在那种情况下
ARKit将在我们的环境中寻找共面的点

581
00:34:00,407 --> 00:34:02,209
并在那儿放置一个适合的平面

582
00:34:02,743 --> 00:34:05,946
之后它将给你返回这个平面的交叉点

583
00:34:08,748 --> 00:34:11,784
如果你想在一个很小的表面放置东西

584
00:34:11,851 --> 00:34:15,922
而那并不能形成一个平面的话
或你在一个非常不规则的环境中

585
00:34:15,989 --> 00:34:19,226
你还可以选择直接与特征点交叉

586
00:34:21,094 --> 00:34:24,598
意思是我们会随着射线找到交叉点

587
00:34:24,665 --> 00:34:26,733
是距离现有的特征点最近的交叉点

588
00:34:26,800 --> 00:34:29,069
并返回这个交叉点作为结果

589
00:34:30,469 --> 00:34:32,539
让我们在代码中看看是如何实现的

590
00:34:34,641 --> 00:34:38,178
那么我们要做的第一件事
就是定义我们的射线

591
00:34:38,712 --> 00:34:41,748
它在我们的设备上进行交叉

592
00:34:43,483 --> 00:34:45,185
你把射线提供为一个CG点

593
00:34:45,252 --> 00:34:48,355
呈现方式是标准化的图像空间坐标

594
00:34:48,422 --> 00:34:53,527
意思是我们的图像左上角是0 0
而右下角是1 1

595
00:34:55,062 --> 00:34:57,297
那么如果我们想发送一条射线

596
00:34:57,364 --> 00:35:00,334
或寻找一个交叉点 在屏幕的中心位置

597
00:35:00,400 --> 00:35:05,239
我们要把CG点的x和y坐标
都定义为0.5

598
00:35:06,473 --> 00:35:10,143
如果你用的是SceneKit或SpriteKit
我们提供一种自定义覆盖

599
00:35:10,210 --> 00:35:16,083
你只需要传递
一个CG点的几个坐标即可

600
00:35:16,149 --> 00:35:19,686
那么你可以优先于触摸手势
先使用UI轻触的结果

601
00:35:19,753 --> 00:35:23,757
作为输入来定义这条射线

602
00:35:25,626 --> 00:35:28,729
那么让我们把这个点
传给碰撞测试方法

603
00:35:28,795 --> 00:35:31,732
并定义我们要使用的碰撞测试类型

604
00:35:31,798 --> 00:35:35,269
在这种情况下我们要使用现有平面
意思是它将

605
00:35:35,335 --> 00:35:38,705
于ARKit已经探测到的任意平面
进行交叉

606
00:35:38,772 --> 00:35:41,074
也会与预估水平平面进行交叉

607
00:35:41,141 --> 00:35:46,446
那么这个可以作为备用方案
如果尚未探测到任何平面的话

608
00:35:48,081 --> 00:35:53,987
之后ARKit将会返回结果数组

609
00:35:54,054 --> 00:35:55,989
你可以获取第一条结果

610
00:35:56,056 --> 00:35:59,459
将会是距离你的相机最近的交叉点

611
00:36:02,129 --> 00:36:05,933
交叉点包含在我们碰撞测试

612
00:36:05,999 --> 00:36:07,534
结果的worldTransform属性中

613
00:36:07,601 --> 00:36:10,671
并且我们可以根据这个结果
创建一个新的ARAnchor

614
00:36:10,737 --> 00:36:14,575
并将其传回给会话
因为我们想与它保持联系

615
00:36:16,410 --> 00:36:17,845
那么如果我们把这段代码

616
00:36:17,911 --> 00:36:23,183
应用到这里的场景中
我们把手机指向桌子

617
00:36:23,250 --> 00:36:28,622
它将会返回在屏幕中央
给我们返回与这张桌子的交叉点

618
00:36:28,689 --> 00:36:32,125
我们可以在这个位置摆放一个虚拟茶杯

619
00:36:34,094 --> 00:36:37,598
默认情况是你的渲染引擎
将假设你的背景图片

620
00:36:37,664 --> 00:36:42,803
光照情况很完美 所以你增加的
虚拟对象看起来应该真的就属于那儿

621
00:36:43,237 --> 00:36:45,539
然而 如果你处于一个较暗的环境中

622
00:36:46,740 --> 00:36:50,277
然后你的相机图像也较暗
意思是你添加的虚拟对象

623
00:36:50,344 --> 00:36:52,946
看起来格格不入 它看起来很亮

624
00:36:54,214 --> 00:36:58,352
要修复这个问题 我们需要调整

625
00:36:58,418 --> 00:37:00,521
虚拟对象的相对亮度

626
00:37:02,089 --> 00:37:04,825
为此 我们提供光估计

627
00:37:07,361 --> 00:37:10,531
那么光估计是在你的相机图像上应用的

628
00:37:10,931 --> 00:37:12,666
它通过它的曝光信息

629
00:37:12,733 --> 00:37:15,636
来决定它的相对亮度

630
00:37:16,737 --> 00:37:20,007
光照良好的图像 默认是1000流明

631
00:37:20,073 --> 00:37:23,143
对于较亮的环境
你将得到一个更高的值

632
00:37:23,210 --> 00:37:25,712
对于较暗的环境
你将得到一个较低的值

633
00:37:26,647 --> 00:37:30,417
你还可以直接把这个值分配给SEN光

634
00:37:30,484 --> 00:37:32,920
作为其外界强度属性

635
00:37:32,986 --> 00:37:35,422
因此如果你使用的是物理光

636
00:37:35,489 --> 00:37:37,724
它将自动利用这个

637
00:37:39,493 --> 00:37:41,895
光预计是默认启动的

638
00:37:41,962 --> 00:37:44,064
你可以通过设置

639
00:37:44,131 --> 00:37:47,701
isLightEstimationEnabled
属性配置光估计

640
00:37:47,768 --> 00:37:49,570
在ARSession配置中

641
00:37:50,437 --> 00:37:52,406
光估计的结果

642
00:37:52,706 --> 00:37:55,142
以光估计属性提供给你

643
00:37:55,209 --> 00:37:59,379
这个属性在ARFrame中
作为其外界强度值

644
00:38:01,682 --> 00:38:03,817
那么让我们看一个演示

645
00:38:03,884 --> 00:38:06,653
看看我们是如何
通过ARKit使用场景理解的

646
00:38:17,431 --> 00:38:19,566
我要给大家展示的应用

647
00:38:19,633 --> 00:38:22,002
是ARKit示例应用

648
00:38:22,069 --> 00:38:26,006
意思是你可以从我们的
开发者网站上下载它

649
00:38:27,074 --> 00:38:30,043
它展示的是在我们的环境中放置对象

650
00:38:30,377 --> 00:38:33,814
为此它使用了场景理解

651
00:38:33,881 --> 00:38:36,383
那么让我们打开它

652
00:38:37,951 --> 00:38:41,221
如果我移动它 你在我前方看到的

653
00:38:41,722 --> 00:38:44,625
是我们的焦距方框

654
00:38:44,992 --> 00:38:49,263
这是通过在我们的屏幕中央做碰撞测试

655
00:38:50,030 --> 00:38:53,200
并寻找在其交叉点上放置对象实现的

656
00:38:53,734 --> 00:38:56,136
那么如果我们沿着桌子移动它

657
00:38:56,203 --> 00:38:58,805
你可以看到它基本上是沿着桌子滑动

658
00:39:00,340 --> 00:39:03,544
它还使用了平行平面探测

659
00:39:03,610 --> 00:39:06,380
我们可以将它可视化
来看看发生了什么

660
00:39:06,680 --> 00:39:08,916
让我们打开我们的调试菜单

661
00:39:09,550 --> 00:39:12,853
并激活这里的第二个选项
即调试视觉展现

662
00:39:13,720 --> 00:39:14,555
让我们关闭它

663
00:39:15,389 --> 00:39:17,925
你可以从这里看到它所检测到的平面

664
00:39:20,127 --> 00:39:22,963
为了让你更好地理解它
让我们重新启动它

665
00:39:26,133 --> 00:39:27,801
看看它是如何发现新平面的

666
00:39:27,868 --> 00:39:31,271
那么如果我移动它
你会看到它探测到了一个新平面

667
00:39:32,105 --> 00:39:34,308
让我们迅速把它指向
这张桌子的另一个部分

668
00:39:34,374 --> 00:39:36,243
它找到了另一个平面

669
00:39:36,677 --> 00:39:40,214
如果我沿着这张桌子移动

670
00:39:40,714 --> 00:39:42,783
它最终会把这两个平面合并到一起

671
00:39:42,850 --> 00:39:45,652
并且算出只有一个平面

672
00:39:54,361 --> 00:39:56,830
那么接下来
让我们放置一些实际的对象

673
00:39:59,533 --> 00:40:02,636
我女儿让我给这场演讲带一些花儿来

674
00:40:02,703 --> 00:40:04,538
而我不想让她失望

675
00:40:05,038 --> 00:40:09,943
那么让我们做得更浪漫一点儿吧
放一个漂亮的花瓶

676
00:40:10,978 --> 00:40:15,983
在那种情况下 我们再一次把它
与我们的屏幕中心进行碰撞测试

677
00:40:16,049 --> 00:40:18,752
并找到交叉点来放置对象

678
00:40:21,588 --> 00:40:25,259
这里有一点很重要 就是这个花瓶

679
00:40:25,325 --> 00:40:28,829
会以现实世界的比例显示
有两个东西造成了这种可能性

680
00:40:29,429 --> 00:40:35,335
其中一个是WorldTracking
给我们提供比例姿态

681
00:40:36,036 --> 00:40:37,337
第二个是

682
00:40:37,404 --> 00:40:41,909
3D模型其实是以现实世界的坐标
来做的3D模型

683
00:40:41,975 --> 00:40:44,444
那么这真的很重要
如果你正在为增强现实创建内容

684
00:40:44,511 --> 00:40:47,714
你要考虑

685
00:40:47,781 --> 00:40:53,220
这个花瓶看起来不应该
跟楼一样高或也不能太小

686
00:40:55,022 --> 00:40:59,226
那么让我们继续放一个
交互性更强的对象

687
00:40:59,660 --> 00:41:01,361
就是我的变色龙朋友

688
00:41:04,364 --> 00:41:07,367
有一点很好…谢谢

689
00:41:07,801 --> 00:41:11,705
有一点很好
就是你总会了解用户的位置

690
00:41:11,772 --> 00:41:15,375
当你运行WorldTracking时

691
00:41:15,676 --> 00:41:20,747
因此你可以让虚拟内容
与用户在现实世界中进行交互

692
00:41:29,590 --> 00:41:30,924
所以如果我移动到这儿

693
00:41:32,759 --> 00:41:37,231
它最终会转向我这边
如果它不害怕的话 是的 就这样

694
00:41:44,037 --> 00:41:47,307
如果我离得再近点儿 他甚至可能会
以不同的方式与我进行交互

695
00:41:47,608 --> 00:41:50,611
让我么来看看 它有点儿…哦！会这样

696
00:41:53,847 --> 00:41:56,984
变色龙还能做一件事
就是改变它们的皮肤颜色

697
00:41:57,150 --> 00:42:01,788
如果我轻触他 他就会调节颜色

698
00:42:03,891 --> 00:42:06,193
那么我们把它变成绿色吧

699
00:42:07,961 --> 00:42:12,833
我们在这里有一个很棒的功能
就是我可以沿着桌子移动他

700
00:42:13,700 --> 00:42:18,906
并且他会使自己适应桌子的背景颜色
以便很好地隐藏它自己

701
00:42:28,982 --> 00:42:31,451
那么这就是我们的示例应用

702
00:42:31,518 --> 00:42:33,687
你可以从网站上下载

703
00:42:34,221 --> 00:42:39,993
并加入你自己的内容然后试试效果

704
00:42:40,327 --> 00:42:44,298
那么接下来
我们要看看用ARKit进行渲染

705
00:42:47,734 --> 00:42:50,571
渲染把追踪和场景理解

706
00:42:50,637 --> 00:42:52,105
与你的内容结合在一起

707
00:42:52,940 --> 00:42:54,908
为了用ARKit进行渲染

708
00:42:54,975 --> 00:42:58,712
你需要处理我们在ARFrame中
给你提供的全部信息

709
00:43:00,113 --> 00:43:02,783
如果你们用的是SceneKit和SpriteKit

710
00:43:02,850 --> 00:43:05,285
我们还有已经创建好的自定义视图

711
00:43:05,352 --> 00:43:08,021
可以为你渲染ARFrame

712
00:43:09,323 --> 00:43:13,260
如果你用的是metal
并且希望创建你自己的渲染引擎

713
00:43:13,327 --> 00:43:16,897
或把ARKit整合到你现有的渲染引擎中

714
00:43:16,964 --> 00:43:20,834
我们提供一个模板
可以告诉你如何实现

715
00:43:20,901 --> 00:43:23,036
并提供一个很好的起点

716
00:43:24,004 --> 00:43:27,241
让我们分别来看一下 从SceneKit开始

717
00:43:28,609 --> 00:43:33,480
对于SceneKit 我们提供一个ARSCNView
是SCNView的子类

718
00:43:34,381 --> 00:43:38,151
它包含ARSession 用于更新其渲染

719
00:43:39,052 --> 00:43:41,855
所以这个包含在后台绘制相机图像

720
00:43:42,623 --> 00:43:45,392
考虑设备的旋转

721
00:43:45,459 --> 00:43:47,294
以及任何的[不清楚]变更

722
00:43:49,263 --> 00:43:53,467
接下来它根据我们在ARCamera中

723
00:43:53,534 --> 00:43:55,736
所提供的追踪转换更新SCNCamera

724
00:43:56,670 --> 00:44:01,074
你的场景保持交互
并且ARKit会控制SCNCamera

725
00:44:01,141 --> 00:44:02,643
通过在场景中移动它

726
00:44:02,709 --> 00:44:05,712
按照你在现实世界移动你设备的方式

727
00:44:07,281 --> 00:44:08,815
如果你使用光估计

728
00:44:08,882 --> 00:44:13,620
我们会自动在你的场景中
放置一个SCN光探针

729
00:44:13,687 --> 00:44:18,892
那么如果你使用启用了物理光的对象

730
00:44:18,959 --> 00:44:21,895
你已经可以利用或自动利用

731
00:44:21,962 --> 00:44:23,263
光估计了

732
00:44:25,866 --> 00:44:32,406
ARCNView要做的另一件事就是
把SCNNotes映射到ARAnchors

733
00:44:32,472 --> 00:44:36,310
所以你其实不需要
直接与ARAnchors交界

734
00:44:36,376 --> 00:44:38,679
但却可以继续使用SCNNotes

735
00:44:39,613 --> 00:44:42,916
意思是无论何时
向会话中添加新ARAnchor时

736
00:44:42,983 --> 00:44:45,652
ARSCNView将为你创建一个节点

737
00:44:45,953 --> 00:44:48,755
我们每次更新ARAnchor时

738
00:44:48,822 --> 00:44:52,659
比如它的转换
我们会自动更新节点转换

739
00:44:54,895 --> 00:44:58,165
并且这是通过ARSCNView委托处理的

740
00:45:00,400 --> 00:45:04,037
所以每次我们向会话中添加新锚点

741
00:45:05,272 --> 00:45:08,709
ARSCNView将为你创建一个新SCNNode

742
00:45:09,209 --> 00:45:14,114
如果你想提供你自己的节点
你可以实施渲染器nodeFor锚点

743
00:45:14,181 --> 00:45:16,283
并返回给你的自定义节点

744
00:45:16,783 --> 00:45:21,889
之后SCNNode将被添加到场景图中

745
00:45:21,955 --> 00:45:26,727
并且你会收到锚点的另一个
委托调用渲染器didAdd节点

746
00:45:28,328 --> 00:45:32,199
无论何时当更新节点时也同样适用

747
00:45:32,266 --> 00:45:33,367
那么在那种情况下

748
00:45:36,270 --> 00:45:41,041
DSCNNodes转换将会自动
被更新为ARAnchors转换

749
00:45:41,108 --> 00:45:44,678
并且你将收到两个回调

750
00:45:44,745 --> 00:45:47,648
在更新其转换之前收到一个

751
00:45:47,714 --> 00:45:50,317
在更新转换之后收到一个

752
00:45:52,586 --> 00:45:55,656
无论何时ARAnchor被从会话中移除时

753
00:45:56,657 --> 00:45:59,092
我们都会自动移除相应的SCNNode

754
00:45:59,159 --> 00:46:02,429
从场景图中
并给你提供锚点回调渲染器

755
00:46:02,496 --> 00:46:04,164
didRemove节点

756
00:46:06,166 --> 00:46:10,437
那么这是SceneKit
接下来让我们看看SpriteKit

757
00:46:13,040 --> 00:46:17,544
对于SpriteKit 我们提供一个ARSKview
是SKView的子类

758
00:46:18,378 --> 00:46:23,016
它包含ARSession 用于更新其渲染

759
00:46:23,083 --> 00:46:25,819
这包含在后台绘制相机图像

760
00:46:26,854 --> 00:46:30,324
在这种情况下
把SKNodes映射到ARAnchors

761
00:46:30,624 --> 00:46:34,027
所以它提供一套与SceneKit
非常类似的委托方法

762
00:46:34,094 --> 00:46:35,095
它可以使用

763
00:46:36,063 --> 00:46:39,633
其中一个主要的不同点是
SpriteKit是一个2D渲染引擎

764
00:46:39,700 --> 00:46:43,937
意思是我们不能更新移动中的相机

765
00:46:44,271 --> 00:46:49,843
那么ARKit的作用是
把我们ARAnchor的位置投影到

766
00:46:49,910 --> 00:46:52,679
SpriteKit视图中

767
00:46:53,046 --> 00:46:55,682
然后将Sprite看作
这些位置上的广告牌进行渲染

768
00:46:55,749 --> 00:46:58,652
在所投影的位置

769
00:46:58,719 --> 00:47:01,922
这意味着Sprite
将总是面对着相机

770
00:47:04,024 --> 00:47:08,028
如果你想了解更多信息
有一场来自SpriteKit团队的演讲

771
00:47:08,529 --> 00:47:10,831
“在SpriteKit中超越2D”

772
00:47:10,898 --> 00:47:14,468
主要讲如何整合ARKit和SpriteKit

773
00:47:17,638 --> 00:47:20,974
接下来让我们看看ARKit自定义渲染

774
00:47:21,041 --> 00:47:21,942
使用Metal

775
00:47:23,443 --> 00:47:27,948
要通过ARKit进行渲染
你需要做四件事

776
00:47:28,582 --> 00:47:31,084
第一件是在后台绘制相机图像

777
00:47:32,352 --> 00:47:36,256
你通常会创建一个纹理
然后在后台进行绘制

778
00:47:37,457 --> 00:47:41,728
接下来是根据我们的ARCamera
更新虚拟相机

779
00:47:42,296 --> 00:47:46,466
这包括设置视图矩阵以及投影矩阵

780
00:47:48,602 --> 00:47:52,639
第三件是更新光照情况或

781
00:47:52,706 --> 00:47:55,075
你场景中的光照 按照我们的光估计

782
00:47:55,976 --> 00:48:01,381
最后如果你按照场景理解放置了几何体

783
00:48:01,448 --> 00:48:07,020
那么你要使用ARAnchors
来正确地设置转换

784
00:48:08,121 --> 00:48:10,858
全部这些信息都包含在
ARFrame中

785
00:48:11,158 --> 00:48:13,727
你有两种方式来获取
这个ARFrame

786
00:48:15,362 --> 00:48:19,032
一种是在ARSession上
查询当前帧属性

787
00:48:20,000 --> 00:48:22,369
那么如果你有自己的渲染循环

788
00:48:22,436 --> 00:48:26,607
你要使用或你可以使用
这种方式来获取当前帧

789
00:48:27,307 --> 00:48:32,012
然后你还应该在ARFrame上
利用时间戳属性

790
00:48:32,079 --> 00:48:35,148
以避免多次渲染同一帧

791
00:48:36,650 --> 00:48:39,486
另一种方式是使用我们的会话委托

792
00:48:39,953 --> 00:48:42,589
它会给你提供会话didUpdate帧

793
00:48:42,656 --> 00:48:44,791
每当计算新帧时

794
00:48:46,026 --> 00:48:50,230
在那种情况下
你只需要利用它来更新你的渲染

795
00:48:50,998 --> 00:48:53,467
默认情况是
这会在主[不清楚]中被调用

796
00:48:53,534 --> 00:48:55,502
但你还可以提供你自己的调度队列

797
00:48:55,569 --> 00:48:59,940
我们将用于调用这个方法

798
00:49:00,741 --> 00:49:05,045
那么让我们看一下更新渲染包含什么

799
00:49:07,281 --> 00:49:10,284
那么第一件事就是在后台绘制相机图像

800
00:49:10,551 --> 00:49:14,054
你可以在ARFrame上
获取所捕捉的图像属性

801
00:49:14,121 --> 00:49:15,689
即CV像素缓冲器

802
00:49:16,790 --> 00:49:20,360
你可以根据这个像素缓冲器
生成一个金属纹理

803
00:49:20,427 --> 00:49:23,096
然后在后台的quad中进行绘制

804
00:49:24,464 --> 00:49:29,303
请注意这是一个像素缓冲器
是通过AV Foundation发布给我们的

805
00:49:29,369 --> 00:49:34,141
所以你不应该长时间等待太多帧

806
00:49:34,208 --> 00:49:36,443
否则你将停止接收更新

807
00:49:38,745 --> 00:49:43,116
第二件事是根据我们的ARCamera
更新虚拟相机

808
00:49:43,884 --> 00:49:46,053
为此我们必须决定视图矩阵

809
00:49:46,119 --> 00:49:47,921
以及防护矩阵

810
00:49:49,056 --> 00:49:52,826
视图矩阵是相机转换的对立面

811
00:49:53,961 --> 00:49:56,230
为了生成投影矩阵

812
00:49:56,296 --> 00:49:59,433
我们会在ARCamera上
向你提供便利方法

813
00:49:59,499 --> 00:50:01,568
向你提供投影矩阵

814
00:50:03,837 --> 00:50:05,973
第三部是更新光照

815
00:50:07,674 --> 00:50:10,344
为此只需要获取光估计属性

816
00:50:10,410 --> 00:50:15,649
并使用其外界强度
以更新你的光照模型

817
00:50:17,084 --> 00:50:21,221
最后应该是迭代锚点
及其3D地理位置

818
00:50:21,288 --> 00:50:23,957
以更新几何体的转换

819
00:50:24,191 --> 00:50:27,494
那么你手动添加到会话中的任意锚点

820
00:50:27,561 --> 00:50:29,663
或已经探测出来的任意锚点

821
00:50:29,730 --> 00:50:34,668
或向平面探测中添加的任意锚点
都将成为这些帧锚点的一部分

822
00:50:37,471 --> 00:50:42,476
然后需要注意一些事情
当根据相机图像进行渲染时

823
00:50:42,976 --> 00:50:44,645
我们想看看这些

824
00:50:45,479 --> 00:50:51,151
那么其中一件事是所捕捉的图像
被包含在ARFrame中

825
00:50:51,218 --> 00:50:52,953
总是会在同一个方向中被提供

826
00:50:53,787 --> 00:50:58,425
然而 如果你旋转你的物理设备
它可能不会

827
00:50:58,492 --> 00:51:00,594
与你的用户界面方向对齐

828
00:51:00,661 --> 00:51:04,598
并且需要应用转换 以正确地进行渲染

829
00:51:06,567 --> 00:51:09,870
另一件事就是相机图像的纵横比可能不

830
00:51:09,937 --> 00:51:12,372
必要与你的设备对齐

831
00:51:12,973 --> 00:51:15,242
意思是我们要考虑到这个

832
00:51:15,309 --> 00:51:19,646
以在屏幕中合适地渲染相机图像

833
00:51:21,081 --> 00:51:23,517
要修复这个或为了让这个变得更简单

834
00:51:23,584 --> 00:51:25,552
我们给你提供了辅助方法

835
00:51:26,720 --> 00:51:31,959
那么在ARFrame上有一个方法
是显示转换

836
00:51:32,659 --> 00:51:36,463
显示转换会将帧空间转换为视图空间

837
00:51:36,730 --> 00:51:40,434
并且你只需要把它提供给
你的视见区尺寸

838
00:51:40,501 --> 00:51:44,938
以及你的界面方向
然后你就会获得一个相应的转换

839
00:51:45,472 --> 00:51:48,375
在我们的Metal示例中
我们用了与这种转换相反的功能

840
00:51:48,442 --> 00:51:51,845
来调节相机背景的纹理坐标

841
00:51:53,514 --> 00:51:57,784
与之相配的是投影矩阵方差

842
00:51:57,851 --> 00:52:00,254
考虑用户界面方向

843
00:52:00,320 --> 00:52:01,889
以及视见区尺寸

844
00:52:02,356 --> 00:52:05,058
那么你要传递这些
和剪裁平面极限范围

845
00:52:05,125 --> 00:52:09,830
并且你可以使用这个投影矩阵
来正确地绘制我们的…

846
00:52:10,264 --> 00:52:13,100
在相机图像顶部绘制你的虚拟内容

847
00:52:15,869 --> 00:52:17,104
那么这就是ARKit

848
00:52:18,939 --> 00:52:22,409
总结一下
ARKit是一个高层级API

849
00:52:22,809 --> 00:52:26,380
用于在iOS上创建增强现实应用

850
00:52:27,948 --> 00:52:29,883
我们提供了世界追踪

851
00:52:29,950 --> 00:52:34,321
它会给你提供你设备
相对于起点的相对位置

852
00:52:35,756 --> 00:52:38,292
为了把对象放置到现实世界中

853
00:52:38,358 --> 00:52:40,260
我们提供了场景理解

854
00:52:41,295 --> 00:52:43,697
场景理解给你提供了平面探测

855
00:52:43,764 --> 00:52:46,533
以及碰撞测试现实世界的能力

856
00:52:46,600 --> 00:52:48,302
以便找到3D坐标

857
00:52:48,368 --> 00:52:49,603
并在那里放置对象

858
00:52:50,671 --> 00:52:54,274
为了改善我们增强内容的现实性

859
00:52:54,341 --> 00:52:57,744
我们还根据相机图像提供了光估计

860
00:52:59,446 --> 00:53:02,850
我们提供了自定义整合到
SceneKit和SpriteKit中

861
00:53:02,916 --> 00:53:06,186
以及一个Metal模板 如果你想

862
00:53:06,687 --> 00:53:09,256
把ARKit整合到你自己的
渲染引擎中的话

863
00:53:11,458 --> 00:53:14,728
你可以在我们的网站上找到
关于我们演讲的更多信息

864
00:53:15,963 --> 00:53:19,933
并且有一些
来自SceneKit团队的相关演讲

865
00:53:20,000 --> 00:53:25,272
他们也会讲如何
通过ARKit和SceneKit使用动态阴影

866
00:53:25,706 --> 00:53:28,609
还有一场来自SpriteKit团队的演讲
他们主要

867
00:53:28,675 --> 00:53:32,813
讲通过SpriteKit使用ARKit

868
00:53:33,180 --> 00:53:36,517
那么我们非常激动地
把这些交到你们的手中

869
00:53:36,583 --> 00:53:38,919
并且我们很期待看到

870
00:53:38,986 --> 00:53:41,522
你们即将用它创建出来的第一批应用

871
00:53:41,588 --> 00:53:43,624
请从我们的网站上下载示例代码

872
00:53:43,690 --> 00:53:45,826
和示例应用

873
00:53:45,893 --> 00:53:49,463
把你的内容加进去 看看效果如何

874
00:53:49,530 --> 00:53:51,565
祝玩得高兴

875
00:53:52,733 --> 00:53:53,567
谢谢

