1
00:00:20,087 --> 00:00:23,257
大家早上好
欢迎参加WWDC周五的演讲

2
00:00:24,057 --> 00:00:24,925
谢谢大家

3
00:00:26,360 --> 00:00:27,594
我是Erik Turnquist

4
00:00:27,761 --> 00:00:31,231
今天我和Brad将跟大家
讲讲HEIF和HEVC的使用

5
00:00:32,533 --> 00:00:34,601
首先 什么是HEVC？

6
00:00:35,102 --> 00:00:37,838
HEVC的全称是高效率视频编码

7
00:00:38,138 --> 00:00:42,109
它是符合行业标准的
下一代视频编码技术

8
00:00:42,476 --> 00:00:44,611
它是H.264的继任者

9
00:00:46,213 --> 00:00:49,249
现在还有一个更重要的问题
就是为什么？

10
00:00:49,316 --> 00:00:52,286
Apple为什么如此努力地
提交一个新编解码器？

11
00:00:52,619 --> 00:00:55,589
在过去的十几年中H.264非常棒

12
00:00:56,456 --> 00:00:59,459
现在我们反复思考这个问题
我们真的很想启动一些新功能

13
00:00:59,526 --> 00:01:02,829
很遗憾H.264已达到其功能的上限

14
00:01:03,664 --> 00:01:06,900
我们想启用新功能
比如4K以及更大的帧尺寸

15
00:01:07,234 --> 00:01:11,104
高位深如10-位元
以及更宽的色彩空间如Rec.2020

16
00:01:11,672 --> 00:01:15,275
现在我们想全部实现 但要降低位速率
而不会增加位速率

17
00:01:15,542 --> 00:01:16,643
那么如何实现呢？

18
00:01:17,611 --> 00:01:19,112
嗯 我们用HEVC实现

19
00:01:19,680 --> 00:01:22,683
那么现在 比我们现在实际
看到的位速率低多少呢？

20
00:01:23,350 --> 00:01:24,818
嗯 对于一般性编码内容

21
00:01:24,885 --> 00:01:28,088
我们看到比H.264的位速率
降低了多达40%

22
00:01:28,255 --> 00:01:29,790
那么这非常了不起

23
00:01:30,224 --> 00:01:31,358
对于摄像头捕捉

24
00:01:31,425 --> 00:01:36,730
我们看到的压缩
与H.264和JPEG相比好2倍

25
00:01:37,397 --> 00:01:38,999
那么这是另一件很了不起的事

26
00:01:39,766 --> 00:01:42,069
而且我们今天实现了全部这些变更

27
00:01:42,603 --> 00:01:45,072
如果你已经安装了iOS
iOS 11种子

28
00:01:45,138 --> 00:01:49,176
我们已默认启动了HEVC电影
和HEIF图片捕捉

29
00:01:49,243 --> 00:01:50,711
那意味着你们中的许多人已捕捉到了

30
00:01:50,777 --> 00:01:53,714
HEIF图片或HEVC电影
而你们自己甚至都不知道

31
00:01:54,047 --> 00:01:55,782
在我们平台上就这么管用了

32
00:01:56,884 --> 00:01:58,752
让我们来看看我们今天要讲的内容

33
00:01:58,819 --> 00:02:00,921
我要讲HEVC电影相关的内容

34
00:02:00,988 --> 00:02:03,423
而Brad将谈谈HEIF图片相关内容

35
00:02:03,490 --> 00:02:05,559
我们要讲获取这些内容

36
00:02:06,026 --> 00:02:08,362
播放并显示它 捕捉它

37
00:02:08,695 --> 00:02:12,299
并创建HEIF和HEVC电影
然后导出并转码

38
00:02:13,267 --> 00:02:14,801
首先 让我们先讲讲权限

39
00:02:16,470 --> 00:02:18,338
你们中有那么多人使用PhotoKit

40
00:02:18,505 --> 00:02:20,941
PhotoKit将给播放提交HEVC资产

41
00:02:21,642 --> 00:02:23,710
所以如果你使用
requestPlayerItem

42
00:02:23,777 --> 00:02:24,978
或requestLivePhoto

43
00:02:25,345 --> 00:02:27,581
它们将提交或它们将为你提供自动播放

44
00:02:27,648 --> 00:02:31,185
而无需采用新的…
任何新的API 就这样管用了

45
00:02:34,121 --> 00:02:36,723
PhotoKit还可以给你提交HEVC资产

46
00:02:36,790 --> 00:02:38,825
所以如果你称它为
requestExportSession

47
00:02:38,892 --> 00:02:42,162
它将转码到你当前正在使用的预设

48
00:02:42,696 --> 00:02:44,298
所以如果你使用其中一个尺寸预设

49
00:02:44,364 --> 00:02:47,534
是过去经常给你提供H.264的
那么它仍会那么做

50
00:02:47,601 --> 00:02:50,070
但我会讲一下
我们为HEVC增加的新预设

51
00:02:50,604 --> 00:02:53,040
如果你调用requestAVAsset
它将给你提供

52
00:02:53,106 --> 00:02:56,844
HEVC媒体文件的权限
这将在其内部有一个HEVC视频轨道

53
00:02:58,979 --> 00:03:02,216
现在如果你要备份应用
你想获取原始位元

54
00:03:02,282 --> 00:03:04,318
那么你很可能给它命名为requestData

55
00:03:04,384 --> 00:03:07,821
那么我想提醒大家注意的是
这将包含HEVC视频轨道

56
00:03:07,888 --> 00:03:11,358
在你接收的电影文件内
所以你需要能处理这个问题

57
00:03:12,492 --> 00:03:15,863
现在你有了这个内容
让我们谈谈播放和显示

58
00:03:17,464 --> 00:03:20,467
我们的现代化媒体框架支持HEVC播放

59
00:03:20,534 --> 00:03:23,537
比如AVKit、AVFoundation
和VideoToolbox

60
00:03:23,937 --> 00:03:25,873
我们支持HTTP实时流媒体

61
00:03:26,240 --> 00:03:29,776
边下边播以及本地文件播放
或本地文件回放

62
00:03:30,511 --> 00:03:33,747
并且我们支持MPEG-4
和QuickTime文件格式作为源

63
00:03:33,814 --> 00:03:36,917
并且在这里不需要选择加入任何API
就那么管用了

64
00:03:38,652 --> 00:03:40,954
我们支持在macOS和iOS上的解码

65
00:03:41,421 --> 00:03:44,291
现在让我们看看哪里支持硬件解码

66
00:03:44,658 --> 00:03:47,060
那么我们在A9芯片上
有8位元和10位元解码器

67
00:03:47,294 --> 00:03:48,795
这是iPhone 6s

68
00:03:49,229 --> 00:03:52,633
并且我们在第6代因特尔内核上
有8位元硬件解码

69
00:03:52,699 --> 00:03:55,569
这是Skylake
这是带触摸条的MacBook Pro

70
00:03:55,636 --> 00:03:56,970
我们还有10位元解码

71
00:03:57,037 --> 00:03:59,373
在第7代因特尔内核处理器上

72
00:03:59,640 --> 00:04:02,843
这是Kaby Lake
这是全新的带触摸条的MacBook Pro

73
00:04:02,910 --> 00:04:07,414
我们在macOS和iOS上还有
8位元和10位元软件解码备用解码器

74
00:04:08,448 --> 00:04:09,950
所以现在让我们来看一些代码

75
00:04:10,017 --> 00:04:11,919
让我们将其转换到HEVC播放

76
00:04:12,386 --> 00:04:14,354
在此我们正播放“我的了不起的电影”

77
00:04:14,421 --> 00:04:16,990
做一个URL 然后一个播放器
然后播放它

78
00:04:17,423 --> 00:04:21,428
那么这就是H.264版本
现在这里是HEVC版本

79
00:04:22,229 --> 00:04:25,299
没有任何改变
那么要播放HEVC电影文件

80
00:04:25,499 --> 00:04:27,201
你不需要修改任何代码

81
00:04:27,267 --> 00:04:29,536
我们想让你考虑一些东西

82
00:04:30,671 --> 00:04:32,739
那么第一是关于解码功能

83
00:04:33,006 --> 00:04:34,208
如果你提出这个问题

84
00:04:34,274 --> 00:04:37,377
系统上有解码器能处理这个内容吗

85
00:04:37,444 --> 00:04:38,579
你可以用这个API

86
00:04:39,112 --> 00:04:42,950
这个可用于非即时操作
比如共享或图像生成

87
00:04:43,016 --> 00:04:45,185
并且它受硬件支持的限制

88
00:04:45,252 --> 00:04:48,322
所以并不是全部硬件解码器
支持每种帧尺寸

89
00:04:49,656 --> 00:04:52,860
现在还有一个更重要的问题
就是关于播放功能

90
00:04:53,093 --> 00:04:56,430
如果你询问如何为客户
打造最好的播放体验

91
00:04:56,496 --> 00:04:57,764
你可以使用这个API

92
00:04:57,831 --> 00:05:00,367
且你们中很多人
很可能已经开始使用这个API了

93
00:05:01,001 --> 00:05:03,437
并不是所有内容都能实时播放

94
00:05:03,737 --> 00:05:06,373
并且针对不同的设备有不同的功能

95
00:05:06,440 --> 00:05:10,177
所以如果你想为用户
提供最好的一站式播放体验

96
00:05:10,244 --> 00:05:14,181
无论是1x或2x播放
后退、拖动或快进

97
00:05:14,481 --> 00:05:15,749
你可以使用这个API

98
00:05:17,818 --> 00:05:20,320
现在让我们接着讲硬件解码功能

99
00:05:20,387 --> 00:05:23,190
如果你想在播放过程中省电

100
00:05:23,257 --> 00:05:26,426
你想在支持硬件解码的系统上播放

101
00:05:26,960 --> 00:05:29,530
这也会给你最好的解码性能

102
00:05:29,863 --> 00:05:32,399
所以我们有一个新的VideoToolbox API
你可以查询

103
00:05:32,799 --> 00:05:35,435
这个编解码器是否支持硬件解码？

104
00:05:35,502 --> 00:05:36,803
在这里我要展示的是HEVC

105
00:05:36,870 --> 00:05:39,006
但你仍然可以
在其它编解码器中使用

106
00:05:40,741 --> 00:05:44,511
现在对于播放有最后一个问题
我应该用哪种编解码器来播放？

107
00:05:45,112 --> 00:05:48,849
我要选择H.264或HEVC？

108
00:05:49,550 --> 00:05:52,953
嗯 如果你担心提交最兼容的内容

109
00:05:53,020 --> 00:05:55,789
或想提交一个能在各处适用的资产

110
00:05:56,623 --> 00:05:58,125
请选择H.264

111
00:05:58,559 --> 00:06:01,161
我们的平台已经
支持这种格式长达10多年了

112
00:06:01,228 --> 00:06:04,965
在第三方生态系统中被广泛采用

113
00:06:05,666 --> 00:06:08,135
然而如果你想获得最小的文件尺寸

114
00:06:08,202 --> 00:06:10,938
和最新、最强的编码技术
如10位元

115
00:06:11,004 --> 00:06:12,172
请选择HEVC

116
00:06:12,472 --> 00:06:15,008
你需要决定哪个适用于你的应用

117
00:06:16,543 --> 00:06:18,512
接着让我们来看捕捉

118
00:06:19,813 --> 00:06:24,484
那么捕捉HEVC支持AVFoundation
并且我们支持MPEG-4

119
00:06:24,551 --> 00:06:26,820
以及QuickTime文件格式作为目的文件

120
00:06:28,655 --> 00:06:33,527
我们在A10芯片上支持HEVC捕捉
那么也就是iPhone 7

121
00:06:33,961 --> 00:06:35,696
现在让我们来看一下…

122
00:06:35,896 --> 00:06:38,832
你们中许多人都已经很熟悉的捕捉图表

123
00:06:39,566 --> 00:06:41,502
（捕捉HEVC电影）

124
00:06:41,568 --> 00:06:45,272
一开始是一个AVCaptureSession
需要从某个地方获取数据

125
00:06:47,975 --> 00:06:50,644
你创建一个AVCaptureDevice
并将其添加为输入

126
00:06:51,778 --> 00:06:53,146
然后数据需要进入某处

127
00:06:53,213 --> 00:06:56,116
在本例中 你使用电影文件
来压缩和写输出文件

128
00:06:57,518 --> 00:07:00,020
这些都与AVCaptureConnection相关联

129
00:07:00,687 --> 00:07:04,024
并且这将创建你的电影文件
那么让我们把这个转化为代码

130
00:07:04,925 --> 00:07:06,894
你们中的许多人很可能
在你们的应用中采纳了这个

131
00:07:06,960 --> 00:07:09,830
首先创建一个AVCaptureConnect
或AVCaptureSession

132
00:07:10,130 --> 00:07:12,399
在这里我们做了一个4k捕捉会话

133
00:07:13,901 --> 00:07:16,570
然后你创建AVCaptureDevice
并将其添加为输入

134
00:07:18,739 --> 00:07:20,073
创建你的MovieFileOutput

135
00:07:20,140 --> 00:07:23,210
并且这会实现写压缩文件
并将其添加为输出

136
00:07:24,211 --> 00:07:27,414
然后startRunning和startRecording
然后我们就进行捕捉

137
00:07:27,481 --> 00:07:28,949
那么我们要如何选择使用HEVC呢？

138
00:07:30,817 --> 00:07:32,586
嗯 在iOS 10中 我们添加了API

139
00:07:32,786 --> 00:07:35,422
在捕捉过程中检测可用视频编解码器

140
00:07:35,689 --> 00:07:38,926
在iOS 11中有一个新功能
可检测是否包含HEVC

141
00:07:39,593 --> 00:07:42,162
在所支持的设备上 它将返回真

142
00:07:42,229 --> 00:07:46,333
并且你可以继续并在输出设置中使用

143
00:07:47,534 --> 00:07:49,469
且如果设备部支持它
你可以继续并退到

144
00:07:49,536 --> 00:07:51,205
其它编解码器 如H.264

145
00:07:51,872 --> 00:07:53,740
现在我想在这里提示一个重点

146
00:07:53,807 --> 00:07:56,109
那个顺序是否
与availableVideoCodecTypes有关

147
00:07:56,176 --> 00:07:58,478
并且为此我们将HEVC
作为了第一选项

148
00:07:58,912 --> 00:08:02,783
那么意思是如果你什么也不做
你将捕捉HEVC内容

149
00:08:03,183 --> 00:08:05,786
我们真的希望能让你熟悉
对这种内容的处理

150
00:08:06,753 --> 00:08:08,422
现在让我们来看Live Photos

151
00:08:09,656 --> 00:08:13,594
那么我们在这里有同样的捕捉图表
但我们要使用AVCapturePhotoOutput

152
00:08:13,827 --> 00:08:16,230
那会拍出
我们喜欢和欣赏的Live Photos

153
00:08:16,563 --> 00:08:19,333
那么首先让我们
看一些Live Photo的新改进

154
00:08:19,399 --> 00:08:20,667
我们去年完成的

155
00:08:21,702 --> 00:08:23,403
我们现在支持视频稳定性

156
00:08:23,470 --> 00:08:25,639
所以在Live Photo播放过程中
不再有摇晃的情况

157
00:08:26,006 --> 00:08:29,243
在捕捉Live Photo的过程中
也不再暂停音乐播放了

158
00:08:29,543 --> 00:08:33,046
并且我们支持更流畅的Live Photo
最高可达每秒30帧

159
00:08:34,248 --> 00:08:37,017
那么让我们通过Live Photo
看一下HEVC的捕捉

160
00:08:38,217 --> 00:08:40,087
那么在iOS 11中有新API

161
00:08:40,153 --> 00:08:42,823
你可以创建
availableLivePhotoVideo CodecTypes

162
00:08:43,890 --> 00:08:48,228
看它是否包含HEVC
在所支持的设备上它将返回真

163
00:08:49,730 --> 00:08:51,064
然后如果它支持 继续并使用它

164
00:08:51,131 --> 00:08:54,168
如果不支持 你可以退回到另一个
现有的编解码器 如H.264

165
00:08:54,935 --> 00:08:58,038
我还想提示一点 这里有同样的顾虑

166
00:08:58,105 --> 00:09:00,374
即那个顺序
与availableVideoCodecTypes有关

167
00:09:00,440 --> 00:09:02,910
为此我们将HEVC作为第一选项

168
00:09:03,677 --> 00:09:07,481
如果你什么也不做
你将捕捉HEVC Live Photo

169
00:09:08,315 --> 00:09:09,783
你可能在这里感知到了一种模式

170
00:09:09,850 --> 00:09:12,653
我们真的很想
让你熟悉对这种内容的处理

171
00:09:14,855 --> 00:09:17,491
现在让我们来看定制性最强的捕捉图表

172
00:09:17,891 --> 00:09:21,428
有AVCaptureVideoDataOutput
和AVAssetWriter

173
00:09:21,995 --> 00:09:24,932
如果你想在某种程度上
修改样本缓冲 请使用这个

174
00:09:24,998 --> 00:09:27,634
也许你可能会实施
一些很酷的筛选操作

175
00:09:29,169 --> 00:09:32,406
通过为HEVC配置AssetWriter
你有两个选项

176
00:09:32,940 --> 00:09:35,375
那么你可以配置自定义输出设置

177
00:09:35,442 --> 00:09:37,344
明确指定HEVC

178
00:09:37,678 --> 00:09:40,380
或视频数据输出会给你推荐那些设置

179
00:09:40,447 --> 00:09:42,082
我们推荐这个API

180
00:09:42,749 --> 00:09:43,984
iOS 7添加了

181
00:09:44,051 --> 00:09:46,153
recommendedVideoSettings
ForAssetWriter

182
00:09:46,220 --> 00:09:48,121
现在这个总是推荐H.264

183
00:09:48,322 --> 00:09:50,657
所以如果你要坚持使用H.264
也没问题

184
00:09:50,858 --> 00:09:53,994
然而在iOS 11中
我们添加了新API 你们可以

185
00:09:54,061 --> 00:09:55,462
传入编解码器类型

186
00:09:55,529 --> 00:09:59,266
我们将在所支持的设备上针对
该编解码器类型给你提供推荐设置

187
00:10:00,200 --> 00:10:03,270
接着让我们来看导出和转码相关的内容

188
00:10:07,274 --> 00:10:11,345
那么你可以通过AVFoundation
和VideoToolbox转码为HEVC

189
00:10:11,712 --> 00:10:15,315
并且我们支持MPEG-4和QuickTime
文件格式作为目的文件

190
00:10:15,716 --> 00:10:17,618
在这里需要选择加入API

191
00:10:20,320 --> 00:10:22,422
在macOS和iOS上我们支持HEVC编码

192
00:10:22,489 --> 00:10:25,759
现在让我们来看一下
哪些支持HEVC硬件编码

193
00:10:27,327 --> 00:10:30,264
那么我们在A10融合芯片中
有一个8-位元硬件编码器

194
00:10:30,330 --> 00:10:31,565
这是iPhone 7

195
00:10:32,032 --> 00:10:34,168
我们在macOS上
支持8-位元硬件编码

196
00:10:34,234 --> 00:10:36,537
在第6代因特尔处理器上

197
00:10:36,837 --> 00:10:40,174
这是Skylake系列
这是带触摸条的MacBook Pro

198
00:10:40,541 --> 00:10:44,178
在macOS上 我们有一个特别的
10-位元非即时时间

199
00:10:44,244 --> 00:10:46,346
高质量软件编码器 你可以使用

200
00:10:46,413 --> 00:10:48,115
我们稍后再讲一下这个

201
00:10:49,082 --> 00:10:51,985
现在让我们先看最高层级的导出API

202
00:10:52,786 --> 00:10:55,255
即通过AVAssetExportSession转码

203
00:10:56,323 --> 00:10:57,991
那么通过这个 你可以提供一个资产

204
00:10:58,058 --> 00:11:00,360
然后你选择一个预设
我们将会替你完成全部操作

205
00:11:00,427 --> 00:11:03,497
包括压缩并且生产一个输出电影

206
00:11:05,499 --> 00:11:08,068
那么对于现有预设来说
行为并没有任何变更

207
00:11:08,135 --> 00:11:10,604
如果你正在使用
现有的基于尺寸的预设

208
00:11:10,671 --> 00:11:14,041
它通常是提供H.264的
它仍然会这样做

209
00:11:14,474 --> 00:11:16,009
我们已经在这里添加了新预设

210
00:11:16,510 --> 00:11:20,714
那些将会从H.264
或任何其它编解码器转为HEVC

211
00:11:21,148 --> 00:11:23,116
并且这些将生成更小尺寸的AVAssets

212
00:11:23,350 --> 00:11:26,286
有时候最高可达40% 质量不变

213
00:11:27,788 --> 00:11:31,592
现在让我们把级别降低一级
看通过AVAssetWriter进行压缩的堆栈

214
00:11:33,827 --> 00:11:37,264
那么AVAssetWriter
你要么自己生成样本缓冲

215
00:11:37,331 --> 00:11:39,533
要么从另一个API中得到样本缓冲

216
00:11:39,600 --> 00:11:41,668
比如VideoDataOutput
或AVAssetWriter

217
00:11:41,869 --> 00:11:44,972
并且AVAssetWriter用于压缩和写文件

218
00:11:46,039 --> 00:11:49,510
正如我之前所说过的
对于AVAssetWriter有两个选项

219
00:11:49,576 --> 00:11:52,179
你可以明确设置自定义输出设置

220
00:11:52,246 --> 00:11:55,315
在本例中我们明确使用HEVC

221
00:11:55,382 --> 00:11:57,818
你还可以指定你的位速率和尺寸

222
00:11:57,885 --> 00:12:00,921
或你可以使用我们的便利设置
在捕捉过程中

223
00:12:00,988 --> 00:12:02,189
你可以使用VideoDataOutput

224
00:12:02,256 --> 00:12:05,058
对于一般性编码 你可以使用
AVOutputSettingsAssistant

225
00:12:05,125 --> 00:12:06,727
我们在这里添加了两个新预设

226
00:12:06,793 --> 00:12:09,930
在所支持的设备上
将返回HEVC输出设置

227
00:12:11,064 --> 00:12:14,401
现在如果你正在创建
你自己的自定义输出设置

228
00:12:14,468 --> 00:12:15,802
可能会很棘手

229
00:12:15,869 --> 00:12:18,272
那么并不是所有的编码器
都支持全部的输出设置

230
00:12:19,006 --> 00:12:21,742
我们在iOS 11和macOS High Sierra中
修复了这个问题

231
00:12:21,808 --> 00:12:23,610
所以现在你可以向编码器
查询所支持的属性

232
00:12:23,677 --> 00:12:25,312
用于输出设置

233
00:12:26,079 --> 00:12:28,148
为此你要在这里传入HEVC

234
00:12:28,215 --> 00:12:31,618
它将返回编码器ID
以及一份所支持属性的列表

235
00:12:32,152 --> 00:12:35,656
编码器ID是那个编码器的唯一标识符

236
00:12:36,356 --> 00:12:39,660
然后就能在输出设置中
指定属性和编码器ID

237
00:12:39,726 --> 00:12:42,896
你就可以确保它可以用于压缩

238
00:12:43,397 --> 00:12:46,066
现在让我们来看最低层级的压缩界面

239
00:12:46,466 --> 00:12:49,203
这是通过
VTCompressionSession压缩的样本

240
00:12:49,670 --> 00:12:50,871
正如AssetWriter一样

241
00:12:50,938 --> 00:12:52,573
你可能会自己生成样本

242
00:12:52,639 --> 00:12:55,175
或从另一个API中获取

243
00:12:55,242 --> 00:12:59,580
VTCompressionsSession压缩它们
并生产已压缩的媒体数据

244
00:13:01,682 --> 00:13:05,786
那么要通过HEVC编码器
创建一个压缩会话非常简单

245
00:13:05,853 --> 00:13:09,456
在本例中 我们创建了一个
压缩成了H.264

246
00:13:09,723 --> 00:13:11,758
让我们把它转为HEVC

247
00:13:12,659 --> 00:13:14,761
好了 现在我们正在用HEVC压缩

248
00:13:14,828 --> 00:13:16,930
通过VideoToolbox 所以这非常简单

249
00:13:17,664 --> 00:13:20,334
现在让我们看一些
macOS上的注意事项

250
00:13:21,668 --> 00:13:23,737
对于macOS上的最佳编码性能

251
00:13:23,804 --> 00:13:25,439
你想选择加入到硬件中的

252
00:13:25,506 --> 00:13:29,343
这将使用硬件 当可用时
如果不可用 就退回使用软件

253
00:13:29,776 --> 00:13:30,878
要实现这个目的

254
00:13:31,845 --> 00:13:35,048
设置EnableHardwareAccelerated
VideoEncoder属性…

255
00:13:35,115 --> 00:13:37,551
在encoderSpecification中属性设为真

256
00:13:38,819 --> 00:13:41,154
然后将其传入
VTCompressionSessionCreate中

257
00:13:43,790 --> 00:13:47,361
现在如果你正在做即时编码
你总会需要硬件

258
00:13:47,427 --> 00:13:49,196
并且永远不会退回去使用软件

259
00:13:49,530 --> 00:13:52,199
为此你要把encoderSpecification

260
00:13:52,266 --> 00:13:54,801
RequireHardwareAccelarated
VideoEncoder设为真

261
00:13:56,303 --> 00:13:58,305
然后把它传入encoderSpecification中

262
00:13:58,372 --> 00:14:01,375
在支持硬件的系统上
这将会成功

263
00:14:01,441 --> 00:14:04,845
但系统上的硬件如果只支持软件编码
将会导致失败

264
00:14:05,913 --> 00:14:09,049
好了 现在让我们继续看一些
高级编码相关的话题

265
00:14:09,116 --> 00:14:10,450
第一个是位深

266
00:14:11,185 --> 00:14:13,854
那么如果你从曾在用户界面中
看到过一个漂亮的渐变

267
00:14:13,921 --> 00:14:17,357
或者漂亮的日出或日落
你会注意到它在现实生活中看起来

268
00:14:17,558 --> 00:14:20,661
与电影中并不是完全一样的

269
00:14:21,261 --> 00:14:23,130
所以你可能会看到这些色彩限制效果

270
00:14:23,197 --> 00:14:25,966
在你电影的视频版中

271
00:14:26,533 --> 00:14:28,769
这是因为通过8-位元
我们没有足够的精确度

272
00:14:28,836 --> 00:14:31,471
来呈现色彩之间微妙的不同点

273
00:14:31,538 --> 00:14:33,774
现在好消息是10-位元可以实现了

274
00:14:33,841 --> 00:14:36,076
所以你会获得这些非常漂亮的渐变

275
00:14:36,777 --> 00:14:40,180
现在通过我们的macOS软件编码器
我们实际上可以支持10-位元编码

276
00:14:40,914 --> 00:14:43,183
那么首先查看是否支持该属性

277
00:14:43,250 --> 00:14:47,521
如果支持 继续并使用我们的
HEVC Main10为软件编码器

278
00:14:48,488 --> 00:14:50,791
并且我们想确保你的整个管道
都是10-位元

279
00:14:50,858 --> 00:14:54,228
我们不想让你从8-位元变为10-位元
然后再变为8-位元

280
00:14:54,294 --> 00:14:55,596
因为那会降低精确度

281
00:14:55,662 --> 00:14:57,764
所以我们添加了
新CoreVideo像素缓冲格式

282
00:14:57,831 --> 00:14:59,700
以确保你可以保持在10-位元中

283
00:14:59,867 --> 00:15:00,834
这里列出了一个

284
00:15:01,268 --> 00:15:03,504
那么现在 你第一次可以
以10-位元进行渲染

285
00:15:03,570 --> 00:15:05,839
以10-位元进行编码
以10-位元进行解码

286
00:15:06,006 --> 00:15:07,941
而且在iOS和macOS上有史以来首次

287
00:15:08,008 --> 00:15:11,378
我们的显示屏管道也支持10-位元
所以我们把它应用到了极致

288
00:15:15,916 --> 00:15:17,584
现在让我们看看第二个高级话题

289
00:15:17,651 --> 00:15:19,286
即阶梯式编码

290
00:15:19,987 --> 00:15:21,588
要了解这个信息

291
00:15:21,655 --> 00:15:24,291
我们需要稍微看一下视频编码101

292
00:15:25,125 --> 00:15:27,327
视频压缩有三个主要的帧类型

293
00:15:27,394 --> 00:15:28,795
第一个是I Frame

294
00:15:29,229 --> 00:15:30,864
你可以把I Frame看做是一个图片文件

295
00:15:30,931 --> 00:15:32,966
可以被独立解码

296
00:15:34,001 --> 00:15:35,469
然后有P Frame

297
00:15:35,536 --> 00:15:37,604
P Frame指的是之前的帧

298
00:15:37,771 --> 00:15:39,273
所以可把它们看作是一个单差

299
00:15:39,339 --> 00:15:41,942
并且它们只包含
不存在于之前帧中的信息

300
00:15:42,442 --> 00:15:44,278
现在还有它们的堂弟B Frame

301
00:15:44,745 --> 00:15:46,980
B Frame指的是之前和将来的帧

302
00:15:47,047 --> 00:15:49,383
它们就像一个奇特的多方向差

303
00:15:49,449 --> 00:15:51,685
所以它们只包含不存在于

304
00:15:51,752 --> 00:15:53,353
引用它们的帧中的信息

305
00:15:53,520 --> 00:15:57,257
现在假装我们有一个解码器
每秒只能处理30帧

306
00:15:57,324 --> 00:16:00,060
假如我们有一个每秒240帧的内容

307
00:16:00,561 --> 00:16:02,296
嗯 那意味着我们需要失掉一些帧

308
00:16:02,362 --> 00:16:04,698
在我们解码之前 因为它跟不上

309
00:16:04,765 --> 00:16:06,066
我们何时可以掉帧呢？

310
00:16:07,000 --> 00:16:09,870
我们可以当另一帧
不依赖于这一帧时掉帧

311
00:16:10,137 --> 00:16:12,105
那么在本例中
我们失掉最后一个P Frame

312
00:16:12,172 --> 00:16:15,475
因为它指的是另一帧
而没有帧指向它

313
00:16:15,876 --> 00:16:17,511
那么让我们继续并失掉它

314
00:16:17,578 --> 00:16:18,946
我们也可以失掉B Frame

315
00:16:19,012 --> 00:16:22,049
因为它指的是其它帧
而没有帧指向它

316
00:16:22,583 --> 00:16:24,084
那么让我们继续并失掉它

317
00:16:24,451 --> 00:16:28,522
现在让我们看一个实际的例子
一个每秒编码240帧的内容

318
00:16:29,823 --> 00:16:31,358
那么这是一个典型的编码机制

319
00:16:31,425 --> 00:16:34,394
用于当通过低端设备创建可兼容内容时

320
00:16:35,262 --> 00:16:37,931
那么比如说
当编码每秒240帧的内容时

321
00:16:37,998 --> 00:16:41,068
每八帧就可以失掉七帧 留下一帧

322
00:16:41,401 --> 00:16:43,971
这就在播放时
给我们提供了许多灵活性

323
00:16:44,037 --> 00:16:47,875
在支持每秒120帧解码的设备上
我们可以处理那个

324
00:16:47,941 --> 00:16:50,811
在只支持每秒30帧的设备上
我们也可以播放

325
00:16:51,678 --> 00:16:53,680
现在让我们添加我们的帧引用

326
00:16:53,747 --> 00:16:56,083
因为这些帧是可失掉的
它们不能互相指向

327
00:16:56,149 --> 00:16:58,452
并且它们全部指向不可失掉的帧

328
00:16:58,752 --> 00:17:00,387
现在你们中有压缩经验的人

329
00:17:00,454 --> 00:17:02,256
应该已经发现了一个问题

330
00:17:02,689 --> 00:17:06,292
那个压缩很别扭是否
因为我们不能指向临近的帧？

331
00:17:06,359 --> 00:17:08,228
那么它们全部指向不可失掉的帧

332
00:17:08,295 --> 00:17:09,262
可能发生了很多变更

333
00:17:09,329 --> 00:17:11,832
在不可失掉的帧和可失掉的帧之间

334
00:17:12,465 --> 00:17:14,568
好了 那么这是我们
要修复的第一个问题

335
00:17:14,635 --> 00:17:17,671
现在让我们逐步来看
解码将至每秒30帧

336
00:17:18,005 --> 00:17:20,941
那么首先
假如我们不能处理每秒240帧

337
00:17:21,608 --> 00:17:23,143
让我们继续并失掉一些帧

338
00:17:23,210 --> 00:17:25,512
那么在这里 我们要降到每秒240帧

339
00:17:25,579 --> 00:17:29,550
假如我们仍然跟不上
我们需要降到每秒60帧

340
00:17:29,616 --> 00:17:31,985
假如我们的解码器
每秒只能处理30帧

341
00:17:32,052 --> 00:17:34,288
我们甚至每秒都不能处理60帧

342
00:17:34,621 --> 00:17:36,657
那么我们继续并失掉这个最后的帧

343
00:17:37,424 --> 00:17:40,194
现在 我真的要猜猜要失掉哪些帧

344
00:17:40,260 --> 00:17:43,564
那么完全没有任何指示
关于我是否应该每两帧就失掉一帧

345
00:17:43,630 --> 00:17:45,966
要么是前一帧 要么是后一帧

346
00:17:46,366 --> 00:17:47,801
所以让我们也来修复一下这个问题

347
00:17:49,169 --> 00:17:52,005
我们可以通过一个叫作
临时等级的概念来修复

348
00:17:52,072 --> 00:17:55,843
这允许我们管理帧
关于要先失掉哪些帧

349
00:17:56,210 --> 00:17:58,445
让我们继续并重新编码我们的内容

350
00:18:00,848 --> 00:18:03,450
你已经看到了 这种方式更有条理

351
00:18:03,517 --> 00:18:05,686
首先 我们把临时等级降到三

352
00:18:05,752 --> 00:18:08,222
然后二 然后一 其中不需要任何猜测

353
00:18:08,288 --> 00:18:09,389
所以这个很有帮助

354
00:18:09,656 --> 00:18:11,658
现在让我们进行帧引用

355
00:18:12,593 --> 00:18:14,661
你已经可以看到
这里有一个很大的不同点

356
00:18:14,728 --> 00:18:16,997
引用帧是否更紧密了

357
00:18:17,064 --> 00:18:20,634
它们经常指向它们的上一帧或下一帧

358
00:18:20,701 --> 00:18:22,269
所以这真的改善了压缩

359
00:18:22,936 --> 00:18:25,305
现在让我们逐步看一下
假如我们有同样的解码器

360
00:18:25,372 --> 00:18:28,809
每秒只能处理30帧
我们需要失掉一些帧

361
00:18:29,076 --> 00:18:32,379
嗯 其中不需要任何猜测
我们将临时等级降为三

362
00:18:33,213 --> 00:18:34,982
现在我们降到了每秒120帧

363
00:18:35,582 --> 00:18:39,086
让我们降到等级一
现在我们每秒60帧

364
00:18:40,087 --> 00:18:43,323
现在等级是我们的解码器
实际上可以处理的等级

365
00:18:43,390 --> 00:18:46,260
所以这就不需要猜到底要失掉哪些帧

366
00:18:47,594 --> 00:18:48,996
让我们回顾所学到的内容

367
00:18:49,062 --> 00:18:52,966
那么通过HEVC阶梯式编码
我们改进了临时可扩展性

368
00:18:53,333 --> 00:18:55,769
有更明显的掉帧模式

369
00:18:55,836 --> 00:18:58,272
并且它移除了需要失掉哪些帧的猜测

370
00:18:58,772 --> 00:19:01,074
我们还改进了动作补偿

371
00:19:01,408 --> 00:19:03,544
引用帧之间的距离更近了

372
00:19:03,610 --> 00:19:07,281
所以我们可以更多地使用
其它帧的一部分 它还改善了压缩

373
00:19:07,347 --> 00:19:09,082
我们还使用了文件注释

374
00:19:09,149 --> 00:19:11,118
如果你们喜欢读说明书

375
00:19:11,185 --> 00:19:14,154
请参看MPEG-4 第15部分的8.4章节

376
00:19:14,621 --> 00:19:18,659
基本上 我们使用的是样本群
所以没有位元流解析为否 抱歉

377
00:19:18,725 --> 00:19:21,728
我们用的是样本群
所以不必要通过解析位元流

378
00:19:21,795 --> 00:19:24,231
来获取这个信息 所以这真的很有帮助

379
00:19:24,798 --> 00:19:26,633
好的 我们要如何选择加入这个？

380
00:19:27,267 --> 00:19:28,368
你想选择加入这个

381
00:19:28,435 --> 00:19:30,838
如果你想创建兼容的高帧率内容

382
00:19:30,904 --> 00:19:33,073
你应该设置两个属性

383
00:19:33,140 --> 00:19:35,642
你要设置基础层和拍摄帧率

384
00:19:36,143 --> 00:19:38,846
首先查看你所使用的编码器是否支持

385
00:19:38,912 --> 00:19:42,749
然后设置BaseLayerFrameRate
这是临时等级0帧率

386
00:19:42,983 --> 00:19:45,485
在我们之前的例子中
这个是每秒30帧

387
00:19:45,552 --> 00:19:47,221
然后设置ExpectedFrameRate

388
00:19:47,287 --> 00:19:50,290
在我们之前的例子中
这个是每秒240帧

389
00:19:50,858 --> 00:19:54,261
必须解码基础层
并且我们可以解码或降到其它等级

390
00:19:54,928 --> 00:19:57,564
那么现在
你们都是阶梯式编码方面的专家了

391
00:19:57,798 --> 00:20:00,367
让我们把舞台交给Brad
让他讲讲与图片有关的内容

392
00:20:00,767 --> 00:20:01,635
谢谢大家

393
00:20:07,207 --> 00:20:08,041
谢谢Erik

394
00:20:08,108 --> 00:20:10,277
我是Brad Ford
我来自摄像软件团队

395
00:20:10,644 --> 00:20:14,982
我要跟大家谈谈以HE开始的另一个
四个首字母缩写的词

396
00:20:17,050 --> 00:20:19,386
这是接下来的演讲的大纲

397
00:20:19,720 --> 00:20:21,989
首先我们要看高等级的HEIF

398
00:20:22,856 --> 00:20:24,825
我们会从最低的等级开始

399
00:20:24,892 --> 00:20:27,928
我们会讲到用HEIF读写文件

400
00:20:28,462 --> 00:20:30,197
然后我们向上进入堆栈的顶层

401
00:20:30,264 --> 00:20:34,134
谈谈如何使用一般性用例
和HEIF的常见情境

402
00:20:34,701 --> 00:20:37,371
最后我们要谈一个
对我来说最亲爱、最亲密的话题

403
00:20:37,437 --> 00:20:38,539
就是捕捉HEIF

404
00:20:39,540 --> 00:20:41,041
首先 什么是HEIF？

405
00:20:42,709 --> 00:20:46,580
HEIF是高频率图档格式

406
00:20:46,947 --> 00:20:49,216
第二个F被隐藏起来了

407
00:20:49,449 --> 00:20:50,784
你不用叫它HEIF…

408
00:20:50,984 --> 00:20:51,818
…F

409
00:20:52,052 --> 00:20:53,253
如果你这样做的话

410
00:20:53,320 --> 00:20:55,122
你只会在你最亲密的朋友们面前丢脸

411
00:20:55,722 --> 00:20:59,560
它是一种静止图像
和图像序列的现代容器格式

412
00:21:00,127 --> 00:21:03,063
它是MPEG-H Part 12规格中的一种

413
00:21:03,130 --> 00:21:06,867
为了满足你们的好奇心
它是在2013年被提议的

414
00:21:07,201 --> 00:21:11,538
然后在2015年夏季获得批准
整整用了一年半

415
00:21:12,339 --> 00:21:14,975
如果你们中有人了解标准机构

416
00:21:15,209 --> 00:21:18,178
他们的一年半就跟我们
现实生活中的两天一样

417
00:21:18,245 --> 00:21:19,947
所以你知道
它一定是了不起的规格

418
00:21:21,315 --> 00:21:23,584
我相信你们一定对技术细节最感兴趣

419
00:21:23,650 --> 00:21:26,687
这也是你们今天来这里的原因
那就是它的读法

420
00:21:27,721 --> 00:21:28,555
那么…

421
00:21:32,593 --> 00:21:36,296
我用了科学方法
我把我们楼层的全部工程师聚到一起

422
00:21:36,630 --> 00:21:39,099
各派系之间的投票差别很大

423
00:21:39,499 --> 00:21:45,772
德国人读“hife” 法国人读“eff”
俄罗斯人读“heef”

424
00:21:45,839 --> 00:21:48,408
而“heef”成为了最终的获胜者

425
00:21:48,842 --> 00:21:53,080
这个“heef”
我几乎不能相信文件能有多大或多小

426
00:21:53,981 --> 00:21:56,683
现在我的芬兰同事迅速指出

427
00:21:56,750 --> 00:21:59,720
诺基亚研究人员
就是那些提出规格的人

428
00:21:59,786 --> 00:22:04,591
所以芬兰读音应该获胜
那应该是1%的“hafe”

429
00:22:07,494 --> 00:22:09,830
嗯 对于我和我们楼层的工程师来说
我们要称它“heef”

430
00:22:10,497 --> 00:22:12,566
它可以使用HEVC内部编码

431
00:22:12,633 --> 00:22:16,537
不出所料地压缩比有20年
悠久历史的JPEG更棒的图片

432
00:22:17,304 --> 00:22:19,373
事实上要比它小两倍

433
00:22:19,740 --> 00:22:23,310
这个平均值 平均小两倍
而不是最多小两倍

434
00:22:23,911 --> 00:22:27,214
我们在大数据图像集上使用定性分析

435
00:22:27,281 --> 00:22:32,119
以实现这个数字
确保视觉质量与JPEG相同

436
00:22:33,754 --> 00:22:37,524
它支持切断图像并压缩为单独的一格

437
00:22:38,058 --> 00:22:42,129
这允许更高效地分段解压大图像

438
00:22:43,797 --> 00:22:47,034
HEIF还特别支持辅助图像

439
00:22:47,301 --> 00:22:50,170
比如阿尔法、视差或深度映射

440
00:22:51,572 --> 00:22:53,273
这是一个深度映射的可视化灰度图

441
00:22:53,340 --> 00:22:55,709
是嵌在这个HEIF文件中的深度映射

442
00:22:56,844 --> 00:23:00,314
拥有深度信息就为你们
打开了图像编辑的世界

443
00:23:00,614 --> 00:23:03,450
比如给背景和前景应用不同的效果

444
00:23:03,750 --> 00:23:04,585
就像这样

445
00:23:05,719 --> 00:23:08,655
在这里 我给背景应用了
黑色和白色筛选器

446
00:23:08,989 --> 00:23:10,591
以及到前景的淡入筛选器

447
00:23:10,657 --> 00:23:13,060
那么请注意
小女孩的紧身衣仍然是粉色的

448
00:23:13,126 --> 00:23:15,028
而后面的一切都是黑色的

449
00:23:15,963 --> 00:23:17,297
了解了深度的层次

450
00:23:17,364 --> 00:23:21,268
我甚至可以移动筛选器的转换点
就像这个

451
00:23:21,502 --> 00:23:22,769
密切注视她手里的花

452
00:23:24,137 --> 00:23:25,939
现在只有她的手和花有颜色

453
00:23:26,006 --> 00:23:27,541
而其它都是黑白的

454
00:23:28,809 --> 00:23:31,111
你甚至可以控制前景和后景的光照

455
00:23:31,178 --> 00:23:33,480
分别曝光 就像这样

456
00:23:34,915 --> 00:23:37,818
现在 她看起来你把她P进了
她自己的照片里

457
00:23:38,552 --> 00:23:40,487
我的意思并不是你应该这样做
而是你可以这样做

458
00:23:41,822 --> 00:23:46,126
这是一场关于深度的两个
半场演讲的开篇

459
00:23:46,193 --> 00:23:48,161
即507和508演讲

460
00:23:48,228 --> 00:23:50,063
我希望你们抽时间看一下这两个视频

461
00:23:51,298 --> 00:23:55,302
提到元数据 HEIF的兼容性很好

462
00:23:55,369 --> 00:23:59,840
它特别支持产业标准Exif和xmp

463
00:24:01,508 --> 00:24:03,410
HEIF并不只是用于单一图像

464
00:24:03,477 --> 00:24:06,446
它还支持图像序列

465
00:24:06,513 --> 00:24:10,584
比如连拍、支架、焦点堆栈

466
00:24:11,251 --> 00:24:15,489
它还用在多媒体上
比如音频和视频轨道

467
00:24:17,291 --> 00:24:18,358
让我们做个演示吧 好吗？

468
00:24:19,626 --> 00:24:23,564
好的 这是一个作品展示
是在Apple自己的Photos应用中

469
00:24:30,571 --> 00:24:34,775
好的 我要从全景开始
这是一个很漂亮的全景

470
00:24:35,209 --> 00:24:37,778
这个来自约塞米蒂国家公园的土伦草甸

471
00:24:38,445 --> 00:24:41,748
看起来很不错
这就是你希望从全景中看到的那种

472
00:24:41,815 --> 00:24:43,584
直到你开始放大

473
00:24:44,284 --> 00:24:45,118
那么让我们放大吧

474
00:24:46,753 --> 00:24:47,588
稍微放大点儿

475
00:24:48,322 --> 00:24:50,390
看起来不错 让我们再放大点儿

476
00:24:51,191 --> 00:24:52,693
然后再放大点儿

477
00:24:53,760 --> 00:24:55,262
然后再放大点儿

478
00:24:56,230 --> 00:24:57,464
一直放大

479
00:24:58,131 --> 00:25:01,768
一直放大 我的天啊
我可以看到速度限制是多少

480
00:25:03,403 --> 00:25:04,271
哇哦

481
00:25:09,409 --> 00:25:11,411
那里有车 有移动厕所

482
00:25:13,380 --> 00:25:15,816
我甚至可以去看一眼背景中的山峰

483
00:25:18,685 --> 00:25:20,888
它是如何随着我的动作
变得这么清晰的呢？

484
00:25:21,455 --> 00:25:25,359
这实际上是一个29亿像素的全景图

485
00:25:25,826 --> 00:25:29,863
大约是91,000像素乘32,000像素

486
00:25:30,464 --> 00:25:33,567
它的RGB TIFF文件大大超过了2GB

487
00:25:33,634 --> 00:25:37,971
并且我向你们保证 它足以让任何
Mac屈服 而HEIF文件

488
00:25:38,038 --> 00:25:40,674
是160MB

489
00:25:41,041 --> 00:25:47,381
这在JPEG来说是根本不可能的
因为JPEG最高是64k乘64k像素

490
00:25:47,981 --> 00:25:49,383
HEIF没有超过最高值

491
00:25:49,883 --> 00:25:55,589
它支持任意大文件 并随时检查内存

492
00:25:55,656 --> 00:25:58,025
高效地加载和卸载片段

493
00:25:58,358 --> 00:26:00,627
那么当我面前摆着这个庞大的数据时

494
00:26:00,928 --> 00:26:04,998
在Photos应用中
我再也不用占用70MB以上的内存了

495
00:26:05,332 --> 00:26:08,068
那么它还具有响应性 我可以缩放

496
00:26:08,135 --> 00:26:10,637
我可以缩放一整天
但我想我们很可能得回到幻灯片中去

497
00:26:19,213 --> 00:26:23,851
在iOS 11和macOS 10.13
所支持的全部硬件上

498
00:26:23,917 --> 00:26:28,055
我们可以读取和解码HEIF的
三种不同风格

499
00:26:28,121 --> 00:26:29,723
你在这里看到的三种扩展

500
00:26:29,790 --> 00:26:32,759
是与如何在文件中编码主图像相关的

501
00:26:34,461 --> 00:26:40,400
指向包含通过HEVC压缩的主图像的

502
00:26:40,467 --> 00:26:43,470
HEIF文件的public.heic的HEIC
.HEIC和UTI

503
00:26:44,171 --> 00:26:50,077
第二个风格是AVCI
通过H.264压缩主图像

504
00:26:50,677 --> 00:26:54,081
然后其它保留.HEIF扩展

505
00:26:54,147 --> 00:26:57,317
可以是JPEG 也可以是
所支持的任意编解码器

506
00:26:59,186 --> 00:27:02,789
对于编码和写入
我们只支持一种形式的HEIF

507
00:27:02,856 --> 00:27:06,994
即HEIC格式
换句话说就是你使用HEVC的那些

508
00:27:07,361 --> 00:27:10,531
我们猜如果你很好地采用了新文件容器

509
00:27:10,797 --> 00:27:12,165
你可能也会采用

510
00:27:12,232 --> 00:27:13,800
最棒的压缩标准

511
00:27:14,101 --> 00:27:18,639
当前仅拥有A10融合芯片的
iOS 11设备支持

512
00:27:19,907 --> 00:27:22,609
好的 让我们从低等级看一下HEIF

513
00:27:23,877 --> 00:27:28,615
我们平台上读写文件的最低等级
界面是ImageIO

514
00:27:29,116 --> 00:27:33,253
它封装读取 从文件或内存数据源

515
00:27:33,320 --> 00:27:35,689
使用一个叫作CGImageSource的对象

516
00:27:36,590 --> 00:27:39,893
它还支持写入文件或写入不可变数据

517
00:27:40,060 --> 00:27:42,229
使用CGImageDestination

518
00:27:42,896 --> 00:27:45,232
这些对象已经存在很长时间了

519
00:27:45,299 --> 00:27:46,333
你们很可能用过它们

520
00:27:47,234 --> 00:27:51,438
要在磁盘上打开一个JPEG图像文件
这是用ImageIO打开图像文件的方式

521
00:27:51,505 --> 00:27:52,906
首先创建URL

522
00:27:53,640 --> 00:27:57,811
然后调用CGImageSourceCreateWithURL
来创建你的源

523
00:27:58,278 --> 00:28:00,147
最后是一个选项库

524
00:28:00,214 --> 00:28:02,783
你可以选择性地传递输入的UTI

525
00:28:03,217 --> 00:28:05,485
当你在磁盘上打开文件时
是不需要这个的

526
00:28:05,652 --> 00:28:09,456
因为可以从文件路径扩展中引用UTI

527
00:28:10,290 --> 00:28:13,994
一旦得到CGImageSource
你可以做一系列的事

528
00:28:14,528 --> 00:28:18,198
比如在任何索引上复制属性

529
00:28:18,265 --> 00:28:20,934
将会从中取出元数据 比如Exif

530
00:28:21,668 --> 00:28:25,205
你还可以从文件中的任何图像中
创建CGImage

531
00:28:25,372 --> 00:28:28,308
对于JPEG
一般来说文件中只能有一个图像

532
00:28:28,909 --> 00:28:32,779
CGImage当然像是一个承诺
一个渲染承诺

533
00:28:33,113 --> 00:28:37,484
必要时可以使用CGImage
不紧不慢地解码JPEG数据

534
00:28:37,684 --> 00:28:40,988
比如当你把它渲染到CG位图情境中时

535
00:28:41,688 --> 00:28:45,826
你还可以通过许多选项获得一个缩略图

536
00:28:46,293 --> 00:28:48,495
比如你想要得到的最大尺寸

537
00:28:48,695 --> 00:28:50,697
如果文件中没有可用的要如何处理

538
00:28:51,231 --> 00:28:53,800
以及何时调用CGImageSourceCreate
ThumbnailAtIndex

539
00:28:53,867 --> 00:28:56,136
它会立即执行解码

540
00:28:57,905 --> 00:29:01,375
现在这是打开.HEIC文件的模拟代码

541
00:29:02,509 --> 00:29:03,944
有人能指出其中的不同点吗？

542
00:29:05,212 --> 00:29:06,313
在这里我把它变简单了

543
00:29:09,149 --> 00:29:09,983
就这样

544
00:29:10,484 --> 00:29:13,654
它是个注释并且它是个文件路径
就是这样

545
00:29:14,154 --> 00:29:16,190
换句话说 CGImageSource就这么管用了

546
00:29:16,356 --> 00:29:19,960
你没有看到的一个不同点是
如何解码HEVC

547
00:29:20,694 --> 00:29:24,631
在最近的iOS设备和Mac上
是硬件解码

548
00:29:25,032 --> 00:29:28,936
而较老的设备上是通过软件解码的
所以速度会较慢

549
00:29:30,470 --> 00:29:33,407
关于我们刚才在演示中看到的
分格支持有一个提示

550
00:29:34,007 --> 00:29:37,945
CGImageSource可以提供
关于图像的属性库

551
00:29:38,011 --> 00:29:42,482
通过调用
CGImageSourceCopyPropertiesAtIndex

552
00:29:42,549 --> 00:29:48,255
并且属性库是元数据的代名词
Exif、Apple Maker Note等等

553
00:29:48,956 --> 00:29:51,859
还有一个子库
叫作TIFF subdictionary

554
00:29:52,192 --> 00:29:54,828
你可以从中找到被编码的格子的尺寸

555
00:29:54,895 --> 00:29:56,797
比如格子长度和格子宽度

556
00:29:57,231 --> 00:30:00,834
默认是编码为512乘512像素

557
00:30:03,003 --> 00:30:06,273
正如我们所看到的
CGImageSource为你提供CG图像

558
00:30:06,673 --> 00:30:10,010
并且CGImage有个很漂亮的方法
叫作cropping(to:)

559
00:30:10,344 --> 00:30:11,945
利用了分格

560
00:30:12,546 --> 00:30:14,515
这个调用会创建一个新CGImage

561
00:30:14,581 --> 00:30:17,551
只包含另一个图像的一小部分

562
00:30:18,285 --> 00:30:21,522
这不是个新API
但它用起来与HEIF相当协调

563
00:30:21,889 --> 00:30:23,991
在HEIF中分格是被单独编码的

564
00:30:24,558 --> 00:30:28,328
你不必担心潜在的分格编码尺寸

565
00:30:28,395 --> 00:30:31,632
你只需要请求分区说你想显示或渲染

566
00:30:32,366 --> 00:30:35,602
并了解在底层
你将会获得全部tile-y好处

567
00:30:35,669 --> 00:30:39,706
它只解码对那个分区有必要的分格

568
00:30:41,208 --> 00:30:45,679
现在让我们谈谈写
这是如何用ImageIO写JPEG

569
00:30:46,480 --> 00:30:48,949
你 在创建CGImageDestination之后

570
00:30:49,016 --> 00:30:51,418
调用CGImageDestination
CreateWithURL

571
00:30:54,555 --> 00:30:58,125
我应该指出一点
你需要制定UTI是什么

572
00:30:58,926 --> 00:31:00,761
在这里我用的是AVFileType.jpg

573
00:31:00,827 --> 00:31:04,331
与UT类型public.jpg一样

574
00:31:05,699 --> 00:31:07,701
我很关心结果

575
00:31:08,268 --> 00:31:11,572
我用了保护 让目的文件为无

576
00:31:12,139 --> 00:31:15,576
现在 对于当前JPEG
它是无的唯一原因

577
00:31:15,642 --> 00:31:19,079
是如果你请求写入一个沙盒外部的文件

578
00:31:19,146 --> 00:31:22,282
但为了防御
你应该以这种方式来写代码

579
00:31:23,116 --> 00:31:25,752
接下来 添加你的CG图像

580
00:31:25,819 --> 00:31:29,089
一次添加一个 可以伴随元数据
如果你喜欢的话

581
00:31:29,523 --> 00:31:32,960
然后当你完成后
调用CGImageDestinationFinalize

582
00:31:33,360 --> 00:31:36,563
这将关闭用于编辑的容器
然后将其写入磁盘

583
00:31:37,965 --> 00:31:41,702
现在 让我们看一些HEIC的写法
再一次 只有很小的不同点

584
00:31:44,004 --> 00:31:47,174
只是文件路径扩展、UTI、注释不同

585
00:31:47,741 --> 00:31:50,377
在JPEG和HEIF之间
有一个很重要的不同点

586
00:31:50,444 --> 00:31:54,348
即创建CGImageDestination将会失败

587
00:31:54,615 --> 00:31:57,584
在没有HEVC硬件编码器的设备上

588
00:31:58,118 --> 00:32:00,287
如果失败 目的即为无

589
00:32:00,687 --> 00:32:03,323
那么我在上一张幻灯片上
写的防御性良好的代码

590
00:32:03,690 --> 00:32:06,693
在HEVC中更加重要

591
00:32:06,860 --> 00:32:09,696
现在有一个目的为无的新理由

592
00:32:10,597 --> 00:32:13,767
请一定要确保你检查这个
这是那个也是唯一一个

593
00:32:13,834 --> 00:32:17,371
了解你当前平台
是否支持写入HEIC的方式

594
00:32:19,106 --> 00:32:20,874
还有不值一提的是那个ImageIO

595
00:32:20,941 --> 00:32:23,577
已经添加了读写深度映射的功能

596
00:32:23,644 --> 00:32:25,078
正如我之前讲过的

597
00:32:25,445 --> 00:32:28,782
我们已经为HEIC实现了那个
并且我们操纵JPEG

598
00:32:28,849 --> 00:32:32,019
以奇怪的魔法方式
我们很可能不应该讲这个

599
00:32:32,419 --> 00:32:34,755
我不会再深入地讲了

600
00:32:34,821 --> 00:32:38,625
因为它在一个
专门的507和508演讲上讲到了

601
00:32:38,859 --> 00:32:39,860
是关于深度的

602
00:32:39,927 --> 00:32:42,062
我希望你们能看一下那些演讲的视频

603
00:32:42,129 --> 00:32:46,366
因为它们讲的是
HEIF中的辅助图像格式

604
00:32:48,168 --> 00:32:50,103
好的 是时候继续下一个主要话题了

605
00:32:50,170 --> 00:32:52,172
即高等级获取HEIF

606
00:32:52,973 --> 00:32:58,345
但在此之前 我感觉WWDC
应该是一种文化体验

607
00:32:58,412 --> 00:33:01,081
具有文化意义上的价值
而不只是一种教育性大会

608
00:33:01,148 --> 00:33:03,250
这就是我想让你们休息片刻的原因

609
00:33:03,317 --> 00:33:04,918
通过一些压缩相关的诗

610
00:33:06,220 --> 00:33:07,054
好的

611
00:33:07,421 --> 00:33:08,255
等着瞧吧

612
00:33:09,389 --> 00:33:13,460
JPEG这么大 但HEIF很简洁

613
00:33:15,062 --> 00:33:15,896
谢谢

614
00:33:18,065 --> 00:33:20,100
看到这个压缩相关的诗句了吧 很短

615
00:33:21,401 --> 00:33:23,136
你喜欢吗？你想听更多吗？

616
00:33:24,104 --> 00:33:26,707
好的 让我们再来一个
这是一个压缩俳句

617
00:33:29,209 --> 00:33:30,911
HEVC

618
00:33:32,246 --> 00:33:33,747
的音节

619
00:33:34,815 --> 00:33:37,251
比JPEG进程多一倍

620
00:33:38,151 --> 00:33:39,953
谢谢大家 好了 让我们继续吧

621
00:33:41,755 --> 00:33:43,290
我确定他们稍后会编辑并发布它

622
00:33:44,224 --> 00:33:46,159
好的 我们要谈谈HEIF和PhotoKit

623
00:33:46,860 --> 00:33:48,562
PhotoKit其实是两个框架

624
00:33:48,629 --> 00:33:51,798
分别是Photos框架和PhotosUI
等级很高

625
00:33:51,865 --> 00:33:53,166
甚至在UIKit之上

626
00:33:54,968 --> 00:33:59,106
当应用调整时
你在PhotoKit中使用HEIF的方式

627
00:33:59,306 --> 00:34:00,874
我们只是简略地谈一下

628
00:34:01,074 --> 00:34:03,010
并且我们会谈如何在三种
不同的情境中应用调整

629
00:34:03,076 --> 00:34:06,213
图片、视频和动态图片

630
00:34:06,547 --> 00:34:10,516
然后我们会谈你与PHPhotoLibrary
一起使用的常见流程

631
00:34:11,784 --> 00:34:13,320
让我们简略地概括一下
其中所包含的步骤

632
00:34:13,387 --> 00:34:17,357
通过PhotoLibrary把编辑或调整
应用到资产中的步骤

633
00:34:18,225 --> 00:34:20,928
你请求PHPhotoLibrary
performChanges

634
00:34:20,994 --> 00:34:25,032
在那个变更请求中
你以你想编辑的PHAsset为开始

635
00:34:25,498 --> 00:34:26,600
比如照片

636
00:34:27,367 --> 00:34:30,637
并且在资产上请求内容编辑输入

637
00:34:30,704 --> 00:34:33,373
以获取PHContentEditingInput

638
00:34:34,474 --> 00:34:38,045
这就是为你提供与你的资产
相关联的全部媒体权限的东西

639
00:34:38,110 --> 00:34:43,550
比如UIImage、URL、AVAsset
或Live Photo

640
00:34:44,418 --> 00:34:47,054
接下来你要创建一个
PHContentEditingOutput

641
00:34:47,788 --> 00:34:49,956
通过与内容编辑输入一起调用

642
00:34:50,724 --> 00:34:53,393
编辑输出会告诉你

643
00:34:53,460 --> 00:34:58,599
要把全部已渲染的文件放在磁盘上
哪个位置 通过提供renderContentURL

644
00:34:59,066 --> 00:35:00,467
然后实施你的编辑

645
00:35:00,734 --> 00:35:04,471
在编辑输入中为你提供的那个媒体上

646
00:35:04,771 --> 00:35:06,974
然后你将它们写入指定位置

647
00:35:07,574 --> 00:35:11,879
最后 PHPhotoLibrary验证你的修改

648
00:35:12,145 --> 00:35:14,581
并将它们作为一个整体接受或拒绝修改

649
00:35:16,917 --> 00:35:20,320
那么renderedOutputImages的规则没变

650
00:35:20,387 --> 00:35:23,023
但你可能没有意识到它们已经生效了

651
00:35:23,524 --> 00:35:28,228
在iOS 10中 你的输出图像
必须被渲染为JPEG

652
00:35:28,729 --> 00:35:30,731
Exif定向为1

653
00:35:30,898 --> 00:35:33,333
也就是如果需要实现任何旋转

654
00:35:33,400 --> 00:35:37,037
它会合并到
outputRendered文件中的图像中

655
00:35:37,504 --> 00:35:38,772
你可能忽视了这个细节

656
00:35:38,839 --> 00:35:43,277
因为很可能你要编辑的99%的内容
都以JPEG格式提供

657
00:35:43,343 --> 00:35:45,546
然后你只需要将其输出位同一个格式

658
00:35:45,913 --> 00:35:49,516
但现在 你看到输入内容的扩散
也就是HEIC

659
00:35:49,716 --> 00:35:50,817
所以你应该注意

660
00:35:50,884 --> 00:35:54,454
你必须仍然将全部输出内容
渲染为JPEG

661
00:35:54,821 --> 00:35:56,490
Exif定向为1

662
00:35:58,258 --> 00:35:59,293
这是代码

663
00:35:59,893 --> 00:36:03,263
首先做一个CIImage
这是其中一种实现方式

664
00:36:03,697 --> 00:36:07,668
你可以从内容编辑输入文件URL中
做一个CIImage

665
00:36:08,468 --> 00:36:10,003
然后应用编辑

666
00:36:10,204 --> 00:36:15,876
在这里 关于定向
我要既应用筛选 又应用合并

667
00:36:16,677 --> 00:36:20,781
然后 当我完成后
我要调用ciContext的掌上兵器库

668
00:36:20,848 --> 00:36:22,482
写JPEGRepresentation

669
00:36:23,283 --> 00:36:25,919
如果你之前曾用过这个样板代码

670
00:36:25,986 --> 00:36:29,323
它仍然能在这里继续使用
因为它输出JPEG

671
00:36:29,389 --> 00:36:31,191
无论输入是什么

672
00:36:32,926 --> 00:36:36,129
我们第二个引用调整用例与食品有关

673
00:36:36,196 --> 00:36:39,166
规则也与iOS 10相同

674
00:36:39,399 --> 00:36:42,369
即无论输入电影内容的格式是什么

675
00:36:42,703 --> 00:36:47,140
你都必须生产用H.264压缩的电影
作为输出

676
00:36:48,075 --> 00:36:50,110
是的 即便源电影是HEVC

677
00:36:50,177 --> 00:36:53,747
你仍需要将其渲染为H.264作为输出

678
00:36:57,651 --> 00:37:00,988
这是一些编辑像这样的
视频内容的样板代码

679
00:37:01,221 --> 00:37:05,325
首先你从PHContentEditingInput中
获取AVAsset

680
00:37:06,360 --> 00:37:09,296
然后你可以创建一个
AVVideoComposition

681
00:37:09,363 --> 00:37:11,798
在AVVideoComposition中
你每次提交一帧

682
00:37:12,933 --> 00:37:14,801
并且你可以获得CIImages

683
00:37:14,868 --> 00:37:18,338
然后请求一个对象 名字是一长串

684
00:37:18,405 --> 00:37:21,909
AVAsynchronous
CoreImageFilteringRequest

685
00:37:22,509 --> 00:37:25,979
你得到了CIImage
然后你生产了CIImage

686
00:37:26,480 --> 00:37:29,349
当你渲染完成后
要调用request.finish

687
00:37:29,983 --> 00:37:30,984
然后作为最后一个步骤

688
00:37:31,285 --> 00:37:35,155
你要把你的AVAsset导出到
磁盘上的文件

689
00:37:35,222 --> 00:37:38,325
其URL为PHContentEditingOutput
告诉你的那个URL

690
00:37:38,926 --> 00:37:39,993
现在这是最重要的部分

691
00:37:40,494 --> 00:37:44,698
要使用的预设是
AVAssetExportPresetHighestQuality

692
00:37:44,898 --> 00:37:49,503
或任意现有的预设 正如Erik所说的
仍压缩为H.264

693
00:37:50,204 --> 00:37:53,874
请不要使用名字类似的新预设
它们的名字中包括HEVC

694
00:37:54,107 --> 00:37:56,710
因为你会变更请求 从而以报错而告终

695
00:37:58,745 --> 00:38:00,781
最后 通过Live Photo

696
00:38:00,848 --> 00:38:04,651
Live Photo的视频内容应用调整

697
00:38:05,786 --> 00:38:07,588
我要讲的是

698
00:38:07,654 --> 00:38:09,423
画面的移动方面

699
00:38:09,489 --> 00:38:12,492
当你在Live Photo之间滑来滑去时

700
00:38:12,559 --> 00:38:16,763
或当你压感触控图片时
或在图片之间滑动时

701
00:38:17,331 --> 00:38:18,832
这是最简单的用例

702
00:38:18,899 --> 00:38:22,269
因为你永远不会直接处理
输入或输出文件

703
00:38:22,669 --> 00:38:25,639
你传递了CIImages 并且生成CIImages

704
00:38:26,039 --> 00:38:27,741
然后就以你的名义完成了编码

705
00:38:29,009 --> 00:38:30,844
这里有很多很棒的代码可以看

706
00:38:30,911 --> 00:38:32,579
但我不会在这上面花太多时间

707
00:38:32,646 --> 00:38:35,382
稍后你可以暂停一下视频 再仔细看

708
00:38:35,883 --> 00:38:37,050
其中最重要的一点是

709
00:38:37,117 --> 00:38:40,721
你在Live Photo电影中筛选完每一帧后

710
00:38:41,054 --> 00:38:44,157
你可以让Live Photo内容
把你的Live Photo保存

711
00:38:44,224 --> 00:38:46,159
到一个指定URL 就是这样

712
00:38:46,960 --> 00:38:50,564
Live Photo将使用H.264被保存下来

713
00:38:51,098 --> 00:38:53,800
就像静态图片那样被编码为JPEG

714
00:38:55,669 --> 00:38:58,772
好的 让我们继续看
PhotoKit的常见流程

715
00:38:59,506 --> 00:39:02,209
当从图库中显示内容时

716
00:39:02,476 --> 00:39:06,013
你使用一个叫作
PHImageManager的对象

717
00:39:06,647 --> 00:39:09,149
这个为你提供了三个的其中一个

718
00:39:09,883 --> 00:39:12,219
如果是图像的话你可以得到
一个UIImage

719
00:39:12,286 --> 00:39:14,321
如果是视频的话可以得到
一个PlayerItem

720
00:39:14,655 --> 00:39:17,691
或如果是Live Photo内容的话
得到一个PHLivePhoto

721
00:39:18,192 --> 00:39:19,927
在这里你不需要进行任何修改

722
00:39:20,127 --> 00:39:23,063
因为这些都是高等级的提取

723
00:39:23,130 --> 00:39:25,299
你不用关心源来自哪里

724
00:39:25,365 --> 00:39:28,368
你要做的就是显示它们
不需要修改任何代码

725
00:39:30,437 --> 00:39:34,241
接下来是备份 当PhotoKit用于备份时

726
00:39:34,675 --> 00:39:37,177
你很可能想获取原始资产

727
00:39:37,244 --> 00:39:39,479
比如HEIC文件和QuickTime电影

728
00:39:39,813 --> 00:39:42,616
并且你通过
PHAssetResourceManager实现

729
00:39:43,016 --> 00:39:45,085
它为以原生格式提供给你

730
00:39:45,419 --> 00:39:47,087
这里唯一需要注意的是

731
00:39:47,688 --> 00:39:50,157
你可能会获得与之前不同的文件类型

732
00:39:50,224 --> 00:39:51,725
所以要准备好接受这一点

733
00:39:53,627 --> 00:39:55,762
第三个也是最复杂的情况是共享

734
00:39:56,730 --> 00:39:57,731
在这里

735
00:39:57,798 --> 00:39:59,900
你即将离开Apple漂亮的围墙花园

736
00:40:00,334 --> 00:40:02,769
你不得不思考你自己的兼容性需求

737
00:40:03,170 --> 00:40:04,838
原生资产可以吗？

738
00:40:05,172 --> 00:40:07,708
你可能会帮你客户一个忙
或你可能帮倒忙

739
00:40:07,774 --> 00:40:11,845
通过为他们提供HEIC内容
取决于他们是否准备好接受它

740
00:40:11,912 --> 00:40:16,550
那么在这里你必须权衡
兼容性与HEIC所提供的功能

741
00:40:17,618 --> 00:40:20,287
如果你选择兼容性高于功能

742
00:40:20,354 --> 00:40:23,257
你可以确保格式兼容性

743
00:40:23,323 --> 00:40:25,325
通过明确指定输出格式

744
00:40:25,692 --> 00:40:28,262
对于图片 你只需要检查UTType

745
00:40:28,729 --> 00:40:32,299
看是否遵从JPEG

746
00:40:32,366 --> 00:40:34,334
如果不遵从 就明确地转换它

747
00:40:34,768 --> 00:40:38,438
对于视频 你总能强制性兼容
通过请求

748
00:40:38,505 --> 00:40:40,274
用预设导出会话

749
00:40:40,340 --> 00:40:45,045
你知道那将提交H.264
比如PresetHighestQuality

750
00:40:47,181 --> 00:40:50,217
好了 进入我们今天的最后一个话题
捕捉HEIF

751
00:40:50,617 --> 00:40:52,853
最后这是一个我知道要讲什么的话题

752
00:40:53,887 --> 00:40:56,723
让我们看第二个压缩相关的俳句
我可以吗？

753
00:40:57,291 --> 00:40:59,326
我觉得很有意思 开始吧

754
00:41:00,127 --> 00:41:02,196
HEIF 一个容器

755
00:41:03,230 --> 00:41:08,268
比HEVC的压缩效果好四倍

756
00:41:09,970 --> 00:41:14,107
思考一下 好的
为什么我们要浪费时间谈HEVC呢

757
00:41:14,174 --> 00:41:16,643
它应该是一个号的编解码器 对吧？
我们为什么不叫它“hevick”

758
00:41:18,512 --> 00:41:20,814
好的 那么Erik提到

759
00:41:21,014 --> 00:41:26,753
AVCapturePhotoOutput添加了对通过
HEVC编码的Live Photo电影的支持

760
00:41:27,421 --> 00:41:29,156
这个类是去年引入的

761
00:41:29,223 --> 00:41:31,825
作为AVCapture
StillImageOutput的后续

762
00:41:32,159 --> 00:41:34,962
它擅长处理复杂的静态图像捕捉请求

763
00:41:35,028 --> 00:41:37,998
当你需要随时间提交多个资产的时候

764
00:41:38,832 --> 00:41:42,669
它目前是我们平台上
捕捉Live Photo的唯一方式

765
00:41:43,403 --> 00:41:47,841
Bayer RAW图像 Apple P3宽彩色图像

766
00:41:48,208 --> 00:41:50,878
在iOS 11中 这是我们平台上

767
00:41:50,944 --> 00:41:53,881
捕捉HEIF内容的唯一界面

768
00:41:54,381 --> 00:41:56,116
（HEIF捕捉支持）

769
00:41:56,183 --> 00:42:01,054
A10芯片设备上支持HEIF捕捉

770
00:42:01,121 --> 00:42:05,592
也就是iPhone 7 Plus、iPhone 7
以及最新发布的iPad Pros

771
00:42:07,828 --> 00:42:09,696
我们会做一个简短的刷新器
关于如何请求

772
00:42:09,763 --> 00:42:12,165
和接收图像 通过图片输出

773
00:42:12,766 --> 00:42:16,970
首先 你要填写一个
叫作AVCapturePhotoSettings的对象

774
00:42:17,037 --> 00:42:18,739
这个类似于请求对象

775
00:42:18,972 --> 00:42:22,376
就是指定你想在图片捕捉中
实施哪些功能的对象

776
00:42:22,743 --> 00:42:24,344
就是橘色框里的内容

777
00:42:25,078 --> 00:42:27,447
在这里我表明我想要自动闪光

778
00:42:27,514 --> 00:42:31,185
意思是仅适用闪光灯进行图片输出
如果必要的话

779
00:42:31,251 --> 00:42:33,654
仅当灯光太暗时使用

780
00:42:34,321 --> 00:42:38,225
我还请求预览尺寸的图像
伴随完整尺寸的图像

781
00:42:38,425 --> 00:42:40,794
以便我可以在屏幕上放一个快速预览

782
00:42:41,361 --> 00:42:43,931
我不知道最终的纵横比是多少

783
00:42:43,997 --> 00:42:48,035
所以我只请求为1440乘1440

784
00:42:48,869 --> 00:42:52,873
然后通过我给照片输出提供的
用于开始或剔除

785
00:42:53,373 --> 00:42:57,845
捕捉请求的委托传递这个设置对象

786
00:43:00,881 --> 00:43:03,784
现在顶部的箭头显示何时提出请求

787
00:43:04,218 --> 00:43:08,288
现在我要追踪这个包裹递送

788
00:43:08,455 --> 00:43:12,759
PhotoOutput把我的委托调回来了
每次调用一个方法

789
00:43:13,360 --> 00:43:16,196
在我提出请求后 PhotoOutput很快

790
00:43:16,263 --> 00:43:17,531
就进行了第一个委托回调

791
00:43:17,598 --> 00:43:21,468
即willBeginCapture
ForResolvedSettings

792
00:43:21,535 --> 00:43:25,873
它传给你这个蓝色框里的内容
即ResolvedPhotoSettings

793
00:43:26,373 --> 00:43:29,309
这就像一封礼貌的邮件 比如说

794
00:43:29,376 --> 00:43:32,880
“我们收到了你的订单”
“这是我们将发送给你的内容”

795
00:43:33,580 --> 00:43:38,252
这个ResolvedPhotoSetting会清除

796
00:43:38,452 --> 00:43:40,787
你一开始在设置中设的不明确的内容

797
00:43:41,154 --> 00:43:44,925
在本例中 我们现在可以看到
闪光灯不是自动 它是真或假

798
00:43:45,292 --> 00:43:48,428
那么如果它是真 我们都知道就会闪光

799
00:43:48,829 --> 00:43:53,200
同时我们现在也知道
最终的预览图像分辨率是多少

800
00:43:55,569 --> 00:43:58,572
最终在我们得到
willBeginCaptureFor之后

801
00:43:58,639 --> 00:44:00,741
我们 我们收到的第二个回调是

802
00:44:00,807 --> 00:44:03,310
willCapturePhoto
ForResolvedSettings

803
00:44:03,777 --> 00:44:07,581
这是与快门声音同时提交的

804
00:44:09,082 --> 00:44:10,484
然后此后不久

805
00:44:10,551 --> 00:44:13,387
就执行了didCapturePhoto
ForResolvedSettings

806
00:44:13,654 --> 00:44:16,490
就在图像被完全曝光和读出来后

807
00:44:17,824 --> 00:44:20,661
然后一般会传递一些时间
当处理一张图像

808
00:44:20,928 --> 00:44:24,665
或几张图像时
应用你所请求的全部功能

809
00:44:25,365 --> 00:44:27,067
当图片准备好后 你会收到

810
00:44:27,134 --> 00:44:30,337
didFinishProcessingPhoto
样本缓冲回调

811
00:44:30,804 --> 00:44:33,640
并向你提交图片

812
00:44:33,907 --> 00:44:36,810
在这里我得到了主图像以及预览图像

813
00:44:37,010 --> 00:44:38,912
它们是在同一个回调中同时提交的

814
00:44:40,347 --> 00:44:42,916
最后你总是会得到

815
00:44:43,083 --> 00:44:46,920
didFinishCaptureFor
ResolvedSettings回调

816
00:44:47,321 --> 00:44:49,489
这是最后要提交的

817
00:44:49,656 --> 00:44:53,861
就好像PhotoOutput在说
我们完成本次交易了

818
00:44:54,027 --> 00:44:57,364
很高兴与你一起合作
你现在可以清除你的委托了

819
00:45:00,868 --> 00:45:03,837
这个编程模型是非常灵活的

820
00:45:03,904 --> 00:45:07,908
我们有一些很成功的例子
因为我们按需要

821
00:45:07,975 --> 00:45:09,676
给委托添加新方法

822
00:45:09,743 --> 00:45:11,011
当我们添加新功能时

823
00:45:11,745 --> 00:45:14,348
比如我们添加了RAW图像支持

824
00:45:14,882 --> 00:45:16,116
这是那个的回调

825
00:45:16,917 --> 00:45:18,552
我们添加了Live Photo的支持

826
00:45:18,952 --> 00:45:21,655
这是那个的单独回调
目的是获得电影

827
00:45:22,322 --> 00:45:24,858
那么看起来HEIF也会简便地添加

828
00:45:24,925 --> 00:45:27,094
到这个非常灵活的编程范例中

829
00:45:27,561 --> 00:45:29,062
很遗憾 并不是

830
00:45:30,697 --> 00:45:33,934
CoreMedia SampleBuffer中的不相容性

831
00:45:34,301 --> 00:45:35,903
现在是并且一直是

832
00:45:35,969 --> 00:45:38,705
AVFoundation领域的一个难题

833
00:45:39,239 --> 00:45:43,010
我们从iOS 4起就开始
将它用于提交静态图像

834
00:45:44,444 --> 00:45:47,481
它是一个很小的媒体数据容器
比如视频样本、

835
00:45:47,548 --> 00:45:50,050
音频样本、文本、闭合字幕

836
00:45:51,652 --> 00:45:55,255
另一方面 HEIF是一种文件格式
不是媒体格式

837
00:45:55,489 --> 00:45:56,990
它可以处理很多种媒体类型

838
00:45:58,158 --> 00:46:03,096
同时 CMSampleBuffers可以 当然了
携带HEVC压缩视频

839
00:46:03,697 --> 00:46:07,534
但那个HEVC压缩视频跟

840
00:46:07,601 --> 00:46:09,536
HEIF集装箱化的HEVC不一样

841
00:46:09,903 --> 00:46:14,875
请记住 HEIF喜欢把元素
切成单独的分格以便快速解码

842
00:46:15,242 --> 00:46:18,779
你不能存储那种HEVC压缩

843
00:46:18,846 --> 00:46:22,616
在QuickTime电影的帧中
那会使解码器混淆

844
00:46:23,350 --> 00:46:24,885
在这点上 你可能会问自己

845
00:46:25,152 --> 00:46:27,387
如果我们有这种根本意义上的紧张

846
00:46:27,454 --> 00:46:30,090
在文件容器和媒体容器之间

847
00:46:30,290 --> 00:46:33,760
我们为什么这么多年
都在使用CMSampleBuffer的

848
00:46:33,927 --> 00:46:36,597
图片输出和静态图片输出？

849
00:46:37,264 --> 00:46:39,366
嗯 答案是JPEG

850
00:46:40,501 --> 00:46:43,837
我们用这个来逃避问题
因为碰巧JPEG

851
00:46:44,338 --> 00:46:47,508
图片编解码器 与JFIF 文件格式

852
00:46:47,674 --> 00:46:49,910
实际上是不能区分开的

853
00:46:50,210 --> 00:46:51,745
这两个都是图片

854
00:46:52,379 --> 00:46:54,848
在另一个容器中
比如QuickTime电影

855
00:46:56,550 --> 00:47:00,120
那么我们那个困境的答案是
提出一个新的特制

856
00:47:00,420 --> 00:47:04,725
内存包装器 用于图像结果
我们管它叫AVCapturePhoto

857
00:47:05,325 --> 00:47:07,794
这是对CMSampleBuffer的插入式替换

858
00:47:09,229 --> 00:47:13,133
事实上比CMSampleBuffer快
因为我们能优化

859
00:47:13,200 --> 00:47:16,570
它的提交
通过媒体服务器中的进程范围

860
00:47:16,737 --> 00:47:19,473
所以你能得到比iOS 10中更好的性能

861
00:47:20,741 --> 00:47:24,778
它是100%不可变
与CMSampleBuffer不同

862
00:47:24,845 --> 00:47:28,081
所以它在代码模型之间共享也会更容易

863
00:47:29,216 --> 00:47:31,752
它还得到了集装箱化数据的支持

864
00:47:31,818 --> 00:47:33,420
我稍后再谈这个

865
00:47:34,588 --> 00:47:36,423
让我们谈谈它的贡献

866
00:47:36,890 --> 00:47:39,960
它有与图片相关的关键信息的权限
比如

867
00:47:40,194 --> 00:47:42,162
何时被捕捉

868
00:47:42,229 --> 00:47:45,465
是否为RAW、Bayer RAW图片

869
00:47:46,066 --> 00:47:49,903
对于未压缩或RAW图片来说
你能获取像素缓冲数据

870
00:47:50,737 --> 00:47:54,842
同时边频带信息也跟随
AVCapturePhoto一起 比如

871
00:47:55,175 --> 00:47:58,745
你能请求的第二个
较小尺寸的预览图像

872
00:47:59,279 --> 00:48:03,717
你现在还可以请求
第三个更小尺寸的图像

873
00:48:03,984 --> 00:48:06,887
并将其作为缩略图嵌入容器中

874
00:48:08,722 --> 00:48:11,058
ImageIO属性样式元数据库

875
00:48:11,124 --> 00:48:15,329
是它可以包含Exif
或其它你所期待的元数据

876
00:48:16,163 --> 00:48:21,335
通过iPhone 7 Plus的双摄像头
你可以请求提交深度数据映像

877
00:48:21,401 --> 00:48:23,971
通过AVCapturePhoto 结果也一样

878
00:48:25,472 --> 00:48:28,942
AVCapturePhoto还提供
一系列的便利访问器

879
00:48:29,009 --> 00:48:32,212
比如引用resolvedSettings对象

880
00:48:32,279 --> 00:48:34,114
我们曾在之前的幻灯片中看到过

881
00:48:34,548 --> 00:48:38,752
此外 它还可以让你简便地获取
关于照片的记账信息

882
00:48:39,119 --> 00:48:43,290
比如 如果你提出RAW加HEIC的请求

883
00:48:43,490 --> 00:48:45,158
你应该会得到两张照片

884
00:48:45,492 --> 00:48:49,496
那么照片计数访问器将会告诉你
这是第一张还是第二张照片？

885
00:48:50,697 --> 00:48:53,066
如果这个照片是同等捕捉的一部分

886
00:48:53,133 --> 00:48:57,371
比如自动曝光三或四个不同的EV值

887
00:48:57,905 --> 00:49:01,608
它会告诉你哪个设置会导致这种结果

888
00:49:01,675 --> 00:49:04,311
以及它的序列号

889
00:49:04,378 --> 00:49:07,080
以及镜头稳定性是否良好

890
00:49:09,116 --> 00:49:12,719
AVCapturePhoto
还支持不同格式的转换

891
00:49:12,786 --> 00:49:16,523
所以它很友好 并能移动到

892
00:49:16,590 --> 00:49:18,425
你用来处理图片的其它框架上

893
00:49:18,892 --> 00:49:22,896
首先也是最重要的
它支持数据表示法的转换

894
00:49:22,963 --> 00:49:24,531
如果你指向写入文件

895
00:49:24,865 --> 00:49:28,168
并且它还可以生成一个CGImage
无论是全尺寸预览

896
00:49:28,902 --> 00:49:29,837
或是…

897
00:49:29,903 --> 00:49:32,172
抱歉 全尺寸图片或预览图片

898
00:49:33,974 --> 00:49:36,710
现在选择加入以获得AVCapturePhoto

899
00:49:36,777 --> 00:49:39,646
而不是CMSampleBuffer的机制是
你需要实施

900
00:49:39,713 --> 00:49:43,984
一个新代理方法
在你的AVCapture PhotoCapture委托中

901
00:49:44,051 --> 00:49:47,888
就是这里的这个 很简单
只有三个参数

902
00:49:48,121 --> 00:49:51,825
它提供AVCapturePhoto
和一个可选的报错

903
00:49:52,459 --> 00:49:56,029
现在 无论是否有报错
你总是能得到AVCapturePhoto

904
00:49:56,296 --> 00:49:58,699
并获得与之相关的尽可能多的信息

905
00:49:58,765 --> 00:50:00,968
即使没有支持像素数据

906
00:50:03,170 --> 00:50:07,674
接下来的两个很长的
代理方法被弃用了

907
00:50:07,975 --> 00:50:10,511
以引导你使用新的、更好的方法

908
00:50:11,044 --> 00:50:15,482
为获得RAW或未压缩或压缩图片

909
00:50:15,549 --> 00:50:19,820
我们经常会执行独立回调
didFinishProcessingPhoto

910
00:50:19,987 --> 00:50:21,722
将为你提供一个CMSampleBuffer

911
00:50:21,788 --> 00:50:25,492
或didFinishProcessingRawPhoto
将为你提供一个SampleBuffer

912
00:50:26,260 --> 00:50:29,096
你不需要这样
你再也不需要使用这些了

913
00:50:29,162 --> 00:50:33,467
你只需要使用那一个新的就可以了
将这两者包含在内了

914
00:50:35,335 --> 00:50:38,939
好的 在iOS 10中
我们支持以下格式

915
00:50:39,540 --> 00:50:41,842
对于压缩 你可以得到的就是JPEG

916
00:50:42,209 --> 00:50:46,513
对于未压缩格式
你可以选择得到420或BGRA

917
00:50:46,713 --> 00:50:48,715
当然了 我们支持Bayer RAW

918
00:50:49,416 --> 00:50:53,987
现在在iOS 11中
除了添加HEVC支持

919
00:50:54,254 --> 00:50:56,256
我们还添加了一个新规格

920
00:50:56,757 --> 00:51:00,394
你所请求的每一个图片格式

921
00:51:00,460 --> 00:51:04,064
也由一个文件容器格式所支持

922
00:51:04,131 --> 00:51:09,102
换句话说 含蓄地 你所捕捉的每一张
图像都被集装箱化了

923
00:51:09,770 --> 00:51:13,907
对于HEVC 隐含的容器是HEIC

924
00:51:14,308 --> 00:51:18,045
对于JPEG是JFIF
对于未压缩格式是TIFF

925
00:51:18,512 --> 00:51:21,248
对于RAW格式 跟以前一样是DNG

926
00:51:21,782 --> 00:51:24,751
现在为什么文件集装箱化是一件好事？

927
00:51:25,452 --> 00:51:28,822
答案是性能
让我们通过一个案例解释一下

928
00:51:29,656 --> 00:51:32,759
那么这是你获得JPEG
并将其写入磁盘的老方法

929
00:51:33,794 --> 00:51:36,263
PhotoOutput
会给你提交一个SampleBuffer

930
00:51:36,430 --> 00:51:39,032
一个全尺寸图像和一个预览图像

931
00:51:40,067 --> 00:51:42,569
且它会附加一些元数据 比如Exif

932
00:51:42,970 --> 00:51:45,105
如果你想以任何方式改变那个

933
00:51:45,172 --> 00:51:48,175
你需要等待 直到它提交回调

934
00:51:48,342 --> 00:51:50,577
然后你会获得包含Exif的附件

935
00:51:50,744 --> 00:51:54,248
篡改它并重新将其添加到
SampleBuffer

936
00:51:54,781 --> 00:51:58,652
然后当把它写入磁盘时
你要调用PhotoOutput的

937
00:51:59,119 --> 00:52:03,590
JPEGDataPhotoRepresentation
并将其传入两种缓冲

938
00:52:04,324 --> 00:52:07,261
结果是一个JPEG数据
已准备写入磁盘

939
00:52:07,861 --> 00:52:11,632
那么在代码中 它看起来很简单
很多都是在底层发生的

940
00:52:12,032 --> 00:52:15,702
因为我们合并了预览图像
和嵌入的缩略图

941
00:52:15,903 --> 00:52:20,407
我们必须从屏幕尺寸大小的图片
缩减为小尺寸图片

942
00:52:20,874 --> 00:52:24,778
压缩到JPEG
合并全部的Exif变更

943
00:52:24,845 --> 00:52:30,784
并重写全尺寸图像
那么要执行大量的缩减和压缩

944
00:52:31,084 --> 00:52:33,754
只因为你想包含一个缩略图

945
00:52:33,954 --> 00:52:36,990
和你的图像一起
并稍微修改一下元数据

946
00:52:37,324 --> 00:52:38,559
一点也没效率

947
00:52:39,760 --> 00:52:41,929
现在通过新方式
AVCapturePhoto

948
00:52:42,329 --> 00:52:46,133
让你提前指定在容器中想要得到什么

949
00:52:47,067 --> 00:52:51,071
如果它第一次有足够的信息
来准备文件容器

950
00:52:51,338 --> 00:52:53,874
那么就会在你获得
第一次回调之前完成

951
00:52:55,075 --> 00:52:57,444
你的实现方式是填写一些附加功能

952
00:52:57,511 --> 00:52:59,179
在AVCapturePhotoSettings中

953
00:52:59,580 --> 00:53:02,516
这次你可以提前指定
你想要的编解码器

954
00:53:02,583 --> 00:53:04,551
以及文件类型（可选）

955
00:53:05,485 --> 00:53:09,156
你指定要添加的元数据 如GPS位置

956
00:53:09,223 --> 00:53:12,426
你现在甚至可以在发起请求之前实现了

957
00:53:12,860 --> 00:53:15,128
你还可以告诉它
“我想要一个嵌入式缩略图

958
00:53:15,195 --> 00:53:19,633
并且我希望它采用这些规格 ”

959
00:53:21,034 --> 00:53:24,404
然后你就把你的请求提交给
AVCapturePhotoOutput 最终

960
00:53:24,705 --> 00:53:29,643
它会给你的委托提供一个
AVCapturePhoto作为结果

961
00:53:30,177 --> 00:53:34,248
这个AVCapturePhoto由已经存在于
HEIC容器中的东西支持

962
00:53:34,781 --> 00:53:36,884
它已经在分格中被压缩了

963
00:53:37,451 --> 00:53:40,988
它已经嵌入了那个
你请求它嵌入的缩略图

964
00:53:41,054 --> 00:53:43,991
它已经把元数据放在了正确的位置

965
00:53:44,825 --> 00:53:47,828
所以你要把它写入
磁盘的最后一个调用

966
00:53:48,595 --> 00:53:51,031
photo.fileDataRepresentation

967
00:53:51,431 --> 00:53:53,600
比之前例子中的那个简单多了

968
00:53:54,168 --> 00:53:59,306
它要做的就是一个简单的字节复制
复制后备存储器的NSData

969
00:53:59,806 --> 00:54:03,010
没有额外的压缩或缩减或其它操作

970
00:54:03,076 --> 00:54:04,778
它都提前做好了

971
00:54:05,112 --> 00:54:08,415
这样更加高效
尤其是当我们处理HEIF时

972
00:54:08,749 --> 00:54:11,084
有必要获得全部性能

973
00:54:11,151 --> 00:54:14,254
就是我之前谈到过的
很棒的分格格式的性能

974
00:54:15,722 --> 00:54:19,660
现在让我们切换到一些
与HEVC和HEIF相关的性能注意事项

975
00:54:20,360 --> 00:54:24,998
第一个是要用静态捕捉
所拍摄的照片做什么

976
00:54:26,366 --> 00:54:30,637
当你在拍摄电影时拍摄HEIC照片时
你应该意识到

977
00:54:30,904 --> 00:54:33,273
与压缩视频用的是同一个硬件模块

978
00:54:33,340 --> 00:54:37,244
这就是执行H.264或HEVC压缩的硬件

979
00:54:37,878 --> 00:54:43,116
如果你想编码HEIC文件
它也得负责执行

980
00:54:43,350 --> 00:54:45,619
当HEVC为压缩格式时

981
00:54:46,420 --> 00:54:49,423
硬件模块可能非常繁忙

982
00:54:49,489 --> 00:54:54,294
如果你正在捕捉高带域的视频
比如4k 30或1080p 60

983
00:54:55,362 --> 00:54:59,666
视频有即时截止时间
所以它比静态图像的优先级高

984
00:55:00,000 --> 00:55:03,136
意思是可能会等很长时间
才能得到静态结果

985
00:55:03,770 --> 00:55:08,542
并且也意味着它们比原本要大20%

986
00:55:08,842 --> 00:55:11,478
因为编码器忙于使用全部功能

987
00:55:11,545 --> 00:55:14,381
如果它不需要赶那个
即时的截止时间的话

988
00:55:14,448 --> 00:55:16,216
每秒30帧或60帧

989
00:55:16,950 --> 00:55:19,620
那么我们的建议是
如果你正在捕捉视频

990
00:55:19,887 --> 00:55:24,424
同时也捕捉静态图像
你应该使用JPEG以让照片

991
00:55:24,591 --> 00:55:29,663
离开HEVC编码器
让编码器对视频尽可能地可用

992
00:55:32,299 --> 00:55:35,802
另一个考虑是HEVC和HEIF连发

993
00:55:36,069 --> 00:55:39,039
这是你用力压住按钮的位置
并尝试获得

994
00:55:40,407 --> 00:55:45,345
一个恒定的帧速率 也许每秒10帧
对于图像捕捉来说

995
00:55:46,246 --> 00:55:49,416
HEVC编码 很明显
它所做的比JPEG要更多

996
00:55:49,716 --> 00:55:53,887
它要提交一个比JPEG大小
要小一半多的文件

997
00:55:54,288 --> 00:55:56,657
因此 HEVC编码需要的时间也更长

998
00:55:57,324 --> 00:56:02,129
现在我们已经基准了
HEVC HEIF

999
00:56:02,663 --> 00:56:09,169
可以达到连拍的最小请求10 fps
我们感到很棒

1000
00:56:09,469 --> 00:56:12,306
但如果你需要捕捉比那更高的帧速率

1001
00:56:12,472 --> 00:56:15,509
我们的建议是返回JPEG来进行连拍

1002
00:56:17,845 --> 00:56:21,014
我们今天了解了很多压缩相关的信息
我觉得我是玩忽职守

1003
00:56:21,081 --> 00:56:27,487
如果我不在WWDC与你们分享我的想法
这毕竟是一场压缩演讲

1004
00:56:27,988 --> 00:56:31,058
那么我不能把这个就这样挂在那儿

1005
00:56:31,825 --> 00:56:33,794
全球开发者大会

1006
00:56:34,628 --> 00:56:35,662
九个音节

1007
00:56:36,797 --> 00:56:40,634
W-W-D-C 八个音节

1008
00:56:41,068 --> 00:56:43,971
这就像是有史以来最糟糕的压缩格式

1009
00:56:44,037 --> 00:56:48,709
它失真了 它像是.1对1压缩比

1010
00:56:48,775 --> 00:56:50,577
甚至比无失真的JPEG更糟糕

1011
00:56:51,011 --> 00:56:53,247
那么作为一个服务给我

1012
00:56:53,614 --> 00:56:55,282
在大会余下的时间里

1013
00:56:55,349 --> 00:57:00,921
请只参加那些Dub-Dub的大会
或者我们也可以接受Wuh-Duck

1014
00:57:03,557 --> 00:57:05,592
好的 让我们总结一下
我们今天学到的内容

1015
00:57:06,793 --> 00:57:11,798
HEVC电影的一般性内容
比H.264要小达40%

1016
00:57:11,865 --> 00:57:15,569
对于在iOS上捕捉的内容也小2倍

1017
00:57:16,170 --> 00:57:20,874
同时 iOS 11和High Sierra
均支持HEVC播放

1018
00:57:21,041 --> 00:57:23,577
有时是软件 有时是硬件

1019
00:57:24,178 --> 00:57:27,447
并且创建HEVC内容
你需要选择加入

1020
00:57:27,514 --> 00:57:30,384
新的捕捉API或新的导出API

1021
00:57:30,884 --> 00:57:35,756
此外 我们了解了HEIC文件
它们比JPEG文件小一倍

1022
00:57:36,256 --> 00:57:40,761
并且iOS 11和macOS上
均支持那种解码

1023
00:57:41,128 --> 00:57:46,500
捕捉仅支持iOS并且配有A10芯片

1024
00:57:46,667 --> 00:57:49,536
你可以通过
新的AVCapturePhoto界面实现

1025
00:57:50,671 --> 00:57:54,675
要获取更多信息
这是今天演讲的URL

1026
00:57:56,276 --> 00:57:59,246
我还想提供一些
与本场演讲相关的姐妹演讲

1027
00:57:59,479 --> 00:58:02,649
列表中的第一个 高效率图档格式

1028
00:58:02,716 --> 00:58:04,451
是直奔视频而去的

1029
00:58:04,952 --> 00:58:10,057
这场演讲是真的深入研究
HEIF文件中的位元的

1030
00:58:10,424 --> 00:58:14,328
这是一场很棒很棒的演讲
你绝对应该听一下

1031
00:58:14,628 --> 00:58:16,196
演讲人是戴维德 所以你可以

1032
00:58:16,263 --> 00:58:18,465
同时感受一下很棒的意大利口音

1033
00:58:19,466 --> 00:58:23,036
还有 HEIF和HEVC简介
是在周二

1034
00:58:23,237 --> 00:58:27,407
提供了一个关于我们今天
所讲内容的较高层级的介绍

1035
00:58:27,708 --> 00:58:31,178
最后是深度演讲 我引用了很多次

1036
00:58:31,245 --> 00:58:33,814
它们有许多针对我们在HEIF中

1037
00:58:33,881 --> 00:58:36,783
存储深度的辅助图像格式的补充

1038
00:58:38,318 --> 00:58:40,153
谢谢大家 祝你们在接下来的
大会中度过美好时光

