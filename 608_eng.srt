1
00:00:30,076 --> 00:00:32,006
welcome to our talk on using

2
00:00:32,006 --> 00:00:32,946
Metal 2 for Compute.

3
00:00:33,886 --> 00:00:35,296
My name is Anna Tikhonova.

4
00:00:35,296 --> 00:00:36,686
I'm an engineer on the GPU

5
00:00:36,686 --> 00:00:37,976
Software Team, so let's begin.

6
00:00:42,156 --> 00:00:44,046
The Metal 2 echo system is so

7
00:00:44,046 --> 00:00:45,836
much more than the Metal API and

8
00:00:45,836 --> 00:00:46,356
the language.

9
00:00:46,796 --> 00:00:48,946
We also have the GPU Tools and

10
00:00:48,946 --> 00:00:50,336
we have the MetalKit and Metal

11
00:00:50,336 --> 00:00:51,586
Performance Shaders frameworks.

12
00:00:53,006 --> 00:00:54,146
You might know Metal as this

13
00:00:54,496 --> 00:00:56,376
great technology for developing

14
00:00:56,376 --> 00:00:57,566
high-end games and graphics.

15
00:00:58,396 --> 00:00:59,616
But it can also be used for

16
00:00:59,616 --> 00:01:00,506
Compute processing.

17
00:01:01,626 --> 00:01:02,806
In fact, the Compute side of

18
00:01:02,806 --> 00:01:04,616
Metal is so powerful and

19
00:01:04,616 --> 00:01:06,526
flexible that the Metal

20
00:01:06,526 --> 00:01:08,196
Performance Shaders framework is

21
00:01:08,196 --> 00:01:09,326
built completely on top of

22
00:01:09,406 --> 00:01:09,756
Compute.

23
00:01:11,026 --> 00:01:12,486
And in this session, we'll talk

24
00:01:12,486 --> 00:01:14,076
about what's new in the Metal

25
00:01:14,076 --> 00:01:15,176
Performance Shaders framework.

26
00:01:17,956 --> 00:01:19,596
We introduced the Metal

27
00:01:19,596 --> 00:01:21,026
Performers Shaders framework, or

28
00:01:21,026 --> 00:01:22,756
MPS in 2015.

29
00:01:23,486 --> 00:01:24,546
And the videos of our past

30
00:01:24,546 --> 00:01:25,916
sessions are available on our

31
00:01:25,916 --> 00:01:28,906
developer website.

32
00:01:29,266 --> 00:01:30,686
MPS uses the compute power of

33
00:01:30,686 --> 00:01:33,446
the GPU to bring GPU accelerated

34
00:01:33,446 --> 00:01:33,816
primitives.

35
00:01:34,286 --> 00:01:35,886
For image processing, linear

36
00:01:35,926 --> 00:01:37,416
algebra and machine learning.

37
00:01:39,146 --> 00:01:40,416
The framework is optimized for

38
00:01:40,416 --> 00:01:42,336
iOS and we're happy to announce

39
00:01:42,336 --> 00:01:44,096
that this year we're also

40
00:01:44,096 --> 00:01:44,836
bringing MPS to the Mac.

41
00:01:45,516 --> 00:01:49,786
[ Applause ]

42
00:01:50,286 --> 00:01:50,716
Thank you.

43
00:01:51,926 --> 00:01:53,366
The entire feature set is

44
00:01:53,366 --> 00:01:55,616
available in both iOS and macOS.

45
00:01:55,616 --> 00:01:58,476
So let's begin with a quick

46
00:01:58,476 --> 00:02:00,336
update on our image processing

47
00:02:00,336 --> 00:02:00,686
support.

48
00:02:02,046 --> 00:02:03,866
So here's a list of all of the

49
00:02:03,866 --> 00:02:05,576
primitives for image processing

50
00:02:05,696 --> 00:02:07,486
that we had available in iOS 10.

51
00:02:08,106 --> 00:02:09,675
So there's Convolution, Gaussian

52
00:02:09,675 --> 00:02:11,586
Blur, Lanczos Resampling, just

53
00:02:11,586 --> 00:02:12,196
to name a few.

54
00:02:13,126 --> 00:02:14,726
They're all now available in

55
00:02:14,726 --> 00:02:14,996
macOS.

56
00:02:16,146 --> 00:02:17,656
And this year we're bringing you

57
00:02:17,656 --> 00:02:18,926
four new image processing

58
00:02:18,926 --> 00:02:19,336
primitives.

59
00:02:20,466 --> 00:02:21,816
The Image Keypoints primitive

60
00:02:22,206 --> 00:02:24,316
can be used -- is often used in

61
00:02:24,316 --> 00:02:26,256
computer vision algorithms such

62
00:02:26,256 --> 00:02:28,596
as image stabilization and

63
00:02:28,596 --> 00:02:29,926
Bilinear Rescale, Image

64
00:02:29,926 --> 00:02:31,726
Statistics, and Element-wise

65
00:02:31,726 --> 00:02:33,246
Arithmetic Operators, are

66
00:02:33,326 --> 00:02:34,826
commonly used to pre-process

67
00:02:34,826 --> 00:02:35,156
images.

68
00:02:35,466 --> 00:02:36,386
For example, in machine

69
00:02:36,386 --> 00:02:36,656
learning.

70
00:02:37,556 --> 00:02:38,926
And the arithmetic filters also

71
00:02:38,926 --> 00:02:40,446
support broadcasting operations.

72
00:02:41,256 --> 00:02:42,716
Which, for example, allow you to

73
00:02:42,716 --> 00:02:44,766
add a 2D image or the 1D image.

74
00:02:46,176 --> 00:02:48,406
So that's it for our very quick

75
00:02:48,406 --> 00:02:49,606
update on image processing.

76
00:02:49,986 --> 00:02:51,286
And now let's talk about the new

77
00:02:51,286 --> 00:02:52,386
Linear Algebra operations.

78
00:02:54,286 --> 00:02:55,686
Without support, Matrix

79
00:02:55,686 --> 00:02:57,436
Multiplication, Matrix Vector

80
00:02:57,436 --> 00:02:59,736
Multiplication, and Triangular

81
00:03:00,076 --> 00:03:01,866
Matrix Factorization and Linear

82
00:03:01,866 --> 00:03:02,306
Solvers.

83
00:03:05,356 --> 00:03:06,376
To support Linear Algebra

84
00:03:06,376 --> 00:03:09,256
operations, we now have multiple

85
00:03:09,256 --> 00:03:10,356
new data representations.

86
00:03:11,066 --> 00:03:13,076
First, we have the MPSVector

87
00:03:13,076 --> 00:03:15,186
object which interprets the data

88
00:03:15,186 --> 00:03:16,496
in a metal buffer as a

89
00:03:16,496 --> 00:03:17,416
one-dimensional array.

90
00:03:19,106 --> 00:03:21,506
And we have an MPSMatrix object

91
00:03:22,076 --> 00:03:23,276
which interprets the data in a

92
00:03:23,276 --> 00:03:24,886
metal buffer as a rectangular

93
00:03:24,886 --> 00:03:25,156
array.

94
00:03:25,886 --> 00:03:27,656
And MPS matrices are in role

95
00:03:27,656 --> 00:03:28,176
major order.

96
00:03:28,956 --> 00:03:30,166
And you can think of both

97
00:03:30,166 --> 00:03:32,956
MPSVectors and MPSMatrices as

98
00:03:33,456 --> 00:03:34,736
wrappers around user data

99
00:03:34,736 --> 00:03:35,096
buffers.

100
00:03:37,436 --> 00:03:39,296
And we also support a temporary

101
00:03:39,296 --> 00:03:40,926
variance of MPSMatrix.

102
00:03:42,296 --> 00:03:45,196
MPS images -- temporary images

103
00:03:45,196 --> 00:03:47,056
and MPSTemporaryMatrices are

104
00:03:47,056 --> 00:03:48,666
allocated from a Metal heap

105
00:03:48,906 --> 00:03:49,956
associated with a command

106
00:03:49,956 --> 00:03:50,216
buffer.

107
00:03:50,766 --> 00:03:51,856
And they are called temporary

108
00:03:52,226 --> 00:03:54,066
because their lifespan is

109
00:03:54,216 --> 00:03:55,996
limited to the lifetime of the

110
00:03:55,996 --> 00:03:56,516
command buffer.

111
00:03:57,546 --> 00:03:58,866
And we recommend you use

112
00:03:58,896 --> 00:04:00,176
temporary images and matrices

113
00:04:00,636 --> 00:04:02,526
for most of your intermediate

114
00:04:03,006 --> 00:04:03,206
storage.

115
00:04:04,316 --> 00:04:07,416
Both MPSVector and MPSMatrix

116
00:04:07,576 --> 00:04:09,116
support a number of input types.

117
00:04:09,646 --> 00:04:11,596
We support single-precision and

118
00:04:11,596 --> 00:04:14,236
half-precision input types and a

119
00:04:14,236 --> 00:04:15,266
floating-point input types.

120
00:04:15,756 --> 00:04:17,696
And 16-bits and 8-bit signed

121
00:04:17,986 --> 00:04:19,125
integer input types.

122
00:04:21,016 --> 00:04:22,136
And now let's take a look at how

123
00:04:22,136 --> 00:04:24,446
we can create an MPSVector of

124
00:04:24,536 --> 00:04:24,886
size N.

125
00:04:24,886 --> 00:04:26,856
So if you don't already have a

126
00:04:26,856 --> 00:04:28,346
Metal buffer, you need to create

127
00:04:28,346 --> 00:04:28,606
one.

128
00:04:29,666 --> 00:04:30,666
And then you need to create a

129
00:04:30,666 --> 00:04:31,686
descriptor for your vector.

130
00:04:32,526 --> 00:04:34,546
And note here that you specify

131
00:04:34,726 --> 00:04:36,086
the length of the vector.

132
00:04:36,666 --> 00:04:38,146
That's because the vector can be

133
00:04:38,146 --> 00:04:39,946
made from a portion of the

134
00:04:39,946 --> 00:04:40,926
original Metal buffer.

135
00:04:41,716 --> 00:04:43,236
And other related offsets can be

136
00:04:43,236 --> 00:04:44,556
set in a kernel that will use

137
00:04:44,556 --> 00:04:45,016
this vector.

138
00:04:45,996 --> 00:04:47,406
And then the last step is

139
00:04:47,466 --> 00:04:49,586
creating a vector from the

140
00:04:49,586 --> 00:04:50,906
buffer with a descriptor.

141
00:04:52,966 --> 00:04:53,976
And now let's take a look at how

142
00:04:53,976 --> 00:04:56,256
you can create an MPSMatrix with

143
00:04:56,326 --> 00:04:57,906
M rows and N columns.

144
00:04:59,516 --> 00:05:00,906
So it's very similar to the way

145
00:05:00,906 --> 00:05:02,916
you would create MPSVector, but

146
00:05:02,916 --> 00:05:04,006
there's just a few things we

147
00:05:04,006 --> 00:05:04,726
want to mention.

148
00:05:06,176 --> 00:05:08,256
We provide a convenient API that

149
00:05:08,256 --> 00:05:09,606
you can use to find the

150
00:05:09,606 --> 00:05:11,966
recommended bytes per row value

151
00:05:12,556 --> 00:05:13,756
for sizing your Metal buffers.

152
00:05:14,666 --> 00:05:15,716
And if you choose to use the

153
00:05:15,796 --> 00:05:17,246
API, this is how you would

154
00:05:17,246 --> 00:05:18,816
create a metal buffer with this

155
00:05:18,816 --> 00:05:19,596
recommended value.

156
00:05:20,596 --> 00:05:22,106
And using this API is completely

157
00:05:22,106 --> 00:05:24,416
optional, but recommended for

158
00:05:24,416 --> 00:05:25,136
better performance.

159
00:05:25,986 --> 00:05:27,156
And then the rest is simple.

160
00:05:28,256 --> 00:05:29,356
You create a descriptor for your

161
00:05:29,356 --> 00:05:30,886
matrix, and then you create a

162
00:05:30,886 --> 00:05:32,346
matrix with a descriptor.

163
00:05:34,936 --> 00:05:36,506
And now that we talked about the

164
00:05:36,506 --> 00:05:37,806
data presentations, now let's

165
00:05:37,806 --> 00:05:39,046
talk about the primitives.

166
00:05:39,896 --> 00:05:41,176
So for Matrix-Matrix and

167
00:05:41,176 --> 00:05:43,136
Matrix-Vector multiplication our

168
00:05:43,136 --> 00:05:44,586
API is modeled after the

169
00:05:44,586 --> 00:05:46,036
standard BLAS GEMM and GEMV

170
00:05:46,036 --> 00:05:46,766
interfaces.

171
00:05:47,646 --> 00:05:48,846
And for triangular matrix

172
00:05:48,846 --> 00:05:50,196
vectorization and linear

173
00:05:50,196 --> 00:05:52,216
solvers, our API is modeled

174
00:05:52,216 --> 00:05:53,246
after standard LAPACK

175
00:05:53,246 --> 00:05:54,916
decomposition and solve

176
00:05:54,916 --> 00:05:55,486
interfaces.

177
00:05:55,846 --> 00:05:57,036
So if you're familiar with those

178
00:05:57,036 --> 00:05:59,106
interfaces our API will look

179
00:05:59,106 --> 00:06:00,526
very familiar to you as well.

180
00:06:02,576 --> 00:06:04,216
And now let's take a look at a

181
00:06:04,216 --> 00:06:05,546
very simple code example.

182
00:06:05,836 --> 00:06:07,536
So we'll be doing matrix

183
00:06:07,536 --> 00:06:08,996
multiplication and computing

184
00:06:08,996 --> 00:06:10,426
just C = A times B.

185
00:06:10,426 --> 00:06:12,716
So first we need to create our

186
00:06:12,716 --> 00:06:14,476
matrices A, B and C.

187
00:06:14,706 --> 00:06:15,646
But I know you know how to do

188
00:06:15,646 --> 00:06:16,786
this, I showed you in a previous

189
00:06:16,836 --> 00:06:18,306
slide, so let's move on.

190
00:06:19,336 --> 00:06:21,116
Now we want to run matrix

191
00:06:21,116 --> 00:06:22,596
multiplication on the GPU.

192
00:06:23,886 --> 00:06:25,386
So first we do our usual Metal

193
00:06:25,466 --> 00:06:27,446
setup to getting a device, a

194
00:06:27,446 --> 00:06:29,076
command queue, and a command

195
00:06:29,076 --> 00:06:29,356
buffer.

196
00:06:29,356 --> 00:06:31,766
And then we need to create our

197
00:06:31,846 --> 00:06:33,186
matrix multiplication kernel.

198
00:06:33,766 --> 00:06:35,196
And note here that you specify

199
00:06:35,196 --> 00:06:36,296
the size of the result.

200
00:06:36,826 --> 00:06:38,216
That's because this kernel can

201
00:06:38,216 --> 00:06:39,896
operate on subregions of

202
00:06:39,896 --> 00:06:40,436
matrices.

203
00:06:41,066 --> 00:06:45,576
And then we encode this kernel

204
00:06:45,656 --> 00:06:47,056
to the GPU and tell it to start

205
00:06:47,056 --> 00:06:47,476
doing the work.

206
00:06:47,476 --> 00:06:51,036
And we already have sample code

207
00:06:51,036 --> 00:06:52,346
from Matrix multiplication

208
00:06:52,586 --> 00:06:53,906
available on our developer

209
00:06:53,906 --> 00:06:56,096
website, and the sample code for

210
00:06:56,096 --> 00:06:57,896
triangular matrix vectorization

211
00:06:58,206 --> 00:06:59,556
and solving a system of linear

212
00:06:59,556 --> 00:07:00,986
equations is coming very soon.

213
00:07:03,026 --> 00:07:05,526
So that's it for our -- for the

214
00:07:05,756 --> 00:07:07,026
linear algebra operations.

215
00:07:07,386 --> 00:07:08,996
Let's now move on to the next

216
00:07:08,996 --> 00:07:10,756
topic, which is Accelerating

217
00:07:10,756 --> 00:07:12,156
Machine Learning Primitives on

218
00:07:12,156 --> 00:07:12,636
the GPU.

219
00:07:14,116 --> 00:07:16,246
There are a number of

220
00:07:16,246 --> 00:07:17,906
machine-learning related talks

221
00:07:17,906 --> 00:07:19,156
at WWDC this year.

222
00:07:19,456 --> 00:07:20,346
And we are a part of the

223
00:07:20,346 --> 00:07:21,416
machine-learning community.

224
00:07:22,306 --> 00:07:23,586
And this slide shows the overall

225
00:07:23,586 --> 00:07:24,206
architecture.

226
00:07:25,086 --> 00:07:26,606
So as an application developer,

227
00:07:26,816 --> 00:07:27,876
you can add machine learning

228
00:07:27,876 --> 00:07:28,816
functionality to your

229
00:07:28,816 --> 00:07:30,956
applications by using high-level

230
00:07:30,996 --> 00:07:32,856
domain-specific frameworks such

231
00:07:32,856 --> 00:07:34,456
as division framework and the

232
00:07:34,456 --> 00:07:35,566
natural language processing

233
00:07:35,616 --> 00:07:37,496
framework, which rely on the

234
00:07:37,496 --> 00:07:38,366
Core ML framework.

235
00:07:39,216 --> 00:07:40,376
And the Core ML framework is

236
00:07:40,446 --> 00:07:42,316
powered by the accelerates

237
00:07:42,316 --> 00:07:44,076
framework BNNS primitives on the

238
00:07:44,076 --> 00:07:44,606
CPU.

239
00:07:45,066 --> 00:07:46,176
And by the machine learning --

240
00:07:47,066 --> 00:07:49,046
and by the Metal Performance

241
00:07:49,046 --> 00:07:51,946
Shaders framework on the GPU.

242
00:07:51,946 --> 00:07:52,706
But if you're writing an

243
00:07:52,706 --> 00:07:54,556
application that uses Metal,

244
00:07:54,906 --> 00:07:56,046
then you can use the MPS

245
00:07:56,046 --> 00:07:58,176
framework directly and I will

246
00:07:58,176 --> 00:07:59,386
show you how in this session.

247
00:08:01,666 --> 00:08:02,676
So let's start with what are we

248
00:08:02,736 --> 00:08:03,486
talking about here?

249
00:08:04,246 --> 00:08:05,116
What is deep learning?

250
00:08:05,366 --> 00:08:06,236
What is Machine learning?

251
00:08:07,686 --> 00:08:08,766
So imagine that this is you.

252
00:08:08,766 --> 00:08:11,436
And when you see an image, you

253
00:08:11,436 --> 00:08:13,076
know immediately what's depicted

254
00:08:13,076 --> 00:08:13,316
on it.

255
00:08:13,416 --> 00:08:13,956
It's a panda.

256
00:08:14,936 --> 00:08:16,686
But now think about all of the

257
00:08:16,686 --> 00:08:18,006
images on your iPhone.

258
00:08:18,596 --> 00:08:20,206
Or all of those pictures in your

259
00:08:20,206 --> 00:08:20,996
family albums.

260
00:08:21,606 --> 00:08:22,646
Or all of the images on the

261
00:08:22,646 --> 00:08:22,966
internet.

262
00:08:23,816 --> 00:08:27,096
No human can possibly -- can

263
00:08:27,096 --> 00:08:28,816
classify these many images.

264
00:08:29,086 --> 00:08:30,506
But deep-learning algorithms is

265
00:08:30,506 --> 00:08:32,056
designed specifically to do

266
00:08:32,056 --> 00:08:32,196
that.

267
00:08:33,186 --> 00:08:34,416
They can be used for sifting

268
00:08:34,416 --> 00:08:35,576
through large amounts of data

269
00:08:36,015 --> 00:08:37,866
and answering questions such as

270
00:08:37,996 --> 00:08:41,676
what is in this image?

271
00:08:42,236 --> 00:08:43,306
Deep learning algorithms have

272
00:08:43,405 --> 00:08:43,905
two phases.

273
00:08:44,206 --> 00:08:45,156
Training and inference.

274
00:08:45,426 --> 00:08:46,426
So let's talk about training

275
00:08:46,426 --> 00:08:46,736
first.

276
00:08:47,946 --> 00:08:49,246
And let's actually use an

277
00:08:49,246 --> 00:08:49,676
example.

278
00:08:49,676 --> 00:08:51,326
Let's train a system to classify

279
00:08:51,326 --> 00:08:51,716
images.

280
00:08:52,586 --> 00:08:53,536
So the training system to

281
00:08:53,536 --> 00:08:56,696
classify images, for example, if

282
00:08:56,696 --> 00:08:58,066
you want to have it recognize

283
00:08:58,126 --> 00:08:58,536
animals.

284
00:08:58,966 --> 00:09:00,516
Like, to have it recognize cats,

285
00:09:00,516 --> 00:09:02,126
you need to feed this system a

286
00:09:02,556 --> 00:09:04,196
large number of labeled images

287
00:09:04,196 --> 00:09:06,126
of cats and then rabbits, and

288
00:09:06,126 --> 00:09:07,336
all the other animals that you

289
00:09:07,336 --> 00:09:08,406
want your system to be able to

290
00:09:08,406 --> 00:09:08,866
recognize.

291
00:09:10,546 --> 00:09:12,316
And this training step is a

292
00:09:12,316 --> 00:09:13,916
one-time computationally

293
00:09:13,916 --> 00:09:16,476
expensive and labor-intensive

294
00:09:16,566 --> 00:09:16,786
step.

295
00:09:17,896 --> 00:09:19,076
And it's usually done offline.

296
00:09:19,696 --> 00:09:20,896
But the results of this training

297
00:09:20,896 --> 00:09:22,336
phase is trained parameters

298
00:09:23,306 --> 00:09:24,656
which are required for the next

299
00:09:24,656 --> 00:09:25,856
phase, the inference phase.

300
00:09:26,906 --> 00:09:28,096
This is when your system is

301
00:09:28,186 --> 00:09:30,156
presented with a new image that

302
00:09:30,156 --> 00:09:31,736
it has never seen before and it

303
00:09:31,736 --> 00:09:33,186
needs to classify, this is a

304
00:09:33,186 --> 00:09:33,396
cap.

305
00:09:35,126 --> 00:09:37,016
We provide view acceleration for

306
00:09:37,016 --> 00:09:38,306
the second phase; the inference

307
00:09:38,306 --> 00:09:38,526
phase.

308
00:09:39,096 --> 00:09:40,966
Specifically, last year we

309
00:09:40,966 --> 00:09:42,936
talked about the building blocks

310
00:09:43,056 --> 00:09:44,296
for building convolutional

311
00:09:44,296 --> 00:09:45,716
neural networks on the GPU for

312
00:09:45,716 --> 00:09:46,146
inference.

313
00:09:48,466 --> 00:09:50,176
So before we move onto any of

314
00:09:50,176 --> 00:09:51,966
the new features for machine

315
00:09:51,966 --> 00:09:52,826
learning that we brought you

316
00:09:52,856 --> 00:09:53,936
this year, we are going to

317
00:09:53,936 --> 00:09:55,136
review some of the core

318
00:09:55,136 --> 00:09:56,886
information that was covered in

319
00:09:56,886 --> 00:09:58,146
our last year's presentation.

320
00:09:58,736 --> 00:10:00,416
Such as, what are convolutional

321
00:10:00,416 --> 00:10:00,966
neural networks?

322
00:10:02,396 --> 00:10:04,326
And once we do that then we can

323
00:10:04,326 --> 00:10:06,056
talk about the new primitives

324
00:10:06,106 --> 00:10:06,836
that we've added for

325
00:10:06,836 --> 00:10:07,906
convolutional neural networks

326
00:10:07,966 --> 00:10:09,426
this year, and then we'll

327
00:10:09,426 --> 00:10:11,146
introduce the new, easy-to-use

328
00:10:11,516 --> 00:10:12,636
neural network graph API.

329
00:10:13,166 --> 00:10:14,746
And our last topic will be

330
00:10:14,746 --> 00:10:15,726
recurrent neural networks.

331
00:10:18,606 --> 00:10:20,976
So let's go into our recap.

332
00:10:21,106 --> 00:10:22,066
So what are convolutional neural

333
00:10:22,066 --> 00:10:22,366
networks?

334
00:10:24,446 --> 00:10:25,576
Convolutional neural networks

335
00:10:25,576 --> 00:10:27,426
are biologically inspired and

336
00:10:27,426 --> 00:10:28,836
designed to resemble the visual

337
00:10:28,836 --> 00:10:29,246
cortex.

338
00:10:29,796 --> 00:10:31,406
So let's think about how our

339
00:10:31,406 --> 00:10:33,076
brain processes visual inputs.

340
00:10:34,256 --> 00:10:35,636
The first hierarchy of neurons

341
00:10:35,736 --> 00:10:37,056
that receive information in the

342
00:10:37,056 --> 00:10:39,396
visual cortex is sensitive to

343
00:10:39,396 --> 00:10:40,786
specific edges and blobs of

344
00:10:40,886 --> 00:10:41,196
color.

345
00:10:42,366 --> 00:10:43,466
While the brain region's further

346
00:10:43,466 --> 00:10:45,966
down the visual pipeline respond

347
00:10:45,966 --> 00:10:47,596
to more complex structures such

348
00:10:47,596 --> 00:10:49,366
as faces of our friends or kinds

349
00:10:49,366 --> 00:10:50,166
of animals like cats.

350
00:10:50,996 --> 00:10:53,646
So in a similar way, CNNs are

351
00:10:53,646 --> 00:10:55,836
organized into a hierarchy of

352
00:10:55,836 --> 00:10:58,176
layers where high-level features

353
00:10:58,296 --> 00:10:59,836
are derived from low-level

354
00:10:59,836 --> 00:11:00,246
features.

355
00:11:01,156 --> 00:11:02,516
So the first few layers in your

356
00:11:02,516 --> 00:11:04,566
network respond to low-level

357
00:11:04,566 --> 00:11:06,766
features like edges and blobs of

358
00:11:06,826 --> 00:11:07,196
color.

359
00:11:07,886 --> 00:11:10,646
While subsequent layers respond

360
00:11:10,766 --> 00:11:12,516
to progressively more complex

361
00:11:12,616 --> 00:11:14,886
features such as faces.

362
00:11:16,106 --> 00:11:17,406
And I keep saying features.

363
00:11:17,846 --> 00:11:19,556
So think of a feature as a

364
00:11:19,556 --> 00:11:21,176
filter that filters your input

365
00:11:21,176 --> 00:11:22,706
data; that particular feature.

366
00:11:25,316 --> 00:11:26,906
And here's a list of all of the

367
00:11:26,976 --> 00:11:28,076
CNN primitives that we had

368
00:11:28,076 --> 00:11:29,356
available in iOS 10.

369
00:11:29,756 --> 00:11:31,806
And in this recap I will be just

370
00:11:31,806 --> 00:11:33,686
talking about the core

371
00:11:34,176 --> 00:11:35,146
convolution layer.

372
00:11:35,216 --> 00:11:36,426
The core building block of a

373
00:11:36,546 --> 00:11:36,756
CNN.

374
00:11:36,756 --> 00:11:39,046
And the rest of these primitives

375
00:11:39,106 --> 00:11:40,906
are covered in great detail in

376
00:11:40,906 --> 00:11:42,266
our presentation -- in our

377
00:11:42,266 --> 00:11:43,076
documentation.

378
00:11:43,196 --> 00:11:44,566
So Pooling, Fully-Connected and

379
00:11:44,616 --> 00:11:45,156
SoftMax.

380
00:11:45,746 --> 00:11:46,856
You can find information on

381
00:11:46,856 --> 00:11:47,056
those.

382
00:11:48,626 --> 00:11:50,386
So let's talk about the core

383
00:11:50,386 --> 00:11:51,186
building block.

384
00:11:52,596 --> 00:11:54,216
So the function of this core

385
00:11:54,216 --> 00:11:56,196
convolution layer is to

386
00:11:56,196 --> 00:11:57,766
recognize features in the input

387
00:11:57,766 --> 00:11:58,906
data and it's called a

388
00:11:58,906 --> 00:12:01,076
convolution layer because it

389
00:12:01,126 --> 00:12:02,576
performs a convolution on its

390
00:12:02,576 --> 00:12:02,846
input.

391
00:12:03,916 --> 00:12:05,246
So let's recall how regular

392
00:12:05,246 --> 00:12:06,076
convolution works.

393
00:12:06,906 --> 00:12:08,146
You have your inputs, your

394
00:12:08,186 --> 00:12:09,366
outputs and the filter.

395
00:12:10,366 --> 00:12:12,386
And to convole a filter with the

396
00:12:12,386 --> 00:12:14,506
input data you need to multiply

397
00:12:14,756 --> 00:12:16,626
each value in your filter with

398
00:12:16,626 --> 00:12:18,446
the value in the input data and

399
00:12:18,446 --> 00:12:19,686
combine that information to

400
00:12:19,686 --> 00:12:21,106
compute a single output value.

401
00:12:22,046 --> 00:12:23,536
And you do the same for the rest

402
00:12:23,636 --> 00:12:25,166
of the output pixels.

403
00:12:27,596 --> 00:12:29,856
And now the convolution layer is

404
00:12:29,856 --> 00:12:31,606
a generalization of regular

405
00:12:31,606 --> 00:12:32,226
convolution.

406
00:12:32,396 --> 00:12:34,666
It allows you to have multiple

407
00:12:34,666 --> 00:12:35,136
filters.

408
00:12:35,526 --> 00:12:37,536
So you have as many filters as

409
00:12:37,536 --> 00:12:38,916
you have output channels -- or

410
00:12:38,916 --> 00:12:39,816
16 in this case.

411
00:12:41,666 --> 00:12:43,046
And these are the filters which

412
00:12:43,046 --> 00:12:44,656
are going to be filtering input

413
00:12:44,656 --> 00:12:46,006
data for particular features.

414
00:12:47,726 --> 00:12:49,016
Now imagine that you're working

415
00:12:49,016 --> 00:12:50,266
with RGB data.

416
00:12:50,356 --> 00:12:52,186
So you actually have three

417
00:12:52,186 --> 00:12:53,266
channels in your input.

418
00:12:54,156 --> 00:12:56,276
And just because how CNNs work,

419
00:12:56,476 --> 00:12:58,816
this means you need three sets

420
00:12:58,886 --> 00:13:00,156
of 16 filters.

421
00:13:00,866 --> 00:13:02,576
One set for each input channel.

422
00:13:03,856 --> 00:13:05,916
And then these filters are

423
00:13:05,916 --> 00:13:07,296
applied to the input data

424
00:13:08,486 --> 00:13:09,036
separately.

425
00:13:09,036 --> 00:13:10,816
And then the final step combines

426
00:13:10,816 --> 00:13:12,386
all of this information to

427
00:13:12,386 --> 00:13:13,796
compute a single output pixel.

428
00:13:15,516 --> 00:13:17,156
So that's it for our recap of

429
00:13:17,156 --> 00:13:18,006
the convolution layer.

430
00:13:18,536 --> 00:13:19,526
Now let's talk about the new

431
00:13:19,576 --> 00:13:20,846
primitives we've added for

432
00:13:20,846 --> 00:13:22,026
convolutional neural networks.

433
00:13:22,116 --> 00:13:25,176
So as you can see, we've added

434
00:13:25,176 --> 00:13:25,686
quite a few.

435
00:13:27,736 --> 00:13:29,096
And I'll be talking about the

436
00:13:29,096 --> 00:13:31,476
ones highlighted in yellow, but

437
00:13:31,476 --> 00:13:33,206
the rest of them like L2Norm

438
00:13:33,256 --> 00:13:34,686
Pooling, Resampling,

439
00:13:34,686 --> 00:13:36,086
Up-sampling, they will all be

440
00:13:36,086 --> 00:13:37,396
covered in our documentation.

441
00:13:39,286 --> 00:13:40,986
So let's talk about updates to

442
00:13:40,986 --> 00:13:42,786
our core convolution layer.

443
00:13:43,916 --> 00:13:45,146
We used to support only single

444
00:13:45,186 --> 00:13:46,716
precision floating-point weight

445
00:13:46,716 --> 00:13:47,026
types.

446
00:13:47,466 --> 00:13:49,076
And now to help you reduce the

447
00:13:49,076 --> 00:13:51,126
memory footprint and to prove

448
00:13:51,126 --> 00:13:51,966
the performance of your

449
00:13:51,966 --> 00:13:52,326
networks.

450
00:13:52,876 --> 00:13:54,546
We also support half-precision

451
00:13:54,546 --> 00:13:56,466
floating points, 8-bit integer,

452
00:13:56,806 --> 00:13:58,076
and binary weight types.

453
00:13:59,496 --> 00:14:01,006
We used to support only standard

454
00:14:01,006 --> 00:14:02,676
convolution and now we also

455
00:14:02,736 --> 00:14:03,926
support binary and XNOR

456
00:14:03,926 --> 00:14:04,646
convolution.

457
00:14:04,986 --> 00:14:06,096
Dilated convolution.

458
00:14:06,096 --> 00:14:08,406
Sub-pixel convolution and

459
00:14:08,406 --> 00:14:09,366
convolution transpose

460
00:14:09,366 --> 00:14:09,996
operations.

461
00:14:11,056 --> 00:14:12,156
And many of these are

462
00:14:12,156 --> 00:14:13,716
orthogonal, so you can even have

463
00:14:14,006 --> 00:14:15,946
dilated sub-pixel convolution if

464
00:14:15,946 --> 00:14:16,276
you want.

465
00:14:17,706 --> 00:14:18,486
So let's go through them

466
00:14:18,486 --> 00:14:19,046
one-by-one.

467
00:14:20,806 --> 00:14:22,216
Binary and XNOR convolution

468
00:14:22,256 --> 00:14:24,276
perform the same exact operation

469
00:14:24,306 --> 00:14:26,336
as regular convolution but they

470
00:14:26,336 --> 00:14:28,016
do so with improved performance

471
00:14:28,476 --> 00:14:29,566
and great space savings.

472
00:14:30,016 --> 00:14:31,726
So in regular convolution, you

473
00:14:31,726 --> 00:14:33,546
may have floating point inputs

474
00:14:33,846 --> 00:14:35,166
and floating point weights.

475
00:14:36,036 --> 00:14:37,446
What binary convolution allow

476
00:14:37,516 --> 00:14:39,236
you to do is to use your

477
00:14:39,236 --> 00:14:41,266
full-sized input with binary

478
00:14:41,266 --> 00:14:41,486
weights.

479
00:14:42,416 --> 00:14:44,776
And for XNOR convolution the

480
00:14:44,776 --> 00:14:46,706
first thing that happens is that

481
00:14:47,226 --> 00:14:48,626
your input is first converted to

482
00:14:48,626 --> 00:14:50,826
binary so that both your inputs

483
00:14:51,176 --> 00:14:52,396
and the weights are binary.

484
00:14:53,476 --> 00:14:55,286
In regular convolution, the

485
00:14:55,286 --> 00:14:57,066
input has to be multiplied with

486
00:14:57,106 --> 00:14:57,446
the weights.

487
00:14:57,886 --> 00:14:59,876
And for XNOR convolution the

488
00:14:59,876 --> 00:15:01,806
separation becomes a simple XNOR

489
00:15:01,806 --> 00:15:02,336
operation.

490
00:15:04,746 --> 00:15:06,066
And now let's talk about dilated

491
00:15:06,066 --> 00:15:06,656
convolution.

492
00:15:07,626 --> 00:15:08,996
So we already know how regular

493
00:15:08,996 --> 00:15:09,786
convolution works.

494
00:15:10,486 --> 00:15:12,396
You need to apply a filter to

495
00:15:12,396 --> 00:15:13,656
the input data to compute a

496
00:15:13,656 --> 00:15:14,706
single output value.

497
00:15:17,526 --> 00:15:18,786
But say you're working on an

498
00:15:18,786 --> 00:15:21,736
algorithm that requires global

499
00:15:21,736 --> 00:15:24,846
integration of a wider context

500
00:15:25,216 --> 00:15:26,166
of your input data.

501
00:15:26,816 --> 00:15:28,466
So instead of a 3 by 3 kernel,

502
00:15:28,536 --> 00:15:30,496
you may be using a 5 by 5 kernel

503
00:15:31,736 --> 00:15:32,476
to look out further.

504
00:15:33,026 --> 00:15:33,666
But that's a lot more

505
00:15:33,666 --> 00:15:34,756
computationally expensive.

506
00:15:35,256 --> 00:15:37,266
What you can do instead is use

507
00:15:37,266 --> 00:15:39,046
dilated convolutions which

508
00:15:39,046 --> 00:15:43,206
allows you to -- which allows

509
00:15:43,206 --> 00:15:45,256
you to use dilation factors to

510
00:15:45,256 --> 00:15:46,836
introduce gaps into your

511
00:15:46,836 --> 00:15:48,836
convolution kernel so that

512
00:15:48,836 --> 00:15:50,576
you're still using just a 3 by 3

513
00:15:50,576 --> 00:15:52,676
kernel, but you can look out

514
00:15:52,676 --> 00:15:53,016
further.

515
00:15:54,996 --> 00:15:55,876
And now let's talk about

516
00:15:55,946 --> 00:15:57,266
subluxal convolution and

517
00:15:57,266 --> 00:15:58,846
convolution transpose primitive;

518
00:15:59,766 --> 00:16:01,266
very commonly used for image

519
00:16:01,266 --> 00:16:01,836
upscaling.

520
00:16:03,066 --> 00:16:04,126
And let's think about how

521
00:16:04,176 --> 00:16:05,366
upscaling usually works.

522
00:16:05,456 --> 00:16:07,456
So you have your input data and

523
00:16:07,516 --> 00:16:09,196
you want to upscale it by a

524
00:16:09,196 --> 00:16:09,956
factor of 2.

525
00:16:12,336 --> 00:16:13,376
So you won't -- you have some

526
00:16:13,376 --> 00:16:14,566
missing pixels to compute.

527
00:16:15,216 --> 00:16:16,536
And usually upscaling is the

528
00:16:16,536 --> 00:16:18,156
fixed operation with a constant

529
00:16:18,156 --> 00:16:18,606
filter.

530
00:16:18,766 --> 00:16:20,336
So for example how would a box

531
00:16:20,336 --> 00:16:22,336
filter help you to upscale this

532
00:16:22,386 --> 00:16:22,756
image?

533
00:16:23,416 --> 00:16:25,146
So the box filter, which is take

534
00:16:25,316 --> 00:16:28,006
the known pixels and copy the

535
00:16:28,006 --> 00:16:29,326
known data into the missing

536
00:16:29,326 --> 00:16:30,786
location to get you upscaled

537
00:16:30,786 --> 00:16:31,216
results.

538
00:16:33,326 --> 00:16:35,196
For sub-pixel convolution, your

539
00:16:35,196 --> 00:16:36,646
filters are not constant.

540
00:16:36,976 --> 00:16:38,226
Your filters are learned from

541
00:16:38,226 --> 00:16:38,626
the data.

542
00:16:38,766 --> 00:16:40,286
They are your trained parameters

543
00:16:40,716 --> 00:16:41,786
that you get from the training

544
00:16:41,786 --> 00:16:42,906
step where the system was

545
00:16:42,986 --> 00:16:45,106
trained to do this task; to do

546
00:16:45,106 --> 00:16:45,946
image upscaling.

547
00:16:47,086 --> 00:16:49,176
So for 2x upscaling you get 4

548
00:16:49,236 --> 00:16:49,686
filters.

549
00:16:49,806 --> 00:16:51,656
For 4x upscaling you get 16

550
00:16:51,656 --> 00:16:52,656
filters and so on.

551
00:16:53,566 --> 00:16:55,446
So for our 2x upscaling we get

552
00:16:55,446 --> 00:16:57,456
our 4 filters and we apply them

553
00:16:57,456 --> 00:16:58,166
to the input data.

554
00:16:58,166 --> 00:17:00,216
And then the output of that

555
00:17:00,216 --> 00:17:02,076
operation is reshuffled to get

556
00:17:02,076 --> 00:17:03,476
your final full-resolution

557
00:17:03,476 --> 00:17:03,856
image.

558
00:17:04,925 --> 00:17:06,445
And now let's talk about how the

559
00:17:06,445 --> 00:17:08,076
convolution transpose primitive

560
00:17:08,156 --> 00:17:09,536
can be used to upscale images.

561
00:17:10,435 --> 00:17:12,026
So we have our inputs and we

562
00:17:12,026 --> 00:17:13,465
still have to compute our

563
00:17:13,465 --> 00:17:14,146
missing data.

564
00:17:14,945 --> 00:17:16,665
So the way that this primitive

565
00:17:17,156 --> 00:17:18,616
computes the missing data is

566
00:17:18,616 --> 00:17:19,586
that it applies a kind of

567
00:17:19,586 --> 00:17:21,296
convolution pass to this

568
00:17:21,296 --> 00:17:23,165
intermediate result with gaps to

569
00:17:23,165 --> 00:17:24,596
compute each output pixel.

570
00:17:25,185 --> 00:17:27,316
So that's how you get your

571
00:17:27,356 --> 00:17:28,256
upscaled output.

572
00:17:31,136 --> 00:17:32,216
And now we're going to show you

573
00:17:32,216 --> 00:17:33,556
how you can use these new

574
00:17:33,556 --> 00:17:35,046
convolution primitives to

575
00:17:35,046 --> 00:17:36,626
implement a real-world network.

576
00:17:37,096 --> 00:17:38,606
So we took this colorization

577
00:17:38,606 --> 00:17:41,486
network that takes black and

578
00:17:41,486 --> 00:17:42,886
white images as input and

579
00:17:42,886 --> 00:17:44,356
produces colorized images.

580
00:17:44,356 --> 00:17:47,156
And this particular network uses

581
00:17:47,236 --> 00:17:48,426
the dilated convolution

582
00:17:48,426 --> 00:17:50,396
primitive to integrate wider

583
00:17:50,396 --> 00:17:52,296
global context quicker.

584
00:17:52,926 --> 00:17:54,756
And it uses the convolution

585
00:17:54,756 --> 00:17:56,726
transpose primitive to upscale

586
00:17:56,726 --> 00:17:57,776
the results of the network.

587
00:18:00,166 --> 00:18:01,276
And now let's look at this

588
00:18:01,666 --> 00:18:03,966
colorization network in action.

589
00:18:10,226 --> 00:18:11,466
So in this demo we have a

590
00:18:11,466 --> 00:18:12,886
collection of black and white

591
00:18:12,886 --> 00:18:14,046
images like this image of a

592
00:18:14,046 --> 00:18:14,406
lion.

593
00:18:15,046 --> 00:18:16,416
And as soon as I tap on this

594
00:18:16,416 --> 00:18:17,796
image, the colorization network

595
00:18:17,796 --> 00:18:20,046
will run right here live on the

596
00:18:20,046 --> 00:18:21,126
device, and we'll see a

597
00:18:21,126 --> 00:18:21,946
colorized image.

598
00:18:23,906 --> 00:18:25,456
And let's try another example

599
00:18:25,456 --> 00:18:27,046
for this beautiful snowy

600
00:18:27,046 --> 00:18:27,616
mountain.

601
00:18:28,836 --> 00:18:30,036
And now we see it in color.

602
00:18:31,586 --> 00:18:33,756
And this beautiful lovely image

603
00:18:33,756 --> 00:18:35,016
of a dad and a daughter playing

604
00:18:35,016 --> 00:18:35,366
guitar.

605
00:18:35,366 --> 00:18:37,386
And now you can see them playing

606
00:18:37,386 --> 00:18:38,026
guitar in color.

607
00:18:39,516 --> 00:18:40,896
And I really like this one, the

608
00:18:40,896 --> 00:18:42,306
brown bear walking in the

609
00:18:42,356 --> 00:18:42,696
forest.

610
00:18:42,696 --> 00:18:43,756
So I think this network does

611
00:18:43,786 --> 00:18:45,146
just a really wonderful job.

612
00:18:46,886 --> 00:18:48,726
Okay. So that's it for the live

613
00:18:48,726 --> 00:18:48,916
demo.

614
00:18:49,516 --> 00:18:54,686
[ Applause ]

615
00:18:55,186 --> 00:18:55,796
Thank you so much.

616
00:18:57,766 --> 00:18:59,216
So we've added all of these new

617
00:18:59,216 --> 00:19:01,456
convolution CNN primitives, but

618
00:19:01,456 --> 00:19:02,056
that's not all.

619
00:19:02,976 --> 00:19:04,666
We also went back and improved

620
00:19:04,666 --> 00:19:06,046
the performance of some of the

621
00:19:06,206 --> 00:19:07,936
core CNN kernels that were

622
00:19:07,936 --> 00:19:09,646
available to you in iOS 10.

623
00:19:10,406 --> 00:19:11,776
So this chart will show the

624
00:19:11,806 --> 00:19:13,846
performance of the Inception-v3

625
00:19:13,846 --> 00:19:15,386
network, which is a commonly

626
00:19:15,386 --> 00:19:16,936
used network for image

627
00:19:17,056 --> 00:19:17,546
recognition.

628
00:19:18,756 --> 00:19:20,396
So it shows the performance of

629
00:19:20,396 --> 00:19:22,196
this network in iOS 11.

630
00:19:22,196 --> 00:19:23,786
And as you can see, we're

631
00:19:23,786 --> 00:19:25,866
bringing you at least 20 percent

632
00:19:25,866 --> 00:19:27,546
performance improvement across

633
00:19:27,756 --> 00:19:28,696
different iOS hardware.

634
00:19:30,406 --> 00:19:33,786
And now let's talk about the new

635
00:19:33,786 --> 00:19:37,096
neural network graph API.

636
00:19:37,716 --> 00:19:40,036
the neural networks are commonly

637
00:19:40,036 --> 00:19:41,416
described using a graph

638
00:19:41,416 --> 00:19:42,476
abstraction like this

639
00:19:42,476 --> 00:19:43,506
visualization of the

640
00:19:43,506 --> 00:19:44,616
Inception-v3 network.

641
00:19:44,616 --> 00:19:46,696
And we're now allowing to do

642
00:19:46,696 --> 00:19:48,706
just this using the new graph

643
00:19:48,706 --> 00:19:48,966
API.

644
00:19:50,446 --> 00:19:51,556
So let's zoom in on one of these

645
00:19:51,556 --> 00:19:53,186
inception modules.

646
00:19:54,676 --> 00:19:56,496
You have filter nodes which

647
00:19:56,496 --> 00:19:58,176
describe the operations that you

648
00:19:58,176 --> 00:19:59,216
can perform on your data.

649
00:19:59,526 --> 00:20:01,146
Such as convolution, pooling,

650
00:20:01,146 --> 00:20:01,566
etcetera.

651
00:20:02,556 --> 00:20:04,316
And you have image nodes which

652
00:20:04,316 --> 00:20:05,736
describe how the data flows

653
00:20:05,786 --> 00:20:06,546
between these different

654
00:20:06,546 --> 00:20:07,096
operations.

655
00:20:07,826 --> 00:20:11,306
So why did we add this new graph

656
00:20:11,306 --> 00:20:11,556
API?

657
00:20:11,926 --> 00:20:13,156
Well because it's easy to use.

658
00:20:13,686 --> 00:20:14,656
You get this compact

659
00:20:14,656 --> 00:20:16,116
representation of your entire

660
00:20:16,116 --> 00:20:18,326
network and you can save it to

661
00:20:18,326 --> 00:20:20,356
disk and restore it, and that

662
00:20:20,466 --> 00:20:21,546
works across platforms.

663
00:20:23,166 --> 00:20:24,426
You only need to initialize the

664
00:20:24,426 --> 00:20:26,366
graph once and then you can

665
00:20:26,366 --> 00:20:27,716
reuse it for multiple input

666
00:20:27,716 --> 00:20:28,106
images.

667
00:20:29,516 --> 00:20:31,476
And you can execute the entire

668
00:20:31,476 --> 00:20:34,056
graph on the GPU with a single

669
00:20:34,056 --> 00:20:34,346
call.

670
00:20:36,276 --> 00:20:37,536
There are no intermediate images

671
00:20:37,536 --> 00:20:39,196
for you to manage, you just need

672
00:20:39,196 --> 00:20:40,756
to take care of your input and

673
00:20:40,756 --> 00:20:41,036
output.

674
00:20:42,146 --> 00:20:45,016
Internally we use Metal heaps to

675
00:20:45,016 --> 00:20:46,796
make sure that the memory

676
00:20:46,796 --> 00:20:47,916
footprint of all your

677
00:20:47,916 --> 00:20:49,506
intermediate images is as small

678
00:20:49,506 --> 00:20:50,126
as possible.

679
00:20:50,606 --> 00:20:51,296
For example, for the

680
00:20:51,296 --> 00:20:53,376
Inception-v3 network this means

681
00:20:53,826 --> 00:20:56,856
5x memory savings and 10x viewer

682
00:20:56,856 --> 00:20:58,496
allocations, which I think is

683
00:20:58,556 --> 00:20:59,256
pretty impressive.

684
00:21:00,836 --> 00:21:02,926
So as I said, the graph does all

685
00:21:02,926 --> 00:21:03,956
the groundwork for you.

686
00:21:04,316 --> 00:21:05,646
It takes care of creating

687
00:21:05,846 --> 00:21:06,846
intermediate images.

688
00:21:06,996 --> 00:21:08,706
It takes care of sizing them.

689
00:21:09,296 --> 00:21:11,086
It also -- it even sizes your

690
00:21:11,086 --> 00:21:11,366
outputs.

691
00:21:11,786 --> 00:21:12,766
It takes care of the padding

692
00:21:12,766 --> 00:21:13,406
policies.

693
00:21:13,796 --> 00:21:15,016
It takes care of censoring.

694
00:21:15,426 --> 00:21:17,516
So in short, it's a lot less

695
00:21:17,566 --> 00:21:19,406
code for you to write and a lot

696
00:21:19,486 --> 00:21:20,746
fewer bugs for you to write as

697
00:21:20,746 --> 00:21:20,956
well.

698
00:21:21,956 --> 00:21:24,066
And when I say less code, I mean

699
00:21:24,596 --> 00:21:25,376
a lot less code.

700
00:21:26,206 --> 00:21:27,906
So last year we released this

701
00:21:27,996 --> 00:21:30,726
Metal recognition sample that

702
00:21:30,726 --> 00:21:32,036
uses the Inception-v3 network

703
00:21:32,036 --> 00:21:33,286
for image recognition.

704
00:21:34,326 --> 00:21:36,026
And we took that sample and

705
00:21:36,026 --> 00:21:37,576
converted it to use the new

706
00:21:37,806 --> 00:21:40,036
graph API and found that we had

707
00:21:40,036 --> 00:21:41,956
to write four times less code.

708
00:21:42,356 --> 00:21:43,956
And that's pretty much the same

709
00:21:43,956 --> 00:21:45,846
number of lines as Python code

710
00:21:46,226 --> 00:21:47,916
you would have to write in the

711
00:21:47,916 --> 00:21:48,886
open-source sensor flow

712
00:21:48,886 --> 00:21:50,436
framework to implement the same

713
00:21:50,436 --> 00:21:50,846
network.

714
00:21:51,476 --> 00:21:52,956
And we just want to mention that

715
00:21:52,956 --> 00:21:54,046
we will be releasing this

716
00:21:54,086 --> 00:21:57,196
updated sample code -- updated

717
00:21:57,196 --> 00:21:58,346
example as sample code.

718
00:21:59,026 --> 00:22:01,796
And now having all this

719
00:22:01,796 --> 00:22:03,276
information about your entire

720
00:22:03,276 --> 00:22:06,116
network allows us to deliver the

721
00:22:06,116 --> 00:22:07,956
best performance across

722
00:22:08,106 --> 00:22:08,866
different views.

723
00:22:09,336 --> 00:22:10,946
We make it easy for your to

724
00:22:10,946 --> 00:22:12,766
parallelize between the CPU and

725
00:22:12,766 --> 00:22:13,206
the GPU.

726
00:22:13,976 --> 00:22:15,876
so as the graph is executing --

727
00:22:16,326 --> 00:22:18,076
as the GPU is executing the

728
00:22:18,076 --> 00:22:19,986
graph of one input image, the

729
00:22:19,986 --> 00:22:21,686
CPU can already prepare to

730
00:22:21,686 --> 00:22:22,776
execute the graph for a

731
00:22:22,776 --> 00:22:23,716
different input image.

732
00:22:25,176 --> 00:22:26,606
We can also fuse graph nodes

733
00:22:26,676 --> 00:22:28,446
together like the convolution

734
00:22:28,856 --> 00:22:29,966
and neuron nodes.

735
00:22:31,856 --> 00:22:33,746
And we can execute graph nodes

736
00:22:33,746 --> 00:22:34,386
concurrently.

737
00:22:34,386 --> 00:22:36,256
So if we look at this inception

738
00:22:36,256 --> 00:22:38,056
module again, you can see that

739
00:22:38,056 --> 00:22:39,886
there are multiple rows of these

740
00:22:39,886 --> 00:22:41,386
nodes that can be executed

741
00:22:41,456 --> 00:22:43,036
completely independently of each

742
00:22:43,036 --> 00:22:43,236
other.

743
00:22:44,176 --> 00:22:45,486
And of course the output of

744
00:22:45,486 --> 00:22:47,326
these independent executions

745
00:22:47,926 --> 00:22:49,216
need to be concatenated via

746
00:22:49,216 --> 00:22:50,216
concatenation nodes.

747
00:22:51,206 --> 00:22:52,656
And the graph is smart enough to

748
00:22:52,656 --> 00:22:54,276
optimize those away as well.

749
00:22:54,886 --> 00:22:57,706
And now let's take a look at how

750
00:22:57,706 --> 00:22:59,586
you can use the new graph API.

751
00:23:00,296 --> 00:23:02,176
So this is the code for creating

752
00:23:02,176 --> 00:23:04,126
a convolution node using the

753
00:23:04,126 --> 00:23:04,646
graph API.

754
00:23:05,826 --> 00:23:07,256
So it takes an image as source

755
00:23:08,226 --> 00:23:09,486
and it also has weights.

756
00:23:09,486 --> 00:23:10,926
So let's talk about weights for

757
00:23:10,926 --> 00:23:11,176
a minute.

758
00:23:13,056 --> 00:23:14,256
Neural networks keep growing

759
00:23:14,256 --> 00:23:15,396
larger and larger in size.

760
00:23:16,136 --> 00:23:17,736
And if you have many convolution

761
00:23:17,736 --> 00:23:19,346
nodes in your networks, that

762
00:23:19,346 --> 00:23:21,436
means that the overall size of

763
00:23:21,496 --> 00:23:22,466
the weights for your entire

764
00:23:22,466 --> 00:23:23,616
network could be quite

765
00:23:23,616 --> 00:23:24,246
considerable.

766
00:23:25,216 --> 00:23:27,016
And to help with that we've

767
00:23:27,016 --> 00:23:30,126
added a convolution data source

768
00:23:30,186 --> 00:23:31,296
protocol that you can implement

769
00:23:31,656 --> 00:23:33,186
and it provides just in time

770
00:23:33,606 --> 00:23:35,036
loading and purging of weights

771
00:23:35,076 --> 00:23:35,276
data.

772
00:23:36,296 --> 00:23:40,046
So the idea is that the weights

773
00:23:40,046 --> 00:23:41,546
for your entire network do not

774
00:23:41,546 --> 00:23:43,196
have to be loaded in memory all

775
00:23:43,196 --> 00:23:44,086
at the same time.

776
00:23:44,626 --> 00:23:45,956
They also do not have to be

777
00:23:45,956 --> 00:23:46,886
loaded in advance.

778
00:23:48,396 --> 00:23:49,656
To help minimize the memory

779
00:23:49,656 --> 00:23:51,556
footprint, when we initialize

780
00:23:51,556 --> 00:23:53,136
the graph and we process a

781
00:23:53,136 --> 00:23:54,516
particular convolution layer,

782
00:23:55,096 --> 00:23:56,126
we'll load the weights for that

783
00:23:56,126 --> 00:23:57,726
convolution layer and then we

784
00:23:57,856 --> 00:23:59,486
purge them before we move on to

785
00:23:59,486 --> 00:24:00,726
the next convolution layer.

786
00:24:02,226 --> 00:24:03,586
What you have to do is to

787
00:24:03,586 --> 00:24:05,146
implement this initialization

788
00:24:05,146 --> 00:24:06,916
method which just knows where

789
00:24:06,916 --> 00:24:08,376
the data is but it doesn't

790
00:24:08,376 --> 00:24:09,086
actually load it.

791
00:24:10,096 --> 00:24:11,256
And then when the graph calls

792
00:24:11,256 --> 00:24:13,266
the load function that alerts

793
00:24:13,266 --> 00:24:14,846
you that the weights need to be

794
00:24:14,846 --> 00:24:15,236
loaded.

795
00:24:15,366 --> 00:24:16,546
And then when the purge function

796
00:24:16,546 --> 00:24:18,166
is called by the graph then you

797
00:24:18,166 --> 00:24:19,316
can release the weights.

798
00:24:21,586 --> 00:24:22,526
And now let's build a graph.

799
00:24:23,446 --> 00:24:24,926
So here we're implementing this

800
00:24:24,926 --> 00:24:26,116
makeGraph function.

801
00:24:26,596 --> 00:24:28,366
And on the left you can see all

802
00:24:28,366 --> 00:24:29,636
the nodes that make up our

803
00:24:29,636 --> 00:24:30,826
network that we need to build.

804
00:24:31,256 --> 00:24:32,926
So then we create the nodes.

805
00:24:33,226 --> 00:24:34,446
So we create the convolution

806
00:24:34,446 --> 00:24:34,836
node.

807
00:24:35,016 --> 00:24:35,616
The pooling node.

808
00:24:35,616 --> 00:24:37,456
And then the rest of the nodes.

809
00:24:37,756 --> 00:24:38,606
So we have the nodes.

810
00:24:38,606 --> 00:24:40,226
How do we connect them into a

811
00:24:40,226 --> 00:24:40,446
graph?

812
00:24:41,646 --> 00:24:43,186
So we just take the result image

813
00:24:43,186 --> 00:24:44,986
of one node and pass it as a

814
00:24:45,066 --> 00:24:46,516
source image to the next node.

815
00:24:46,516 --> 00:24:48,106
And then we have our graph.

816
00:24:49,736 --> 00:24:51,236
And now let's run it on the GPU.

817
00:24:51,906 --> 00:24:54,066
So first we do our usual Metal

818
00:24:54,066 --> 00:24:54,456
setup.

819
00:24:54,886 --> 00:24:56,016
We initialize the graph.

820
00:24:56,676 --> 00:24:58,166
We take care of our input data

821
00:24:58,866 --> 00:25:01,066
and then we encode the graph to

822
00:25:01,066 --> 00:25:01,506
the GPU.

823
00:25:02,276 --> 00:25:04,026
And the data in the output image

824
00:25:04,556 --> 00:25:06,546
will be -- the output image will

825
00:25:06,546 --> 00:25:08,466
be populated with data when the

826
00:25:08,466 --> 00:25:09,576
command buffer completes.

827
00:25:10,086 --> 00:25:11,526
And then we have an option to

828
00:25:11,526 --> 00:25:12,996
wait for the GPU to finish.

829
00:25:13,406 --> 00:25:14,686
But we don't want you to do

830
00:25:14,686 --> 00:25:14,956
that.

831
00:25:15,886 --> 00:25:17,506
When this happens the CPU is

832
00:25:17,506 --> 00:25:19,246
waiting for the GPU to finish

833
00:25:19,836 --> 00:25:21,486
before it can start encoding the

834
00:25:21,486 --> 00:25:22,846
next run of the graph.

835
00:25:23,486 --> 00:25:25,256
And this introduces bubbles into

836
00:25:25,256 --> 00:25:26,266
your pipeline, which can

837
00:25:26,526 --> 00:25:28,036
adversely affect performance.

838
00:25:29,816 --> 00:25:30,686
So what we want you to do

839
00:25:30,686 --> 00:25:32,606
instead is to use the new

840
00:25:32,606 --> 00:25:34,596
asynchronous executeAsync API.

841
00:25:35,426 --> 00:25:37,896
So with this API your Metal

842
00:25:37,966 --> 00:25:39,436
setup is even smaller.

843
00:25:39,626 --> 00:25:41,076
So you just need to get the

844
00:25:41,076 --> 00:25:41,736
Metal device.

845
00:25:42,136 --> 00:25:43,096
Then you still need to

846
00:25:43,096 --> 00:25:44,016
initialize your graph.

847
00:25:44,056 --> 00:25:46,426
Prepare the input data and then

848
00:25:46,426 --> 00:25:47,856
you executeAsync call.

849
00:25:49,586 --> 00:25:52,376
It returns immediately and then

850
00:25:52,376 --> 00:25:54,216
the output image will be ready

851
00:25:55,196 --> 00:25:56,236
when this code inside the

852
00:25:56,236 --> 00:25:57,086
closure executes.

853
00:25:57,796 --> 00:25:59,056
But in the meantime, you don't

854
00:25:59,056 --> 00:26:00,136
have to wait for the GPU to

855
00:26:00,306 --> 00:26:02,056
finish, you can already proceed

856
00:26:02,056 --> 00:26:03,676
with a coding and new GPU task.

857
00:26:04,396 --> 00:26:07,236
And this way the CPU and the GPU

858
00:26:07,236 --> 00:26:08,846
are executing concurrently.

859
00:26:09,406 --> 00:26:10,316
There are no bubbles in your

860
00:26:10,316 --> 00:26:12,756
pipeline and they're both

861
00:26:12,756 --> 00:26:14,376
utilized to full capacity.

862
00:26:16,756 --> 00:26:18,996
Okay. And now I will do a live

863
00:26:18,996 --> 00:26:21,106
demo that demonstrates the

864
00:26:21,146 --> 00:26:22,846
performance difference between

865
00:26:22,966 --> 00:26:24,526
the synchronous and asynchronous

866
00:26:24,526 --> 00:26:24,786
APIs.

867
00:26:24,786 --> 00:26:27,576
And this demo will be using the

868
00:26:27,576 --> 00:26:29,576
Inception-v3 network for image

869
00:26:29,576 --> 00:26:30,116
recognition.

870
00:26:30,516 --> 00:26:30,806
All right.

871
00:26:31,136 --> 00:26:32,726
So I will be starting with

872
00:26:32,726 --> 00:26:34,416
synchronous API and here we're

873
00:26:34,416 --> 00:26:35,706
detecting a water bottle.

874
00:26:35,706 --> 00:26:38,276
And we're getting about 50

875
00:26:38,276 --> 00:26:41,756
milliseconds per second per

876
00:26:41,756 --> 00:26:42,596
image on average.

877
00:26:42,826 --> 00:26:44,066
And now I will switch to the

878
00:26:44,066 --> 00:26:44,956
asynchronous API.

879
00:26:44,956 --> 00:26:47,586
And now we're getting about 36

880
00:26:47,586 --> 00:26:49,546
milliseconds per image on

881
00:26:49,546 --> 00:26:49,936
average.

882
00:26:49,936 --> 00:26:51,656
So that's pretty good

883
00:26:51,686 --> 00:26:52,666
performance improvement.

884
00:26:54,776 --> 00:26:55,066
All right.

885
00:26:55,196 --> 00:26:56,436
So that's it for the live demo.

886
00:26:58,516 --> 00:27:04,016
[ Applause ]

887
00:27:04,516 --> 00:27:04,876
Thank you.

888
00:27:06,566 --> 00:27:07,656
Okay. Now that we've talked

889
00:27:07,656 --> 00:27:08,626
about the new neural network

890
00:27:08,626 --> 00:27:10,926
graph API and I showed you how

891
00:27:10,956 --> 00:27:12,716
easy it is to use and what great

892
00:27:12,716 --> 00:27:14,016
performance you can achieve with

893
00:27:14,016 --> 00:27:16,086
it, let's now switch gears and

894
00:27:16,086 --> 00:27:17,166
talk about recurrent neural

895
00:27:17,166 --> 00:27:17,566
networks.

896
00:27:19,416 --> 00:27:20,386
So what are recurrent neural

897
00:27:20,386 --> 00:27:20,786
networks?

898
00:27:23,406 --> 00:27:25,456
So one disadvantage of CNNs is

899
00:27:25,456 --> 00:27:27,276
their inability to remember

900
00:27:27,306 --> 00:27:28,326
anything that happened in the

901
00:27:28,326 --> 00:27:28,466
past.

902
00:27:29,456 --> 00:27:31,226
They can take one image as input

903
00:27:31,896 --> 00:27:33,926
and generate a single output

904
00:27:34,446 --> 00:27:36,316
such as the set of probabilities

905
00:27:36,356 --> 00:27:37,396
of what is depicted in the

906
00:27:37,396 --> 00:27:37,836
image.

907
00:27:39,056 --> 00:27:41,016
RNNs on the other hand have

908
00:27:41,016 --> 00:27:41,446
memory.

909
00:27:42,346 --> 00:27:43,466
And they're good at operating on

910
00:27:43,546 --> 00:27:44,066
sequences.

911
00:27:44,536 --> 00:27:48,256
So they can take one input such

912
00:27:48,256 --> 00:27:49,576
as a set of probabilities of

913
00:27:49,636 --> 00:27:51,016
what is depicted in the image

914
00:27:51,616 --> 00:27:52,966
and generate a sequence of

915
00:27:53,036 --> 00:27:53,366
outputs.

916
00:27:53,366 --> 00:27:56,196
So a sequence of words that make

917
00:27:56,196 --> 00:27:57,586
up a caption for this image.

918
00:27:59,416 --> 00:28:01,716
They can also take a sequence of

919
00:28:01,806 --> 00:28:03,506
inputs such as a sentence in

920
00:28:03,506 --> 00:28:06,626
English and generate a sequence

921
00:28:06,626 --> 00:28:08,506
of outputs such as the same

922
00:28:08,666 --> 00:28:09,946
sentence translated to a

923
00:28:09,946 --> 00:28:12,376
different language like Russian

924
00:28:12,466 --> 00:28:13,056
or Finnish.

925
00:28:13,366 --> 00:28:16,356
And we support a number of

926
00:28:16,356 --> 00:28:17,756
different of variants of RNNs.

927
00:28:18,586 --> 00:28:20,146
The single gate RNN, the long

928
00:28:20,146 --> 00:28:22,156
short-term memory RNN or LSTM,

929
00:28:22,596 --> 00:28:24,586
and multiple variants of LSTMs.

930
00:28:24,866 --> 00:28:26,336
The GRU and the MGU.

931
00:28:27,766 --> 00:28:29,336
So let's talk about the simplest

932
00:28:29,366 --> 00:28:31,326
kind of RNN, the single gate

933
00:28:31,326 --> 00:28:31,526
RNN.

934
00:28:33,666 --> 00:28:34,966
the single gate RNN has a

935
00:28:34,966 --> 00:28:37,006
recurrent unit which enables the

936
00:28:37,066 --> 00:28:38,676
previous output over RNN to

937
00:28:38,996 --> 00:28:40,346
affect the output of the

938
00:28:40,406 --> 00:28:41,846
subsequent iterations of the

939
00:28:41,846 --> 00:28:42,406
same RNN.

940
00:28:43,606 --> 00:28:45,496
But the single gate RNNs are not

941
00:28:45,546 --> 00:28:47,466
powerful enough to carry on

942
00:28:47,466 --> 00:28:48,746
important information for many

943
00:28:48,746 --> 00:28:49,286
iterations.

944
00:28:50,136 --> 00:28:51,676
Because the current output of an

945
00:28:51,786 --> 00:28:53,976
RNN -- of the single gate RNN is

946
00:28:53,976 --> 00:28:54,996
also its current state.

947
00:28:54,996 --> 00:28:55,976
There's nothing else there.

948
00:28:57,146 --> 00:28:59,426
The solution to this is the long

949
00:28:59,476 --> 00:29:01,596
short-term memory RNN or LSTM.

950
00:29:02,376 --> 00:29:03,906
It's built from single gate RNNs

951
00:29:03,906 --> 00:29:06,246
and it has an internal memory

952
00:29:06,246 --> 00:29:06,506
cell.

953
00:29:07,436 --> 00:29:08,856
And a certain combination of

954
00:29:08,956 --> 00:29:10,596
gates control how the

955
00:29:10,596 --> 00:29:13,106
information flows inside LSTM.

956
00:29:13,436 --> 00:29:15,016
And what is stored and not

957
00:29:15,136 --> 00:29:16,266
stored in the memory cell.

958
00:29:16,846 --> 00:29:19,656
So let's take a look at the

959
00:29:19,656 --> 00:29:21,316
architecture of LSTM in more

960
00:29:21,316 --> 00:29:21,776
detail.

961
00:29:22,206 --> 00:29:25,246
As I said, the most important

962
00:29:25,356 --> 00:29:27,846
entity inside LSTM is the memory

963
00:29:27,886 --> 00:29:30,406
cell which is updated in every

964
00:29:30,476 --> 00:29:31,536
duration of LSTM.

965
00:29:31,536 --> 00:29:33,426
So you can think of each

966
00:29:33,846 --> 00:29:35,246
iteration of LSTM is this

967
00:29:35,306 --> 00:29:37,456
transition between the old and

968
00:29:37,456 --> 00:29:38,016
new memory.

969
00:29:38,676 --> 00:29:40,786
And now let's talk about the

970
00:29:40,816 --> 00:29:41,036
gates.

971
00:29:41,566 --> 00:29:43,376
So first there is a forget gate

972
00:29:44,216 --> 00:29:45,876
which decides what to keep and

973
00:29:45,876 --> 00:29:47,206
what not to keep from old

974
00:29:47,206 --> 00:29:47,566
memory.

975
00:29:48,966 --> 00:29:50,456
And then there are the inputs

976
00:29:50,456 --> 00:29:51,836
and the cell gates and their

977
00:29:51,836 --> 00:29:53,996
combined contribution determines

978
00:29:54,066 --> 00:29:55,786
what from the current input will

979
00:29:55,786 --> 00:29:56,936
affect the new memory.

980
00:29:56,936 --> 00:29:59,136
And then the combination of all

981
00:29:59,136 --> 00:30:00,866
of these three gates is combined

982
00:30:01,226 --> 00:30:04,596
to update the memory cell.

983
00:30:05,696 --> 00:30:07,576
And finally, there is the output

984
00:30:07,626 --> 00:30:09,656
gate which determines what from

985
00:30:09,656 --> 00:30:11,976
the previous inputs the -- the

986
00:30:12,476 --> 00:30:14,076
previous output, the current

987
00:30:14,076 --> 00:30:16,126
inputs and the new memory will

988
00:30:16,126 --> 00:30:17,936
affect the output of LSTM.

989
00:30:19,196 --> 00:30:20,626
So now that you know what LSTM

990
00:30:20,626 --> 00:30:22,206
is made up of, let's take a look

991
00:30:22,206 --> 00:30:24,006
at how you can create one using

992
00:30:24,006 --> 00:30:24,576
our framework.

993
00:30:25,616 --> 00:30:27,536
So first you create a descriptor

994
00:30:27,756 --> 00:30:28,576
for the LSTM.

995
00:30:29,146 --> 00:30:31,306
And then you need to initialize

996
00:30:31,526 --> 00:30:31,876
the gates.

997
00:30:32,436 --> 00:30:33,676
So what controls the gates?

998
00:30:33,676 --> 00:30:35,146
So what controls the gates with

999
00:30:35,306 --> 00:30:36,156
-- what controls how they

1000
00:30:36,156 --> 00:30:37,756
operate is the trained

1001
00:30:37,756 --> 00:30:38,386
parameters.

1002
00:30:39,146 --> 00:30:40,156
The ones that come from the

1003
00:30:40,156 --> 00:30:41,726
training step where you train a

1004
00:30:41,726 --> 00:30:43,526
system to do a particular task.

1005
00:30:45,566 --> 00:30:47,496
And there are multiple gates for

1006
00:30:47,496 --> 00:30:48,586
you to initialize as you can

1007
00:30:48,646 --> 00:30:49,856
see, but we're only showing two

1008
00:30:49,856 --> 00:30:52,356
initializations just to be

1009
00:30:52,356 --> 00:30:52,726
brief.

1010
00:30:53,206 --> 00:30:54,336
And as you can see, we're also

1011
00:30:54,336 --> 00:30:56,046
using a data source provider.

1012
00:30:56,046 --> 00:30:57,446
The same one I showed you before

1013
00:30:57,556 --> 00:30:58,606
to initialize the weights.

1014
00:30:59,656 --> 00:31:01,536
And the next step is to create

1015
00:31:01,536 --> 00:31:03,606
our LSTM layer and now we want

1016
00:31:03,606 --> 00:31:04,876
to run it on the GPU.

1017
00:31:06,586 --> 00:31:08,646
So we need to create our arrays

1018
00:31:08,646 --> 00:31:10,976
that will hold the input and

1019
00:31:10,976 --> 00:31:13,236
output for the sequence of the

1020
00:31:13,236 --> 00:31:14,266
LSTM executions.

1021
00:31:14,886 --> 00:31:16,076
And then we encode the sequence

1022
00:31:16,076 --> 00:31:16,716
to the GPU.

1023
00:31:17,416 --> 00:31:19,446
And here we're showing you the

1024
00:31:19,666 --> 00:31:21,546
-- a matrix-based RNN, but we

1025
00:31:21,546 --> 00:31:22,646
just want to mention that we

1026
00:31:22,686 --> 00:31:25,726
also support RNNs that operate

1027
00:31:25,726 --> 00:31:27,636
on MPS images via convolutions.

1028
00:31:30,176 --> 00:31:31,216
And now let's take a look at an

1029
00:31:31,216 --> 00:31:32,016
actual example.

1030
00:31:32,596 --> 00:31:34,366
So we'll use image captioning as

1031
00:31:34,366 --> 00:31:35,706
an example of using LSTM.

1032
00:31:36,726 --> 00:31:38,566
So as you recall, I told you

1033
00:31:38,896 --> 00:31:40,646
that deep learning algorithms

1034
00:31:40,646 --> 00:31:42,076
have two phases.

1035
00:31:42,346 --> 00:31:43,316
The training phase and the

1036
00:31:43,316 --> 00:31:44,026
inference phase.

1037
00:31:44,816 --> 00:31:46,816
So to train a system to caption

1038
00:31:46,816 --> 00:31:49,196
images you need to feed it a

1039
00:31:49,196 --> 00:31:51,056
large number of images with

1040
00:31:51,056 --> 00:31:52,356
human-generated captions.

1041
00:31:53,836 --> 00:31:56,606
So what does this system have?

1042
00:31:56,656 --> 00:31:57,686
Like what is it made out of?

1043
00:31:58,536 --> 00:32:02,016
So this system has a CNN and a

1044
00:32:02,016 --> 00:32:03,906
RNN working together to generate

1045
00:32:03,906 --> 00:32:04,346
captions.

1046
00:32:04,746 --> 00:32:07,316
The CNN is used to figure out

1047
00:32:07,316 --> 00:32:09,316
what's depicted in the image and

1048
00:32:09,316 --> 00:32:10,886
then the RNN is used to generate

1049
00:32:10,956 --> 00:32:11,806
the actual caption.

1050
00:32:13,256 --> 00:32:15,076
And the output of that process

1051
00:32:15,156 --> 00:32:17,006
is the trained parameters which

1052
00:32:17,006 --> 00:32:19,036
are required for the next step,

1053
00:32:20,186 --> 00:32:20,886
the inference step.

1054
00:32:21,676 --> 00:32:25,606
So in the inference phase, the

1055
00:32:25,656 --> 00:32:27,766
trained parameters control both

1056
00:32:27,766 --> 00:32:29,826
the operation of the CNN layers

1057
00:32:29,826 --> 00:32:31,866
and the operation of the RNN

1058
00:32:31,866 --> 00:32:32,146
gates.

1059
00:32:33,206 --> 00:32:37,276
And then for each image it's

1060
00:32:37,336 --> 00:32:39,166
processed by both the CNN and

1061
00:32:39,446 --> 00:32:41,326
the RNN to generate a caption.

1062
00:32:42,216 --> 00:32:43,246
So we already know a good

1063
00:32:43,246 --> 00:32:44,636
network for figuring out what's

1064
00:32:44,706 --> 00:32:45,816
depicted in the image.

1065
00:32:46,086 --> 00:32:47,506
It's the Inception-v3 network,

1066
00:32:47,876 --> 00:32:48,626
so we'll use that.

1067
00:32:49,186 --> 00:32:50,456
And we just talked about LSTMs,

1068
00:32:50,456 --> 00:32:52,236
so let's use that to generate

1069
00:32:52,446 --> 00:32:53,036
our caption.

1070
00:32:53,996 --> 00:32:56,456
And the caption generation phase

1071
00:32:57,266 --> 00:32:58,176
-- the caption generation

1072
00:32:58,176 --> 00:32:59,726
process also has two phases.

1073
00:33:00,006 --> 00:33:02,206
So first we have the LSTM

1074
00:33:02,486 --> 00:33:03,646
initialization phase.

1075
00:33:04,846 --> 00:33:06,126
So we run our Inception-v3

1076
00:33:06,126 --> 00:33:08,616
network and we actually run all

1077
00:33:08,616 --> 00:33:10,496
of the layers except the very

1078
00:33:10,496 --> 00:33:11,986
last SoftMax layer.

1079
00:33:12,236 --> 00:33:13,386
And the output of that is a

1080
00:33:13,386 --> 00:33:15,016
feature vector which has

1081
00:33:15,016 --> 00:33:16,196
information about what is

1082
00:33:16,196 --> 00:33:17,226
depicted in the image.

1083
00:33:17,906 --> 00:33:19,016
And then we take that feature

1084
00:33:19,016 --> 00:33:20,566
vector and convert it to a

1085
00:33:20,566 --> 00:33:23,246
compact representation that's

1086
00:33:23,246 --> 00:33:24,286
required by LSTM.

1087
00:33:24,286 --> 00:33:26,596
And then run that through LSTM

1088
00:33:26,946 --> 00:33:27,746
to initialize it.

1089
00:33:28,736 --> 00:33:30,466
And then once we have our

1090
00:33:30,466 --> 00:33:32,436
initialized LSTM, then we're

1091
00:33:32,436 --> 00:33:33,586
ready for the next phase.

1092
00:33:34,606 --> 00:33:36,066
Our actual caption generation

1093
00:33:36,066 --> 00:33:36,376
phase.

1094
00:33:38,116 --> 00:33:39,676
And we start this process by

1095
00:33:39,676 --> 00:33:41,366
passing in a special sentence

1096
00:33:41,436 --> 00:33:44,026
start ID token to our LSTM.

1097
00:33:44,236 --> 00:33:45,046
And the output of that

1098
00:33:45,046 --> 00:33:46,756
operations is a sequence of

1099
00:33:46,756 --> 00:33:50,006
words which are, you know, the

1100
00:33:50,006 --> 00:33:51,276
words that are connected to what

1101
00:33:51,276 --> 00:33:52,666
is depicted in the image.

1102
00:33:53,526 --> 00:33:55,806
And then we pass those words to

1103
00:33:55,806 --> 00:33:57,226
a SoftMax layer which computes

1104
00:33:57,226 --> 00:33:58,796
probabilities for these words.

1105
00:33:59,326 --> 00:34:01,176
And we pick the three best ones.

1106
00:34:01,326 --> 00:34:03,276
And these three best words are

1107
00:34:03,276 --> 00:34:05,816
also our one-word partial

1108
00:34:05,816 --> 00:34:07,546
captions for a particular image.

1109
00:34:08,126 --> 00:34:09,726
So we take those words and pass

1110
00:34:09,806 --> 00:34:11,856
them to the next situation of

1111
00:34:11,946 --> 00:34:15,045
LSTM which function is to now

1112
00:34:15,096 --> 00:34:16,886
come up with three best two-word

1113
00:34:16,916 --> 00:34:19,216
captions for our image and so

1114
00:34:19,216 --> 00:34:19,386
on.

1115
00:34:19,466 --> 00:34:21,416
We execute for N iterations

1116
00:34:21,886 --> 00:34:23,076
until we reach a stopping

1117
00:34:23,076 --> 00:34:25,596
condition, which is when we

1118
00:34:25,626 --> 00:34:27,116
either reach the maximum number

1119
00:34:27,116 --> 00:34:28,966
of words that we want to be in

1120
00:34:28,966 --> 00:34:30,856
our caption, or when the

1121
00:34:30,856 --> 00:34:32,136
probabilities evolving the newly

1122
00:34:32,136 --> 00:34:33,906
generating captions drop to 0.

1123
00:34:34,985 --> 00:34:35,996
So I know this is still pretty

1124
00:34:35,996 --> 00:34:36,505
abstract.

1125
00:34:36,846 --> 00:34:39,005
So let's look at the output of

1126
00:34:39,386 --> 00:34:42,266
LSTM -- of an actual output of

1127
00:34:42,326 --> 00:34:44,556
LSTM for several iterations for

1128
00:34:44,556 --> 00:34:45,545
a particular image.

1129
00:34:46,216 --> 00:34:49,786
So in this image we have, you

1130
00:34:49,786 --> 00:34:51,636
know, our surfer riding a wave.

1131
00:34:51,636 --> 00:34:53,146
And we want to compute the top

1132
00:34:53,146 --> 00:34:54,666
three captions for this image.

1133
00:34:55,696 --> 00:34:57,286
And in the first iteration of

1134
00:34:57,366 --> 00:34:59,936
LSTM we generate three best

1135
00:34:59,936 --> 00:35:00,306
words.

1136
00:35:02,336 --> 00:35:04,446
So -- which are our best

1137
00:35:04,446 --> 00:35:05,686
one-word captions for this

1138
00:35:05,686 --> 00:35:05,996
image.

1139
00:35:06,506 --> 00:35:07,596
So "man", "a", and "the".

1140
00:35:08,316 --> 00:35:10,596
And the word "a" has the highest

1141
00:35:10,596 --> 00:35:11,186
probability.

1142
00:35:11,936 --> 00:35:13,546
So we take these three words and

1143
00:35:13,546 --> 00:35:15,196
we pass them to the next

1144
00:35:15,196 --> 00:35:16,436
iteration of LSTM.

1145
00:35:17,166 --> 00:35:19,356
And in this iteration, for every

1146
00:35:19,356 --> 00:35:21,416
one of these three starter

1147
00:35:22,436 --> 00:35:24,416
words, LSTM generates three new

1148
00:35:24,416 --> 00:35:26,026
words that have the highest

1149
00:35:26,026 --> 00:35:28,496
probability of following each

1150
00:35:28,496 --> 00:35:29,606
one of these starter words.

1151
00:35:30,666 --> 00:35:31,966
Right? So we have three new

1152
00:35:31,966 --> 00:35:33,556
words to follow the word "man".

1153
00:35:33,946 --> 00:35:35,256
Three new words to follow the

1154
00:35:35,256 --> 00:35:37,766
word "a", and three new words to

1155
00:35:37,826 --> 00:35:38,976
follow the word "the".

1156
00:35:40,686 --> 00:35:42,296
And now as you can see, each one

1157
00:35:42,296 --> 00:35:44,486
of these two-word captions also

1158
00:35:44,486 --> 00:35:45,486
have a probability.

1159
00:35:46,026 --> 00:35:47,866
And because the word "a" had

1160
00:35:47,936 --> 00:35:49,286
such a high probability in the

1161
00:35:49,286 --> 00:35:53,366
first iteration, then the

1162
00:35:53,366 --> 00:35:54,646
captions that start with the

1163
00:35:54,646 --> 00:35:56,426
word "a" in the second iteration

1164
00:35:56,476 --> 00:35:58,156
also end up having the highest

1165
00:35:58,156 --> 00:35:58,796
probability.

1166
00:35:58,896 --> 00:36:00,916
Why? Because the probability of

1167
00:36:00,916 --> 00:36:02,506
a two-word caption is just a

1168
00:36:02,536 --> 00:36:04,516
product of the probabilities of

1169
00:36:04,516 --> 00:36:05,706
the words that make up that

1170
00:36:05,706 --> 00:36:06,006
caption.

1171
00:36:07,116 --> 00:36:08,886
So that's how we get these three

1172
00:36:08,886 --> 00:36:09,396
best ones.

1173
00:36:09,696 --> 00:36:11,226
And then we take them and we

1174
00:36:11,226 --> 00:36:12,836
move on to the next iteration.

1175
00:36:13,186 --> 00:36:14,406
And in the next iteration we

1176
00:36:14,406 --> 00:36:15,936
just add one more word to our

1177
00:36:15,936 --> 00:36:17,536
captions so that we have

1178
00:36:17,846 --> 00:36:18,806
three-word captions.

1179
00:36:19,206 --> 00:36:19,996
And then we compute the

1180
00:36:19,996 --> 00:36:21,836
probabilities of those captions

1181
00:36:21,836 --> 00:36:22,886
and pick the best three.

1182
00:36:23,936 --> 00:36:25,106
And we move on to the next

1183
00:36:25,106 --> 00:36:27,026
iteration where we just end up

1184
00:36:27,026 --> 00:36:28,746
adding one more word to our

1185
00:36:28,746 --> 00:36:29,186
caption.

1186
00:36:29,186 --> 00:36:30,566
So we have four-word captions.

1187
00:36:30,976 --> 00:36:31,876
And then we compute the

1188
00:36:31,926 --> 00:36:33,186
probabilities of all these

1189
00:36:33,236 --> 00:36:34,546
captions and pick the best

1190
00:36:34,606 --> 00:36:34,816
three.

1191
00:36:36,176 --> 00:36:37,306
And so on -- I think you get the

1192
00:36:37,306 --> 00:36:37,646
idea.

1193
00:36:37,866 --> 00:36:38,936
Let's just skip to the end.

1194
00:36:39,296 --> 00:36:41,416
So in the end, we get our three

1195
00:36:41,416 --> 00:36:43,506
top captions for this particular

1196
00:36:43,506 --> 00:36:43,936
image.

1197
00:36:43,936 --> 00:36:45,906
And the best one is a man riding

1198
00:36:45,906 --> 00:36:47,426
a wave on top of a surfboard,

1199
00:36:47,656 --> 00:36:48,616
which I think it's pretty close.

1200
00:36:50,966 --> 00:36:52,346
So -- [applause] and let's now

1201
00:36:52,346 --> 00:36:52,726
do a demo.

1202
00:36:53,508 --> 00:36:55,508
[ Applause ]

1203
00:36:58,476 --> 00:37:00,116
So now we'll do a demo of this

1204
00:37:00,606 --> 00:37:02,206
-- of the captioning network.

1205
00:37:03,346 --> 00:37:04,856
So we have a collection of

1206
00:37:04,856 --> 00:37:07,156
images here, and as soon as I

1207
00:37:07,156 --> 00:37:09,036
tap on an image then the CNN

1208
00:37:09,136 --> 00:37:10,826
will run to determine what is

1209
00:37:10,826 --> 00:37:12,526
depicted in the image.

1210
00:37:12,526 --> 00:37:14,046
And then the RNN will run to

1211
00:37:14,046 --> 00:37:15,226
generate the actual caption.

1212
00:37:15,356 --> 00:37:16,036
So let's try it out.

1213
00:37:17,826 --> 00:37:19,446
>> A man riding a wave on top of

1214
00:37:19,446 --> 00:37:19,766
a surfboard.

1215
00:37:19,766 --> 00:37:22,356
>> So we already know this.

1216
00:37:23,526 --> 00:37:24,566
Now let's try another image.

1217
00:37:24,996 --> 00:37:26,536
>> An old truck is parked in the

1218
00:37:26,536 --> 00:37:27,106
field.

1219
00:37:27,626 --> 00:37:28,936
>> So the network actually knows

1220
00:37:28,976 --> 00:37:30,356
that it's an old truck and that

1221
00:37:30,356 --> 00:37:31,786
it's parked and not moving,

1222
00:37:31,966 --> 00:37:33,336
which I think is pretty

1223
00:37:33,556 --> 00:37:34,156
impressive.

1224
00:37:34,786 --> 00:37:35,656
Now let's try one more.

1225
00:37:37,026 --> 00:37:38,496
>> A black and white dog laying

1226
00:37:38,496 --> 00:37:39,086
in the grass.

1227
00:37:39,796 --> 00:37:40,896
>> So the network knows that

1228
00:37:40,896 --> 00:37:42,176
it's a black and white dog and

1229
00:37:42,176 --> 00:37:43,636
that it's laying in the grass,

1230
00:37:44,326 --> 00:37:45,106
not running.

1231
00:37:45,236 --> 00:37:46,356
Not walking.

1232
00:37:46,726 --> 00:37:47,876
Not sitting.

1233
00:37:48,336 --> 00:37:50,266
Laying in the grass.

1234
00:37:50,766 --> 00:37:52,796
So pretty cool.

1235
00:37:53,516 --> 00:37:57,786
[ Applause ]

1236
00:37:58,286 --> 00:37:58,726
Thank you.

1237
00:37:59,306 --> 00:38:01,166
And on this note, let's go to

1238
00:38:01,236 --> 00:38:01,586
the summary.

1239
00:38:02,066 --> 00:38:03,536
So in this session we talked

1240
00:38:03,536 --> 00:38:05,246
about all of the new primitives

1241
00:38:05,276 --> 00:38:07,006
that we added to the MPS

1242
00:38:07,336 --> 00:38:08,206
framework this year.

1243
00:38:08,586 --> 00:38:10,236
We've expanded our support for

1244
00:38:10,236 --> 00:38:12,386
image processing primitives and

1245
00:38:12,786 --> 00:38:13,736
for convolutional neural

1246
00:38:13,736 --> 00:38:14,136
networks.

1247
00:38:15,006 --> 00:38:16,596
And we've added support for

1248
00:38:16,596 --> 00:38:18,886
linear algebra and recurrent

1249
00:38:18,886 --> 00:38:22,226
neural networks.

1250
00:38:23,096 --> 00:38:24,666
the framework is optimized for

1251
00:38:24,666 --> 00:38:26,146
iOS, as I told you, and now

1252
00:38:26,596 --> 00:38:29,226
these primitives are also all

1253
00:38:29,226 --> 00:38:30,016
available on the Mac.

1254
00:38:31,116 --> 00:38:32,416
We also talked about the new

1255
00:38:32,416 --> 00:38:34,676
neural network graph API and we

1256
00:38:34,676 --> 00:38:36,406
showed you how easy it is to

1257
00:38:36,406 --> 00:38:38,336
use, to build and execute your

1258
00:38:38,336 --> 00:38:39,666
networks on the GPU.

1259
00:38:40,426 --> 00:38:42,186
And that it makes it possible

1260
00:38:42,186 --> 00:38:43,586
for us to deliver the best

1261
00:38:43,666 --> 00:38:45,346
performance for your networks

1262
00:38:45,346 --> 00:38:46,336
across the different GPUs.

1263
00:38:46,336 --> 00:38:49,776
And we would love to see you use

1264
00:38:49,776 --> 00:38:51,976
all of this new functionality to

1265
00:38:51,976 --> 00:38:53,526
create a really great apps and

1266
00:38:53,526 --> 00:38:55,836
tell us about it.

1267
00:38:56,116 --> 00:38:57,296
So please check out the related

1268
00:38:57,296 --> 00:38:58,856
Metal 2 sessions and the

1269
00:38:58,856 --> 00:39:01,186
sessions on the core ML and

1270
00:39:01,676 --> 00:39:03,006
Accelerate and Vision

1271
00:39:03,006 --> 00:39:03,486
Frameworks.

1272
00:39:04,716 --> 00:39:06,036
And for more information about

1273
00:39:06,096 --> 00:39:07,636
this session and for links of

1274
00:39:07,706 --> 00:39:09,936
sample code, please check out

1275
00:39:09,986 --> 00:39:11,426
this link on our developer

1276
00:39:11,426 --> 00:39:14,276
website and thank you so much

1277
00:39:14,276 --> 00:39:15,636
for coming, and have a great

1278
00:39:15,636 --> 00:39:15,846
WWDC.

1279
00:39:16,516 --> 00:39:20,500
[ Applause ]

